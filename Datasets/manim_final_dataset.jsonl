{"prompt": "Demonstrate how a square's FadeIn animation can be overridden to instead create the shape from scratch.", "response": "from manim import *\n\nclass MySquare(Square):\n    @override_animation(FadeIn)\n    def _fade_in_override(self, **kwargs):\n        return Create(self, **kwargs)\n\nclass OverrideAnimationExample(Scene):\n    def construct(self):\n        self.play(FadeIn(MySquare()))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.animation.html", "original_name": "animation "}}
{"prompt": "Show surrounding rectangle and add three texts over it with Succession animation.", "response": "from manim import *\n\nclass DefaultAddScene(Scene):\n    def construct(self):\n        text_1 = Text(\"I was added with Add!\")\n        text_2 = Text(\"Me too!\")\n        text_3 = Text(\"And me!\")\n        texts = VGroup(text_1, text_2, text_3).arrange(DOWN)\n        rect = SurroundingRectangle(texts, buff=0.5)\n\n        self.play(\n            Create(rect, run_time=3.0),\n            Succession(\n                Wait(1.0),\n                # You can Add a Mobject in the middle of an animation...\n                Add(text_1),\n                Wait(1.0),\n                # ...or multiple Mobjects at once!\n                Add(text_2, text_3),\n            ),\n        )\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.animation.Add.html", "original_name": "Add "}}
{"prompt": "Demonstrate the animation of dot groups with varying lag ratios and vertical shifts, displaying corresponding text labels.", "response": "from manim import *\n\nclass LagRatios(Scene):\n    def construct(self):\n        ratios = [0, 0.1, 0.5, 1, 2]  # demonstrated lag_ratios\n\n        # Create dot groups\n        group = VGroup(*[Dot() for _ in range(4)]).arrange_submobjects()\n        groups = VGroup(*[group.copy() for _ in ratios]).arrange_submobjects(buff=1)\n        self.add(groups)\n\n        # Label groups\n        self.add(Text(\"lag_ratio = \", font_size=36).next_to(groups, UP, buff=1.5))\n        for group, ratio in zip(groups, ratios):\n            self.add(Text(str(ratio), font_size=36).next_to(group, UP))\n\n        #Animate groups with different lag_ratios\n        self.play(AnimationGroup(*[\n            group.animate(lag_ratio=ratio, run_time=1.5).shift(DOWN * 2)\n            for group, ratio in zip(groups, ratios)\n        ]))\n\n        # lag_ratio also works recursively on nested submobjects:\n        self.play(groups.animate(run_time=1, lag_ratio=0.1).shift(UP * 2))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.animation.Animation.html", "original_name": "Animation "}}
{"prompt": "Demonstrate how to create a square that simultaneously fades in and appears.", "response": "from manim import *\n\nclass MySquare(Square):\n    @override_animation(FadeIn)\n    def _fade_in_override(self, **kwargs):\n        return Create(self, **kwargs)\n\nclass OverrideAnimationExample(Scene):\n    def construct(self):\n        self.play(FadeIn(MySquare()))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.animation.html#manim.animation.animation.override_animation", "original_name": "animation "}}
{"prompt": "Demonstrate a square animation with a fade-in effect.", "response": "from manim import *\n\nclass MySquare(Square):\n    @override_animation(FadeIn)\n    def _fade_in_override(self, **kwargs):\n        return Create(self, **kwargs)\n\nclass OverrideAnimationExample(Scene):\n    def construct(self):\n        self.play(FadeIn(MySquare()))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.animation.html#manim.animation.animation.prepare_animation", "original_name": "animation "}}
{"prompt": "Demonstrate a visually animated boundary effect on a rotating colored text element.", "response": "from manim import *\n\nclass AnimatedBoundaryExample(Scene):\n    def construct(self):\n        text = Text(\"So shiny!\")\n        boundary = AnimatedBoundary(text, colors=[RED, GREEN, BLUE],\n                                    cycle_rate=3)\n        self.add(text, boundary)\n        self.wait(2)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.changing.AnimatedBoundary.html", "original_name": "AnimatedBoundary "}}
{"prompt": "Demonstrate a red circle tracing its path as it rolls to the right, with a fixed dot following along.", "response": "from manim import *\n\nclass TracedPathExample(Scene):\n    def construct(self):\n        circ = Circle(color=RED).shift(4*LEFT)\n        dot = Dot(color=RED).move_to(circ.get_start())\n        rolling_circle = VGroup(circ, dot)\n        trace = TracedPath(circ.get_start)\n        rolling_circle.add_updater(lambda m: m.rotate(-0.3))\n        self.add(trace, rolling_circle)\n        self.play(rolling_circle.animate.shift(8*RIGHT), run_time=4, rate_func=linear)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.changing.TracedPath.html", "original_name": "TracedPath "}}
{"prompt": "Demonstrate a scene with dots moving in sync, labeled \"25%\", while showing smooth lagged start animation.", "response": "from manim import *\n\nclass LaggedStartExample(Scene):\n    def construct(self):\n        title = Text(\"lag_ratio = 0.25\").to_edge(UP)\n\n        dot1 = Dot(point=LEFT * 2 + UP, radius=0.16)\n        dot2 = Dot(point=LEFT * 2, radius=0.16)\n        dot3 = Dot(point=LEFT * 2 + DOWN, radius=0.16)\n        line_25 = DashedLine(\n            start=LEFT + UP * 2,\n            end=LEFT + DOWN * 2,\n            color=RED\n        )\n        label = Text(\"25%\", font_size=24).next_to(line_25, UP)\n        self.add(title, dot1, dot2, dot3, line_25, label)\n\n        self.play(LaggedStart(\n            dot1.animate.shift(RIGHT * 4),\n            dot2.animate.shift(RIGHT * 4),\n            dot3.animate.shift(RIGHT * 4),\n            lag_ratio=0.25,\n            run_time=4\n        ))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.composition.LaggedStart.html", "original_name": "LaggedStart "}}
{"prompt": "Create a 5x7 grid of dots and title text, then show a lagged yellow ripple effect animation.", "response": "from manim import *\n\nclass LaggedStartMapExample(Scene):\n    def construct(self):\n        title = Tex(\"LaggedStartMap\").to_edge(UP, buff=LARGE_BUFF)\n        dots = VGroup(\n            *[Dot(radius=0.16) for _ in range(35)]\n            ).arrange_in_grid(rows=5, cols=7, buff=MED_LARGE_BUFF)\n        self.add(dots, title)\n\n        # Animate yellow ripple effect\n        for mob in dots, title:\n            self.play(LaggedStartMap(\n                ApplyMethod, mob,\n                lambda m : (m.set_color, YELLOW),\n                lag_ratio = 0.1,\n                rate_func = there_and_back,\n                run_time = 2\n            ))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.composition.LaggedStartMap.html", "original_name": "LaggedStartMap "}}
{"prompt": "Demonstrate the movement of four dots around a square shape in a Manim animation, showcasing their successive transitions.", "response": "from manim import *\n\nclass SuccessionExample(Scene):\n    def construct(self):\n        dot1 = Dot(point=LEFT * 2 + UP * 2, radius=0.16, color=BLUE)\n        dot2 = Dot(point=LEFT * 2 + DOWN * 2, radius=0.16, color=MAROON)\n        dot3 = Dot(point=RIGHT * 2 + DOWN * 2, radius=0.16, color=GREEN)\n        dot4 = Dot(point=RIGHT * 2 + UP * 2, radius=0.16, color=YELLOW)\n        self.add(dot1, dot2, dot3, dot4)\n\n        self.play(Succession(\n            dot1.animate.move_to(dot2),\n            dot2.animate.move_to(dot3),\n            dot3.animate.move_to(dot4),\n            dot4.animate.move_to(dot1)\n        ))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.composition.Succession.html", "original_name": "Succession "}}
{"prompt": "Demonstrate the creation of a square using the Manim animation library.", "response": "from manim import *\n\nclass CreateScene(Scene):\n    def construct(self):\n        self.play(Create(Square()))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.creation.Create.html", "original_name": "Create "}}
{"prompt": "Demonstrate a square drawing itself with a border that then fills in fully.", "response": "from manim import *\n\nclass ShowDrawBorderThenFill(Scene):\n    def construct(self):\n        self.play(DrawBorderThenFill(Square(fill_opacity=1, fill_color=ORANGE)))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.creation.DrawBorderThenFill.html", "original_name": "DrawBorderThenFill "}}
{"prompt": "Demonstrate increasing subsets of geometric shapes (dot, square, triangle) in a seamless animation.", "response": "from manim import *\n\nclass ShowIncreasingSubsetsScene(Scene):\n    def construct(self):\n        p = VGroup(Dot(), Square(), Triangle())\n        self.add(p)\n        self.play(ShowIncreasingSubsets(p))\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.creation.ShowIncreasingSubsets.html", "original_name": "ShowIncreasingSubsets "}}
{"prompt": "Demonstrate a spiral animation that moves in towards a group of three shapes.", "response": "from manim import *\n\nclass SpiralInExample(Scene):\n    def construct(self):\n        pi = MathTex(r\"\\pi\").scale(7)\n        pi.shift(2.25 * LEFT + 1.5 * UP)\n        circle = Circle(color=GREEN_C, fill_opacity=1).shift(LEFT)\n        square = Square(color=BLUE_D, fill_opacity=1).shift(UP)\n        shapes = VGroup(pi, circle, square)\n        self.play(SpiralIn(shapes))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.creation.SpiralIn.html", "original_name": "SpiralIn "}}
{"prompt": "Demonstrate inserting text with a cursor that blinks and highlights with purple color.", "response": "from manim import *\n\nclass InsertingTextExample(Scene):\n    def construct(self):\n        text = Text(\"Inserting\", color=PURPLE).scale(1.5).to_edge(LEFT)\n        cursor = Rectangle(\n            color = GREY_A,\n            fill_color = GREY_A,\n            fill_opacity = 1.0,\n            height = 1.1,\n            width = 0.5,\n        ).move_to(text[0]) # Position the cursor\n\n        self.play(TypeWithCursor(text, cursor))\n        self.play(Blink(cursor, blinks=2))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.creation.TypeWithCursor.html", "original_name": "TypeWithCursor "}}
{"prompt": "Create a visual representation of an object (a square) being uncreated, implying its disappearance.", "response": "from manim import *\n\nclass ShowUncreate(Scene):\n    def construct(self):\n        self.play(Uncreate(Square()))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.creation.Uncreate.html", "original_name": "Uncreate "}}
{"prompt": "Create a scene demonstrating text with a moving cursor that blinks and appears to be deleted by changing its opacity.", "response": "from manim import *\n\nclass DeletingTextExample(Scene):\n    def construct(self):\n        text = Text(\"Deleting\", color=PURPLE).scale(1.5).to_edge(LEFT)\n        cursor = Rectangle(\n            color = GREY_A,\n            fill_color = GREY_A,\n            fill_opacity = 1.0,\n            height = 1.1,\n            width = 0.5,\n        ).move_to(text[0]) # Position the cursor\n\n        self.play(UntypeWithCursor(text, cursor))\n        self.play(Blink(cursor, blinks=2))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.creation.UntypeWithCursor.html", "original_name": "UntypeWithCursor "}}
{"prompt": "Demonstrate a reversible unwrapping of a scaled text label.", "response": "from manim import *\n\nclass UnwriteReverseTrue(Scene):\n    def construct(self):\n        text = Tex(\"Alice and Bob\").scale(3)\n        self.add(text)\n        self.play(Unwrite(text))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.creation.Unwrite.html", "original_name": "Unwrite "}}
{"prompt": "Demonstrate a large, bold \"Hello\" text appearing on screen.", "response": "from manim import *\n\nclass ShowWrite(Scene):\n    def construct(self):\n        self.play(Write(Text(\"Hello\", font_size=144)))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.creation.Write.html", "original_name": "Write "}}
{"prompt": "Demonstrate a fade-in and out animation of two texts, shifting and scaling them horizontally over time.", "response": "from manim import *\n\nclass Fading(Scene):\n    def construct(self):\n        tex_in = Tex(\"Fade\", \"In\").scale(3)\n        tex_out = Tex(\"Fade\", \"Out\").scale(3)\n        self.play(FadeIn(tex_in, shift=DOWN, scale=0.66))\n        self.play(ReplacementTransform(tex_in, tex_out))\n        self.play(FadeOut(tex_out, shift=DOWN * 2, scale=1.5))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.fading.html", "original_name": "fading "}}
{"prompt": "Demonstrate a group animation that fades in multiple elements with varying transformations (shift, target position, and scale) on the screen simultaneously.", "response": "from manim import *\n\nclass FadeInExample(Scene):\n    def construct(self):\n        dot = Dot(UP * 2 + LEFT)\n        self.add(dot)\n        tex = Tex(\n            \"FadeIn with \", \"shift \", r\" or target\\_position\", \" and scale\"\n        ).scale(1)\n        animations = [\n            FadeIn(tex[0]),\n            FadeIn(tex[1], shift=DOWN),\n            FadeIn(tex[2], target_position=dot),\n            FadeIn(tex[3], scale=1.5),\n        ]\n        self.play(AnimationGroup(*animations, lag_ratio=0.5))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.fading.FadeIn.html", "original_name": "FadeIn "}}
{"prompt": "Demonstrate the animation of a dot fading in and out while applying various transformations such as shift, target position, and scale over time.", "response": "from manim import *\n\nclass FadeInExample(Scene):\n    def construct(self):\n        dot = Dot(UP * 2 + LEFT)\n        self.add(dot)\n        tex = Tex(\n            \"FadeOut with \", \"shift \", r\" or target\\_position\", \" and scale\"\n        ).scale(1)\n        animations = [\n            FadeOut(tex[0]),\n            FadeOut(tex[1], shift=DOWN),\n            FadeOut(tex[2], target_position=dot),\n            FadeOut(tex[3], scale=0.5),\n        ]\n        self.play(AnimationGroup(*animations, lag_ratio=0.5))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.fading.FadeOut.html", "original_name": "FadeOut "}}
{"prompt": "Demonstrate the growth of various shapes and objects from different directions in a scene.", "response": "from manim import *\n\nclass Growing(Scene):\n    def construct(self):\n        square = Square()\n        circle = Circle()\n        triangle = Triangle()\n        arrow = Arrow(LEFT, RIGHT)\n        star = Star()\n\n        VGroup(square, circle, triangle).set_x(0).arrange(buff=1.5).set_y(2)\n        VGroup(arrow, star).move_to(DOWN).set_x(0).arrange(buff=1.5).set_y(-2)\n\n        self.play(GrowFromPoint(square, ORIGIN))\n        self.play(GrowFromCenter(circle))\n        self.play(GrowFromEdge(triangle, DOWN))\n        self.play(GrowArrow(arrow))\n        self.play(SpinInFromNothing(star))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.growing.html", "original_name": "growing "}}
{"prompt": "Create a growing animation of two arrows, one stretching from left to right and the other from bottom left to top right, with the second arrow displayed in red.", "response": "from manim import *\n\nclass GrowArrowExample(Scene):\n    def construct(self):\n        arrows = [Arrow(2 * LEFT, 2 * RIGHT), Arrow(2 * DR, 2 * UL)]\n        VGroup(*arrows).set_x(0).arrange(buff=2)\n        self.play(GrowArrow(arrows[0]))\n        self.play(GrowArrow(arrows[1], point_color=RED))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.growing.GrowArrow.html", "original_name": "GrowArrow "}}
{"prompt": "Demonstrate a square growing from the center, with one square changing color upon growth.", "response": "from manim import *\n\nclass GrowFromCenterExample(Scene):\n    def construct(self):\n        squares = [Square() for _ in range(2)]\n        VGroup(*squares).set_x(0).arrange(buff=2)\n        self.play(GrowFromCenter(squares[0]))\n        self.play(GrowFromCenter(squares[1], point_color=RED))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.growing.GrowFromCenter.html", "original_name": "GrowFromCenter "}}
{"prompt": "Demonstrate a 4-square grid growing from its edges in alternating directions.", "response": "from manim import *\n\nclass GrowFromEdgeExample(Scene):\n    def construct(self):\n        squares = [Square() for _ in range(4)]\n        VGroup(*squares).set_x(0).arrange(buff=1)\n        self.play(GrowFromEdge(squares[0], DOWN))\n        self.play(GrowFromEdge(squares[1], RIGHT))\n        self.play(GrowFromEdge(squares[2], UR))\n        self.play(GrowFromEdge(squares[3], UP, point_color=RED))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.growing.GrowFromEdge.html", "original_name": "GrowFromEdge "}}
{"prompt": "Demonstrate a geometric figure (four squares) growing from specific points with varying colors.", "response": "from manim import *\n\nclass GrowFromPointExample(Scene):\n    def construct(self):\n        dot = Dot(3 * UR, color=GREEN)\n        squares = [Square() for _ in range(4)]\n        VGroup(*squares).set_x(0).arrange(buff=1)\n        self.add(dot)\n        self.play(GrowFromPoint(squares[0], ORIGIN))\n        self.play(GrowFromPoint(squares[1], [-2, 2, 0]))\n        self.play(GrowFromPoint(squares[2], [3, -2, 0], RED))\n        self.play(GrowFromPoint(squares[3], dot, dot.get_color()))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.growing.GrowFromPoint.html", "original_name": "GrowFromPoint "}}
{"prompt": "Demonstrate the animation of three squares spinning into view with varying color and rotation.", "response": "from manim import *\n\nclass SpinInFromNothingExample(Scene):\n    def construct(self):\n        squares = [Square() for _ in range(3)]\n        VGroup(*squares).set_x(0).arrange(buff=2)\n        self.play(SpinInFromNothing(squares[0]))\n        self.play(SpinInFromNothing(squares[1], angle=2 * PI))\n        self.play(SpinInFromNothing(squares[2], point_color=RED))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.growing.SpinInFromNothing.html", "original_name": "SpinInFromNothing "}}
{"prompt": "Demonstrate a sequence of animations showcasing 7 visual effects, transitioning between each by fading out and fading in subsequent elements with slight vertical shifts.", "response": "from manim import *\n\nclass Indications(Scene):\n    def construct(self):\n        indications = [ApplyWave,Circumscribe,Flash,FocusOn,Indicate,ShowPassingFlash,Wiggle]\n        names = [Tex(i.__name__).scale(3) for i in indications]\n\n        self.add(names[0])\n        for i in range(len(names)):\n            if indications[i] is Flash:\n                self.play(Flash(UP))\n            elif indications[i] is ShowPassingFlash:\n                self.play(ShowPassingFlash(Underline(names[i])))\n            else:\n                self.play(indications[i](names[i]))\n            self.play(AnimationGroup(\n                FadeOut(names[i], shift=UP*1.5),\n                FadeIn(names[(i+1)%len(names)], shift=UP*1.5),\n            ))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.indication.html", "original_name": "indication "}}
{"prompt": "Demonstrate wave motion on a single text element, with varying speed and ripple effects.", "response": "from manim import *\n\nclass ApplyingWaves(Scene):\n    def construct(self):\n        tex = Tex(\"WaveWaveWaveWaveWave\").scale(2)\n        self.play(ApplyWave(tex))\n        self.play(ApplyWave(\n            tex,\n            direction=RIGHT,\n            time_width=0.5,\n            amplitude=0.3\n        ))\n        self.play(ApplyWave(\n            tex,\n            rate_func=linear,\n            ripples=4\n        ))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.indication.ApplyWave.html", "original_name": "ApplyWave "}}
{"prompt": "Create a scene with a large, blinking text label that switches between visibility in 3 successive intervals.", "response": "from manim import *\n\nclass BlinkingExample(Scene):\n    def construct(self):\n        text = Text(\"Blinking\").scale(1.5)\n        self.add(text)\n        self.play(Blink(text, blinks=3))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.indication.Blink.html", "original_name": "Blink "}}
{"prompt": "Create a 2x scaled label with the text \"Circum-scribe\" and demonstrate its transformation into a circle.", "response": "from manim import *\n\nclass UsingCircumscribe(Scene):\n    def construct(self):\n        lbl = Tex(r\"Circum-\\\\scribe\").scale(2)\n        self.add(lbl)\n        self.play(Circumscribe(lbl))\n        self.play(Circumscribe(lbl, Circle))\n        self.play(Circumscribe(lbl, fade_out=True))\n        self.play(Circumscribe(lbl, time_width=2))\n        self.play(Circumscribe(lbl, Circle, True))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.indication.Circumscribe.html", "original_name": "Circumscribe "}}
{"prompt": "Create a flash animation that moves a yellow dot downwards.", "response": "from manim import *\n\nclass UsingFlash(Scene):\n    def construct(self):\n        dot = Dot(color=YELLOW).shift(DOWN)\n        self.add(Tex(\"Flash the dot below:\"), dot)\n        self.play(Flash(dot))\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.indication.Flash.html", "original_name": "Flash "}}
{"prompt": "Create a visual focus effect on a yellow dot that moves downward.", "response": "from manim import *\n\nclass UsingFocusOn(Scene):\n    def construct(self):\n        dot = Dot(color=YELLOW).shift(DOWN)\n        self.add(Tex(\"Focusing on the dot below:\"), dot)\n        self.play(FocusOn(dot))\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.indication.FocusOn.html", "original_name": "FocusOn "}}
{"prompt": "Demonstrate a highlighted text box using the Indicate animation, scaling it to 3 times its size.", "response": "from manim import *\n\nclass UsingIndicate(Scene):\n    def construct(self):\n        tex = Tex(\"Indicate\").scale(3)\n        self.play(Indicate(tex))\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.indication.Indicate.html", "original_name": "Indicate "}}
{"prompt": "Create a visual demonstration of different time widths, transitioning between colored polygons and labeled text.", "response": "from manim import *\n\nclass TimeWidthValues(Scene):\n    def construct(self):\n        p = RegularPolygon(5, color=DARK_GRAY, stroke_width=6).scale(3)\n        lbl = VMobject()\n        self.add(p, lbl)\n        p = p.copy().set_color(BLUE)\n        for time_width in [0.2, 0.5, 1, 2]:\n            lbl.become(Tex(r\"\\texttt{time\\_width={{%.1f}}}\"%time_width))\n            self.play(ShowPassingFlash(\n                p.copy().set_color(BLUE),\n                run_time=2,\n                time_width=time_width\n            ))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.indication.ShowPassingFlash.html", "original_name": "ShowPassingFlash "}}
{"prompt": "Demonstrate a wiggling animation of the text \"Wiggle\" scaled up to 3 units.", "response": "from manim import *\n\nclass ApplyingWaves(Scene):\n    def construct(self):\n        tex = Tex(\"Wiggle\").scale(3)\n        self.play(Wiggle(tex))\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.indication.Wiggle.html", "original_name": "Wiggle "}}
{"prompt": "Create a visual demonstration of a 3D square that smoothly translates and oscillates along the Y-axis over time.", "response": "from manim import *\n\nclass HomotopyExample(Scene):\n    def construct(self):\n        square = Square()\n\n        def homotopy(x, y, z, t):\n            if t <= 0.25:\n                progress = t / 0.25\n                return (x, y + progress * 0.2 * np.sin(x), z)\n            else:\n                wave_progress = (t - 0.25) / 0.75\n                return (x, y + 0.2 * np.sin(x + 10 * wave_progress), z)\n\n        self.play(Homotopy(homotopy, square, rate_func= linear, run_time=2))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.movement.Homotopy.html", "original_name": "Homotopy "}}
{"prompt": "Create a Manim animation demonstrating the movement of an orange dot along a line path while maintaining its color.", "response": "from manim import *\n\nclass MoveAlongPathExample(Scene):\n    def construct(self):\n        d1 = Dot().set_color(ORANGE)\n        l1 = Line(LEFT, RIGHT)\n        l2 = VMobject()\n        self.add(d1, l1, l2)\n        l2.add_updater(lambda x: x.become(Line(LEFT, d1.get_center()).set_color(ORANGE)))\n        self.play(MoveAlongPath(d1, l1), rate_func=linear)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.movement.MoveAlongPath.html", "original_name": "MoveAlongPath "}}
{"prompt": "Show a rotating square animation with smooth motion around its center point.", "response": "from manim import *\n\nclass UsingRotate(Scene):\n    def construct(self):\n        self.play(\n            Rotate(\n                Square(side_length=0.5).shift(UP * 2),\n                angle=2*PI,\n                about_point=ORIGIN,\n                rate_func=linear,\n            ),\n            Rotate(Square(side_length=0.5), angle=2*PI, rate_func=linear),\n            )", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.rotation.Rotate.html", "original_name": "Rotate "}}
{"prompt": "Show a teal circle being broadcasted across the screen with smooth animation.", "response": "from manim import *\n\nclass BroadcastExample(Scene):\n    def construct(self):\n        mob = Circle(radius=4, color=TEAL_A)\n        self.play(Broadcast(mob))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.specialized.Broadcast.html", "original_name": "Broadcast "}}
{"prompt": "Demonstrate the animation of two dots moving at different speeds along a horizontal axis, with a smooth transition between them.", "response": "from manim import *\n\nclass SpeedModifierExample(Scene):\n    def construct(self):\n        a = Dot().shift(LEFT * 4)\n        b = Dot().shift(RIGHT * 4)\n        self.add(a, b)\n        self.play(\n            ChangeSpeed(\n                AnimationGroup(\n                    a.animate(run_time=1).shift(RIGHT * 8),\n                    b.animate(run_time=1).shift(LEFT * 8),\n                ),\n                speedinfo={0.3: 1, 0.4: 0.1, 0.6: 0.1, 1: 1},\n                rate_func=linear,\n            )\n        )", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.speedmodifier.ChangeSpeed.html", "original_name": "ChangeSpeed "}}
{"prompt": "Demonstrate the application of a 2x2 transformation matrix to display text and a coordinate plane with scaling.", "response": "from manim import *\n\nclass ApplyMatrixExample(Scene):\n    def construct(self):\n        matrix = [[1, 1], [0, 2/3]]\n        self.play(ApplyMatrix(matrix, Text(\"Hello World!\")), ApplyMatrix(matrix, NumberPlane()))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.ApplyMatrix.html", "original_name": "ApplyMatrix "}}
{"prompt": "Demonstrate a 2D square warping into a 3D cube by transforming its points along the z-axis.", "response": "from manim import *\n\nclass WarpSquare(Scene):\n    def construct(self):\n        square = Square()\n        self.play(\n            ApplyPointwiseFunction(\n                lambda point: complex_to_R3(np.exp(R3_to_complex(point))), square\n            )\n        )\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.ApplyPointwiseFunction.html", "original_name": "ApplyPointwiseFunction "}}
{"prompt": "Create a transformation sequence showing two squares rotating clockwise around the dots.", "response": "from manim import *\n\nclass ClockwiseExample(Scene):\n    def construct(self):\n        dl, dr = Dot(), Dot()\n        sl, sr = Square(), Square()\n\n        VGroup(dl, sl).arrange(DOWN).shift(2*LEFT)\n        VGroup(dr, sr).arrange(DOWN).shift(2*RIGHT)\n\n        self.add(dl, dr)\n        self.wait()\n        self.play(\n            ClockwiseTransform(dl, sl),\n            Transform(dr, sr)\n        )\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.ClockwiseTransform.html", "original_name": "ClockwiseTransform "}}
{"prompt": "Demonstrate the transformation of two numbers with a counterclockwise and clockwise motion, scaling their texts.", "response": "from manim import *\n\nclass CounterclockwiseTransform_vs_Transform(Scene):\n    def construct(self):\n        # set up the numbers\n        c_transform = VGroup(DecimalNumber(number=3.141, num_decimal_places=3), DecimalNumber(number=1.618, num_decimal_places=3))\n        text_1 = Text(\"CounterclockwiseTransform\", color=RED)\n        c_transform.add(text_1)\n\n        transform = VGroup(DecimalNumber(number=1.618, num_decimal_places=3), DecimalNumber(number=3.141, num_decimal_places=3))\n        text_2 = Text(\"Transform\", color=BLUE)\n        transform.add(text_2)\n\n        ints = VGroup(c_transform, transform)\n        texts = VGroup(text_1, text_2).scale(0.75)\n        c_transform.arrange(direction=UP, buff=1)\n        transform.arrange(direction=UP, buff=1)\n\n        ints.arrange(buff=2)\n        self.add(ints, texts)\n\n        # The mobs move in clockwise direction for ClockwiseTransform()\n        self.play(CounterclockwiseTransform(c_transform[0], c_transform[1]))\n\n        # The mobs move straight up for Transform()\n        self.play(Transform(transform[0], transform[1]))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.CounterclockwiseTransform.html", "original_name": "CounterclockwiseTransform "}}
{"prompt": "Create a rotating, cyclic animation of shapes (square, circle, triangle, star) by replacing each shape with the next one.", "response": "from manim import *\n\nclass CyclicReplaceExample(Scene):\n    def construct(self):\n        group = VGroup(Square(), Circle(), Triangle(), Star())\n        group.arrange(RIGHT)\n        self.add(group)\n\n        for _ in range(4):\n            self.play(CyclicReplace(*group))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.CyclicReplace.html", "original_name": "CyclicReplace "}}
{"prompt": "Demonstrate a simple animation where text fades to red.", "response": "from manim import *\n\nclass FadeToColorExample(Scene):\n    def construct(self):\n        self.play(FadeToColor(Text(\"Hello World!\"), color=RED))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.FadeToColor.html", "original_name": "FadeToColor "}}
{"prompt": "Demonstrate a series of transformations involving fading and scaling of rectangles to circles with varying opacity and scale.", "response": "from manim import *\n\nclass DifferentFadeTransforms(Scene):\n    def construct(self):\n        starts = [Rectangle(width=4, height=1) for _ in range(3)]\n        VGroup(*starts).arrange(DOWN, buff=1).shift(3*LEFT)\n        targets = [Circle(fill_opacity=1).scale(0.25) for _ in range(3)]\n        VGroup(*targets).arrange(DOWN, buff=1).shift(3*RIGHT)\n\n        self.play(*[FadeIn(s) for s in starts])\n        self.play(\n            FadeTransform(starts[0], targets[0], stretch=True),\n            FadeTransform(starts[1], targets[1], stretch=False, dim_to_match=0),\n            FadeTransform(starts[2], targets[2], stretch=False, dim_to_match=1)\n        )\n\n        self.play(*[FadeOut(mobj) for mobj in self.mobjects])", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.FadeTransform.html", "original_name": "FadeTransform "}}
{"prompt": "Demonstrate the transformation of two groups of shapes with fading and sliding motions.", "response": "from manim import *\n\nclass FadeTransformSubmobjects(Scene):\n    def construct(self):\n        src = VGroup(Square(), Circle().shift(LEFT + UP))\n        src.shift(3*LEFT + 2*UP)\n        src_copy = src.copy().shift(4*DOWN)\n\n        target = VGroup(Circle(), Triangle().shift(RIGHT + DOWN))\n        target.shift(3*RIGHT + 2*UP)\n        target_copy = target.copy().shift(4*DOWN)\n\n        self.play(FadeIn(src), FadeIn(src_copy))\n        self.play(\n            FadeTransform(src, target),\n            FadeTransformPieces(src_copy, target_copy)\n        )\n        self.play(*[FadeOut(mobj) for mobj in self.mobjects])", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.FadeTransformPieces.html", "original_name": "FadeTransformPieces "}}
{"prompt": "Create a green circle that smoothly moves to its target position, scaling down and shifting slightly along the way.", "response": "from manim import *\n\nclass MoveToTargetExample(Scene):\n    def construct(self):\n        c = Circle()\n\n        c.generate_target()\n        c.target.set_fill(color=GREEN, opacity=0.5)\n        c.target.shift(2*RIGHT + UP).scale(0.5)\n\n        self.add(c)\n        self.play(MoveToTarget(c))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.MoveToTarget.html", "original_name": "MoveToTarget "}}
{"prompt": "Create a Manim animation that shows two groups of integers (one labeled \"ReplacementTransform\" and one labeled \"Transform\") replacing each other, with the second group lingering after the transformation.", "response": "from manim import *\n\nclass ReplacementTransformOrTransform(Scene):\n    def construct(self):\n        # set up the numbers\n        r_transform = VGroup(*[Integer(i) for i in range(1,4)])\n        text_1 = Text(\"ReplacementTransform\", color=RED)\n        r_transform.add(text_1)\n\n        transform = VGroup(*[Integer(i) for i in range(4,7)])\n        text_2 = Text(\"Transform\", color=BLUE)\n        transform.add(text_2)\n\n        ints = VGroup(r_transform, transform)\n        texts = VGroup(text_1, text_2).scale(0.75)\n        r_transform.arrange(direction=UP, buff=1)\n        transform.arrange(direction=UP, buff=1)\n\n        ints.arrange(buff=2)\n        self.add(ints, texts)\n\n        # The mobs replace each other and none are left behind\n        self.play(ReplacementTransform(r_transform[0], r_transform[1]))\n        self.play(ReplacementTransform(r_transform[1], r_transform[2]))\n\n        # The mobs linger after the Transform()\n        self.play(Transform(transform[0], transform[1]))\n        self.play(Transform(transform[1], transform[2]))\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.ReplacementTransform.html", "original_name": "ReplacementTransform "}}
{"prompt": "Demonstrate a square transforming into its original state over time, with a fade-in and fade-out effect.", "response": "from manim import *\n\nclass RestoreExample(Scene):\n    def construct(self):\n        s = Square()\n        s.save_state()\n        self.play(FadeIn(s))\n        self.play(s.animate.set_color(PURPLE).set_opacity(0.5).shift(2*LEFT).scale(3))\n        self.play(s.animate.shift(5*DOWN).rotate(PI/4))\n        self.wait()\n        self.play(Restore(s), run_time=2)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.Restore.html", "original_name": "Restore "}}
{"prompt": "Show a \"Hello World!\" text scaling in place from its original size to twice its original size.", "response": "from manim import *\n\nclass ScaleInPlaceExample(Scene):\n    def construct(self):\n        self.play(ScaleInPlace(Text(\"Hello World!\"), 2))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.ScaleInPlace.html", "original_name": "ScaleInPlace "}}
{"prompt": "Demonstrate text shrinking to the center of the screen.", "response": "from manim import *\n\nclass ShrinkToCenterExample(Scene):\n    def construct(self):\n        self.play(ShrinkToCenter(Text(\"Hello World!\")))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.ShrinkToCenter.html", "original_name": "ShrinkToCenter "}}
{"prompt": "Demonstrate colorful circles rotating around each other with smoothly animated arcs.", "response": "from manim import *\n\nclass TransformPathArc(Scene):\n    def construct(self):\n        def make_arc_path(start, end, arc_angle):\n            points = []\n            p_fn = path_along_arc(arc_angle)\n            # alpha animates between 0.0 and 1.0, where 0.0\n            # is the beginning of the animation and 1.0 is the end.\n            for alpha in range(0, 11):\n                points.append(p_fn(start, end, alpha / 10.0))\n            path = VMobject(stroke_color=YELLOW)\n            path.set_points_smoothly(points)\n            return path\n\n        left = Circle(stroke_color=BLUE_E, fill_opacity=1.0, radius=0.5).move_to(LEFT * 2)\n        colors = [TEAL_A, TEAL_B, TEAL_C, TEAL_D, TEAL_E, GREEN_A]\n        # Positive angles move counter-clockwise, negative angles move clockwise.\n        examples = [-90, 0, 30, 90, 180, 270]\n        anims = []\n        for idx, angle in enumerate(examples):\n            left_c = left.copy().shift((3 - idx) * UP)\n            left_c.fill_color = colors[idx]\n            right_c = left_c.copy().shift(4 * RIGHT)\n            path_arc = make_arc_path(left_c.get_center(), right_c.get_center(),\n                                     arc_angle=angle * DEGREES)\n            desc = Text('%d\u00b0' % examples[idx]).next_to(left_c, LEFT)\n            # Make the circles in front of the text in front of the arcs.\n            self.add(\n                path_arc.set_z_index(1),\n                desc.set_z_index(2),\n                left_c.set_z_index(3),\n            )\n            anims.append(Transform(left_c, right_c, path_arc=angle * DEGREES))\n\n        self.play(*anims, run_time=2)\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform.Transform.html", "original_name": "Transform "}}
{"prompt": "Demonstrate an anagram transformation between two Morse code sequences using a text-to-text animation with a circular overlap.", "response": "from manim import *\n\nclass Anagram(Scene):\n    def construct(self):\n        src = Text(\"the morse code\")\n        tar = Text(\"here come dots\")\n        self.play(Write(src))\n        self.wait(0.5)\n        self.play(TransformMatchingShapes(src, tar, path_arc=PI/2))\n        self.wait(0.5)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform_matching_parts.TransformMatchingShapes.html", "original_name": "TransformMatchingShapes "}}
{"prompt": "Demonstrate transformation of equations by superimposing variables, showcasing match and substitution changes with animations.", "response": "from manim import *\n\nclass MatchingEquationParts(Scene):\n    def construct(self):\n        variables = VGroup(MathTex(\"a\"), MathTex(\"b\"), MathTex(\"c\")).arrange_submobjects().shift(UP)\n\n        eq1 = MathTex(\"{{x}}^2\", \"+\", \"{{y}}^2\", \"=\", \"{{z}}^2\")\n        eq2 = MathTex(\"{{a}}^2\", \"+\", \"{{b}}^2\", \"=\", \"{{c}}^2\")\n        eq3 = MathTex(\"{{a}}^2\", \"=\", \"{{c}}^2\", \"-\", \"{{b}}^2\")\n\n        self.add(eq1)\n        self.wait(0.5)\n        self.play(TransformMatchingTex(Group(eq1, variables), eq2))\n        self.wait(0.5)\n        self.play(TransformMatchingTex(eq2, eq3))\n        self.wait(0.5)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.animation.transform_matching_parts.TransformMatchingTex.html", "original_name": "TransformMatchingTex "}}
{"prompt": "Demonstrate a dynamic graph with moving vertices and edges.", "response": "from manim import *\n\nclass MovingDiGraph(Scene):\n    def construct(self):\n        vertices = [1, 2, 3, 4]\n        edges = [(1, 2), (2, 3), (3, 4), (1, 3), (1, 4)]\n\n        g = DiGraph(vertices, edges)\n\n        self.add(g)\n        self.play(\n            g[1].animate.move_to([1, 1, 1]),\n            g[2].animate.move_to([-1, 1, 2]),\n            g[3].animate.move_to([1, -1, -1]),\n            g[4].animate.move_to([-1, -1, 0]),\n        )\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.graph.DiGraph.html", "original_name": "DiGraph "}}
{"prompt": "Demonstrate a graph with initial layout and transform its structure to a circular layout.", "response": "from manim import *\n\nclass ChangeGraphLayout(Scene):\n    def construct(self):\n        G = Graph([1, 2, 3, 4, 5], [(1, 2), (2, 3), (3, 4), (4, 5)],\n                  layout={1: [-2, 0, 0], 2: [-1, 0, 0], 3: [0, 0, 0],\n                          4: [1, 0, 0], 5: [2, 0, 0]}\n                  )\n        self.play(Create(G))\n        self.play(G.animate.change_layout(\"circular\"))\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.graph.GenericGraph.html", "original_name": "GenericGraph "}}
{"prompt": "Show a square grid of moving vertices on an axis.", "response": "from manim import *\n\nclass MovingVertices(Scene):\n    def construct(self):\n        vertices = [1, 2, 3, 4]\n        edges = [(1, 2), (2, 3), (3, 4), (1, 3), (1, 4)]\n        g = Graph(vertices, edges)\n        self.play(Create(g))\n        self.wait()\n        self.play(g[1].animate.move_to([1, 1, 0]),\n                  g[2].animate.move_to([-1, 1, 0]),\n                  g[3].animate.move_to([1, -1, 0]),\n                  g[4].animate.move_to([-1, -1, 0]))\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.graph.Graph.html", "original_name": "Graph "}}
{"prompt": "Show a graph with custom layout, displaying nodes in a staggered pattern across the screen.", "response": "from manim import *\n\nclass CustomLayoutExample(Scene):\n    def construct(self):\n        import numpy as np\n        import networkx as nx\n\n        # create custom layout\n        def custom_layout(\n            graph: nx.Graph,\n            scale: float | tuple[float, float, float] = 2,\n            n: int | None = None,\n            *args: Any,\n            **kwargs: Any,\n        ):\n            nodes = sorted(list(graph))\n            height = len(nodes) // n\n            return {\n                node: (scale * np.array([\n                    (i % n) - (n-1)/2,\n                    -(i // n) + height/2,\n                    0\n                ])) for i, node in enumerate(graph)\n            }\n\n        # draw graph\n        n = 4\n        graph = Graph(\n            [i for i in range(4 * 2 - 1)],\n            [(0, 1), (0, 4), (1, 2), (1, 5), (2, 3), (2, 6), (4, 5), (5, 6)],\n            labels=True,\n            layout=custom_layout,\n            layout_config={'n': n}\n        )\n        self.add(graph)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.graph.LayoutFunction.html", "original_name": "LayoutFunction "}}
{"prompt": "Create a visually striking dark-themed banner that expands and then disappears.", "response": "from manim import *\n\nclass DarkThemeBanner(Scene):\n    def construct(self):\n        banner = ManimBanner()\n        self.play(banner.create())\n        self.play(banner.expand())\n        self.wait()\n        self.play(Unwrite(banner))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.logo.ManimBanner.html", "original_name": "ManimBanner "}}
{"prompt": "Demonstrate a grid arrangement of matrices and Mobjects with various visual elements, including circles, squares, math symbols, and stars.", "response": "from manim import *\n\nclass MatrixExamples(Scene):\n    def construct(self):\n        m0 = Matrix([[\"\\\\pi\", 0], [-1, 1]])\n        m1 = IntegerMatrix([[1.5, 0.], [12, -1.3]],\n            left_bracket=\"(\",\n            right_bracket=\")\")\n        m2 = DecimalMatrix(\n            [[3.456, 2.122], [33.2244, 12.33]],\n            element_to_mobject_config={\"num_decimal_places\": 2},\n            left_bracket=r\"\\{\",\n            right_bracket=r\"\\}\")\n        m3 = MobjectMatrix(\n            [[Circle().scale(0.3), Square().scale(0.3)],\n            [MathTex(\"\\\\pi\").scale(2), Star().scale(0.3)]],\n            left_bracket=\"\\\\langle\",\n            right_bracket=\"\\\\rangle\")\n        g = Group(m0, m1, m2, m3).arrange_in_grid(buff=2)\n        self.add(g)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.matrix.html", "original_name": "matrix "}}
{"prompt": "Demonstrate a DecimalMatrix with rounded numbers using curly brackets and precise formatting.", "response": "from manim import *\n\nclass DecimalMatrixExample(Scene):\n    def construct(self):\n        m0 = DecimalMatrix(\n            [[3.456, 2.122], [33.2244, 12]],\n            element_to_mobject_config={\"num_decimal_places\": 2},\n            left_bracket=\"\\\\{\",\n            right_bracket=\"\\\\}\")\n        self.add(m0)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.matrix.DecimalMatrix.html", "original_name": "DecimalMatrix "}}
{"prompt": "Create a visual representation of an integer matrix with rounded numbers and boxed border.\n\nOr:\n\nShow an example of an integer matrix with precise decimal values in a bordered display.", "response": "from manim import *\n\nclass IntegerMatrixExample(Scene):\n    def construct(self):\n        m0 = IntegerMatrix(\n            [[3.7, 2], [42.2, 12]],\n            left_bracket=\"(\",\n            right_bracket=\")\")\n        self.add(m0)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.matrix.IntegerMatrix.html", "original_name": "IntegerMatrix "}}
{"prompt": "Create a grid of six matrices with various transformations and colorings, showcasing different alignments and brackets.", "response": "from manim import *\n\nclass MatrixExamples(Scene):\n    def construct(self):\n        m0 = Matrix([[2, r\"\\pi\"], [-1, 1]])\n        m1 = Matrix([[2, 0, 4], [-1, 1, 5]],\n            v_buff=1.3,\n            h_buff=0.8,\n            bracket_h_buff=SMALL_BUFF,\n            bracket_v_buff=SMALL_BUFF,\n            left_bracket=r\"\\{\",\n            right_bracket=r\"\\}\")\n        m1.add(SurroundingRectangle(m1.get_columns()[1]))\n        m2 = Matrix([[2, 1], [-1, 3]],\n            element_alignment_corner=UL,\n            left_bracket=\"(\",\n            right_bracket=\")\")\n        m3 = Matrix([[2, 1], [-1, 3]],\n            left_bracket=r\"\\langle\",\n            right_bracket=r\"\\rangle\")\n        m4 = Matrix([[2, 1], [-1, 3]],\n        ).set_column_colors(RED, GREEN)\n        m5 = Matrix([[2, 1], [-1, 3]],\n        ).set_row_colors(RED, GREEN)\n        g = Group(\n            m0,m1,m2,m3,m4,m5\n        ).arrange_in_grid(buff=2)\n        self.add(g)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.matrix.Matrix.html", "original_name": "Matrix "}}
{"prompt": "Demonstrate a 2x2 matrix layout with Circle, Square, and MathTex elements, using Manim's MobjectMatrix.", "response": "from manim import *\n\nclass MobjectMatrixExample(Scene):\n    def construct(self):\n        a = Circle().scale(0.3)\n        b = Square().scale(0.3)\n        c = MathTex(\"\\\\pi\").scale(2)\n        d = Star().scale(0.3)\n        m0 = MobjectMatrix([[a, b], [c, d]])\n        self.add(m0)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.matrix.MobjectMatrix.html", "original_name": "MobjectMatrix "}}
{"prompt": "Demonstrate creation of matrices with various visual elements, including integers, decimals, and custom shapes.", "response": "from manim import *\n\nclass MatrixExamples(Scene):\n    def construct(self):\n        m0 = Matrix([[\"\\\\pi\", 0], [-1, 1]])\n        m1 = IntegerMatrix([[1.5, 0.], [12, -1.3]],\n            left_bracket=\"(\",\n            right_bracket=\")\")\n        m2 = DecimalMatrix(\n            [[3.456, 2.122], [33.2244, 12.33]],\n            element_to_mobject_config={\"num_decimal_places\": 2},\n            left_bracket=r\"\\{\",\n            right_bracket=r\"\\}\")\n        m3 = MobjectMatrix(\n            [[Circle().scale(0.3), Square().scale(0.3)],\n            [MathTex(\"\\\\pi\").scale(2), Star().scale(0.3)]],\n            left_bracket=\"\\\\langle\",\n            right_bracket=\"\\\\rangle\")\n        g = Group(m0, m1, m2, m3).arrange_in_grid(buff=2)\n        self.add(g)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.matrix.html#manim.mobject.matrix.get_det_text", "original_name": "matrix "}}
{"prompt": "Demonstrate a group of matrices and 3D objects in a grid layout.", "response": "from manim import *\n\nclass MatrixExamples(Scene):\n    def construct(self):\n        m0 = Matrix([[\"\\\\pi\", 0], [-1, 1]])\n        m1 = IntegerMatrix([[1.5, 0.], [12, -1.3]],\n            left_bracket=\"(\",\n            right_bracket=\")\")\n        m2 = DecimalMatrix(\n            [[3.456, 2.122], [33.2244, 12.33]],\n            element_to_mobject_config={\"num_decimal_places\": 2},\n            left_bracket=r\"\\{\",\n            right_bracket=r\"\\}\")\n        m3 = MobjectMatrix(\n            [[Circle().scale(0.3), Square().scale(0.3)],\n            [MathTex(\"\\\\pi\").scale(2), Star().scale(0.3)]],\n            left_bracket=\"\\\\langle\",\n            right_bracket=\"\\\\rangle\")\n        g = Group(m0, m1, m2, m3).arrange_in_grid(buff=2)\n        self.add(g)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.matrix.html#manim.mobject.matrix.matrix_to_mobject", "original_name": "matrix "}}
{"prompt": "Create a visual representation of mathematical matrices with various formatting options and display in a grid layout.", "response": "from manim import *\n\nclass MatrixExamples(Scene):\n    def construct(self):\n        m0 = Matrix([[\"\\\\pi\", 0], [-1, 1]])\n        m1 = IntegerMatrix([[1.5, 0.], [12, -1.3]],\n            left_bracket=\"(\",\n            right_bracket=\")\")\n        m2 = DecimalMatrix(\n            [[3.456, 2.122], [33.2244, 12.33]],\n            element_to_mobject_config={\"num_decimal_places\": 2},\n            left_bracket=r\"\\{\",\n            right_bracket=r\"\\}\")\n        m3 = MobjectMatrix(\n            [[Circle().scale(0.3), Square().scale(0.3)],\n            [MathTex(\"\\\\pi\").scale(2), Star().scale(0.3)]],\n            left_bracket=\"\\\\langle\",\n            right_bracket=\"\\\\rangle\")\n        g = Group(m0, m1, m2, m3).arrange_in_grid(buff=2)\n        self.add(g)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.matrix.html#manim.mobject.matrix.matrix_to_tex_string", "original_name": "matrix "}}
{"prompt": "Create a circle with text content and demonstrate its clear animation, revealing a blank space.", "response": "from manim import *\n\nclass CircleWithContent(VGroup):\n    def __init__(self, content):\n        super().__init__()\n        self.circle = Circle()\n        self.content = content\n        self.add(self.circle, content)\n        content.move_to(self.circle.get_center())\n\n    def clear_content(self):\n        self.remove(self.content)\n        self.content = None\n\n    @override_animate(clear_content)\n    def _clear_content_animation(self, anim_args=None):\n        if anim_args is None:\n            anim_args = {}\n        anim = Uncreate(self.content, **anim_args)\n        self.clear_content()\n        return anim\n\nclass AnimationOverrideExample(Scene):\n    def construct(self):\n        t = Text(\"hello!\")\n        my_mobject = CircleWithContent(t)\n        self.play(Create(my_mobject))\n        self.play(my_mobject.animate.clear_content())\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.mobject.html", "original_name": "mobject "}}
{"prompt": "Create a circle with animated text content, demonstrating circle creation and content removal.", "response": "from manim import *\n\nclass CircleWithContent(VGroup):\n    def __init__(self, content):\n        super().__init__()\n        self.circle = Circle()\n        self.content = content\n        self.add(self.circle, content)\n        content.move_to(self.circle.get_center())\n\n    def clear_content(self):\n        self.remove(self.content)\n        self.content = None\n\n    @override_animate(clear_content)\n    def _clear_content_animation(self, anim_args=None):\n        if anim_args is None:\n            anim_args = {}\n        anim = Uncreate(self.content, **anim_args)\n        self.clear_content()\n        return anim\n\nclass AnimationOverrideExample(Scene):\n    def construct(self):\n        t = Text(\"hello!\")\n        my_mobject = CircleWithContent(t)\n        self.play(Create(my_mobject))\n        self.play(my_mobject.animate.clear_content())\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.mobject.html#manim.mobject.mobject.TimeBasedUpdater", "original_name": "mobject "}}
{"prompt": "Create a visual example demonstrating the creation and removal of animated circle with content, showcasing circle transformations and text overlay.", "response": "from manim import *\n\nclass CircleWithContent(VGroup):\n    def __init__(self, content):\n        super().__init__()\n        self.circle = Circle()\n        self.content = content\n        self.add(self.circle, content)\n        content.move_to(self.circle.get_center())\n\n    def clear_content(self):\n        self.remove(self.content)\n        self.content = None\n\n    @override_animate(clear_content)\n    def _clear_content_animation(self, anim_args=None):\n        if anim_args is None:\n            anim_args = {}\n        anim = Uncreate(self.content, **anim_args)\n        self.clear_content()\n        return anim\n\nclass AnimationOverrideExample(Scene):\n    def construct(self):\n        t = Text(\"hello!\")\n        my_mobject = CircleWithContent(t)\n        self.play(Create(my_mobject))\n        self.play(my_mobject.animate.clear_content())\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.mobject.html#manim.mobject.mobject.NonTimeBasedUpdater", "original_name": "mobject "}}
{"prompt": "\"Show a circle with text content that fades away over time.\"", "response": "from manim import *\n\nclass CircleWithContent(VGroup):\n    def __init__(self, content):\n        super().__init__()\n        self.circle = Circle()\n        self.content = content\n        self.add(self.circle, content)\n        content.move_to(self.circle.get_center())\n\n    def clear_content(self):\n        self.remove(self.content)\n        self.content = None\n\n    @override_animate(clear_content)\n    def _clear_content_animation(self, anim_args=None):\n        if anim_args is None:\n            anim_args = {}\n        anim = Uncreate(self.content, **anim_args)\n        self.clear_content()\n        return anim\n\nclass AnimationOverrideExample(Scene):\n    def construct(self):\n        t = Text(\"hello!\")\n        my_mobject = CircleWithContent(t)\n        self.play(Create(my_mobject))\n        self.play(my_mobject.animate.clear_content())\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.mobject.html#manim.mobject.mobject.Updater", "original_name": "mobject "}}
{"prompt": "Create an animated dot that rotates around its center point.", "response": "from manim import *\n\nclass NextToUpdater(Scene):\n    def construct(self):\n        def dot_position(mobject):\n            mobject.set_value(dot.get_center()[0])\n            mobject.next_to(dot)\n\n        dot = Dot(RIGHT*3)\n        label = DecimalNumber()\n        label.add_updater(dot_position)\n        self.add(dot, label)\n\n        self.play(Rotating(dot, about_point=ORIGIN, angle=TAU, run_time=TAU, rate_func=linear))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.mobject.Mobject.html", "original_name": "Mobject "}}
{"prompt": "\"Create and animate a circle with content, demonstrating clear animation and transformation over time.\"", "response": "from manim import *\n\nclass CircleWithContent(VGroup):\n    def __init__(self, content):\n        super().__init__()\n        self.circle = Circle()\n        self.content = content\n        self.add(self.circle, content)\n        content.move_to(self.circle.get_center())\n\n    def clear_content(self):\n        self.remove(self.content)\n        self.content = None\n\n    @override_animate(clear_content)\n    def _clear_content_animation(self, anim_args=None):\n        if anim_args is None:\n            anim_args = {}\n        anim = Uncreate(self.content, **anim_args)\n        self.clear_content()\n        return anim\n\nclass AnimationOverrideExample(Scene):\n    def construct(self):\n        t = Text(\"hello!\")\n        my_mobject = CircleWithContent(t)\n        self.play(Create(my_mobject))\n        self.play(my_mobject.animate.clear_content())\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.mobject.html#manim.mobject.mobject.override_animate", "original_name": "mobject "}}
{"prompt": "Demonstrate a visual comparison of two tables with different data types and layouts, featuring highlighted cells, color-coded lines, and interactive crosshairs.", "response": "from manim import *\n\nclass TableExamples(Scene):\n    def construct(self):\n        t0 = Table(\n            [[\"First\", \"Second\"],\n            [\"Third\",\"Fourth\"]],\n            row_labels=[Text(\"R1\"), Text(\"R2\")],\n            col_labels=[Text(\"C1\"), Text(\"C2\")],\n            top_left_entry=Text(\"TOP\"))\n        t0.add_highlighted_cell((2,2), color=GREEN)\n        x_vals = np.linspace(-2,2,5)\n        y_vals = np.exp(x_vals)\n        t1 = DecimalTable(\n            [x_vals, y_vals],\n            row_labels=[MathTex(\"x\"), MathTex(\"f(x)\")],\n            include_outer_lines=True)\n        t1.add(t1.get_cell((2,2), color=RED))\n        t2 = MathTable(\n            [[\"+\", 0, 5, 10],\n            [0, 0, 5, 10],\n            [2, 2, 7, 12],\n            [4, 4, 9, 14]],\n            include_outer_lines=True)\n        t2.get_horizontal_lines()[:3].set_color(BLUE)\n        t2.get_vertical_lines()[:3].set_color(BLUE)\n        t2.get_horizontal_lines()[:3].set_z_index(1)\n        cross = VGroup(\n            Line(UP + LEFT, DOWN + RIGHT),\n            Line(UP + RIGHT, DOWN + LEFT))\n        a = Circle().set_color(RED).scale(0.5)\n        b = cross.set_color(BLUE).scale(0.5)\n        t3 = MobjectTable(\n            [[a.copy(),b.copy(),a.copy()],\n            [b.copy(),a.copy(),a.copy()],\n            [a.copy(),b.copy(),b.copy()]])\n        t3.add(Line(\n            t3.get_corner(DL), t3.get_corner(UR)\n        ).set_color(RED))\n        vals = np.arange(1,21).reshape(5,4)\n        t4 = IntegerTable(\n            vals,\n            include_outer_lines=True\n        )\n        g1 = Group(t0, t1).scale(0.5).arrange(buff=1).to_edge(UP, buff=1)\n        g2 = Group(t2, t3, t4).scale(0.5).arrange(buff=1).to_edge(DOWN, buff=1)\n        self.add(g1, g2)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.table.html", "original_name": "table "}}
{"prompt": "Create a decimal table displaying the exponential function f(x) = e^x, with rounded values.", "response": "from manim import *\n\nclass DecimalTableExample(Scene):\n    def construct(self):\n        x_vals = [-2,-1,0,1,2]\n        y_vals = np.exp(x_vals)\n        t0 = DecimalTable(\n            [x_vals, y_vals],\n            row_labels=[MathTex(\"x\"), MathTex(\"f(x)=e^{x}\")],\n            h_buff=1,\n            element_to_mobject_config={\"num_decimal_places\": 2})\n        self.add(t0)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.table.DecimalTable.html", "original_name": "DecimalTable "}}
{"prompt": "Demonstrate a rotating table showing angles and trigonometric values from 0 to \u03c0/2 radians.", "response": "from manim import *\n\nclass IntegerTableExample(Scene):\n    def construct(self):\n        t0 = IntegerTable(\n            [[0,30,45,60,90],\n            [90,60,45,30,0]],\n            col_labels=[\n                MathTex(r\"\\frac{\\sqrt{0}}{2}\"),\n                MathTex(r\"\\frac{\\sqrt{1}}{2}\"),\n                MathTex(r\"\\frac{\\sqrt{2}}{2}\"),\n                MathTex(r\"\\frac{\\sqrt{3}}{2}\"),\n                MathTex(r\"\\frac{\\sqrt{4}}{2}\")],\n            row_labels=[MathTex(r\"\\sin\"), MathTex(r\"\\cos\")],\n            h_buff=1,\n            element_to_mobject_config={\"unit\": r\"^{\\circ}\"})\n        self.add(t0)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.table.IntegerTable.html", "original_name": "IntegerTable "}}
{"prompt": "Demonstrate a static math table with various visual elements, including rows and columns of numbers.", "response": "from manim import *\n\nclass MathTableExample(Scene):\n    def construct(self):\n        t0 = MathTable(\n            [[\"+\", 0, 5, 10],\n            [0, 0, 5, 10],\n            [2, 2, 7, 12],\n            [4, 4, 9, 14]],\n            include_outer_lines=True)\n        self.add(t0)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.table.MathTable.html", "original_name": "MathTable "}}
{"prompt": "Create a rotating table with colored circles that demonstrate transformation and alignment of MobjectTable.", "response": "from manim import *\n\nclass MobjectTableExample(Scene):\n    def construct(self):\n        cross = VGroup(\n            Line(UP + LEFT, DOWN + RIGHT),\n            Line(UP + RIGHT, DOWN + LEFT),\n        )\n        a = Circle().set_color(RED).scale(0.5)\n        b = cross.set_color(BLUE).scale(0.5)\n        t0 = MobjectTable(\n            [[a.copy(),b.copy(),a.copy()],\n            [b.copy(),a.copy(),a.copy()],\n            [a.copy(),b.copy(),b.copy()]]\n        )\n        line = Line(\n            t0.get_corner(DL), t0.get_corner(UR)\n        ).set_color(RED)\n        self.add(t0, line)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.table.MobjectTable.html", "original_name": "MobjectTable "}}
{"prompt": "Demonstrate the creation of multiple tables with various visual elements, including row and column labels, highlighted cells, and custom line styles.", "response": "from manim import *\n\nclass TableExamples(Scene):\n    def construct(self):\n        t0 = Table(\n            [[\"This\", \"is a\"],\n            [\"simple\", \"Table in \\\\n Manim.\"]])\n        t1 = Table(\n            [[\"This\", \"is a\"],\n            [\"simple\", \"Table.\"]],\n            row_labels=[Text(\"R1\"), Text(\"R2\")],\n            col_labels=[Text(\"C1\"), Text(\"C2\")])\n        t1.add_highlighted_cell((2,2), color=YELLOW)\n        t2 = Table(\n            [[\"This\", \"is a\"],\n            [\"simple\", \"Table.\"]],\n            row_labels=[Text(\"R1\"), Text(\"R2\")],\n            col_labels=[Text(\"C1\"), Text(\"C2\")],\n            top_left_entry=Star().scale(0.3),\n            include_outer_lines=True,\n            arrange_in_grid_config={\"cell_alignment\": RIGHT})\n        t2.add(t2.get_cell((2,2), color=RED))\n        t3 = Table(\n            [[\"This\", \"is a\"],\n            [\"simple\", \"Table.\"]],\n            row_labels=[Text(\"R1\"), Text(\"R2\")],\n            col_labels=[Text(\"C1\"), Text(\"C2\")],\n            top_left_entry=Star().scale(0.3),\n            include_outer_lines=True,\n            line_config={\"stroke_width\": 1, \"color\": YELLOW})\n        t3.remove(*t3.get_vertical_lines())\n        g = Group(\n            t0,t1,t2,t3\n        ).scale(0.7).arrange_in_grid(buff=1)\n        self.add(g)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.table.Table.html", "original_name": "Table "}}
{"prompt": "Demonstrate movement of a dot on the complex plane in response to value updates.", "response": "from manim import *\n\nclass ComplexValueTrackerExample(Scene):\n    def construct(self):\n        tracker = ComplexValueTracker(-2+1j)\n        dot = Dot().add_updater(\n            lambda x: x.move_to(tracker.points)\n        )\n\n        self.add(NumberPlane(), dot)\n\n        self.play(tracker.animate.set_value(3+2j))\n        self.play(tracker.animate.set_value(tracker.get_value() * 1j))\n        self.play(tracker.animate.set_value(tracker.get_value() - 2j))\n        self.play(tracker.animate.set_value(tracker.get_value() / (-2 + 3j)))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.value_tracker.ComplexValueTracker.html", "original_name": "ComplexValueTracker "}}
{"prompt": "Show a dynamic number line with an animated pointer, tracking changes to its position, and updating the label accordingly.", "response": "from manim import *\n\nclass ValueTrackerExample(Scene):\n    def construct(self):\n        number_line = NumberLine()\n        pointer = Vector(DOWN)\n        label = MathTex(\"x\").add_updater(lambda m: m.next_to(pointer, UP))\n\n        tracker = ValueTracker(0)\n        pointer.add_updater(\n            lambda m: m.next_to(\n                        number_line.n2p(tracker.get_value()),\n                        UP\n                    )\n        )\n        self.add(number_line, pointer,label)\n        tracker += 1.5\n        self.wait(1)\n        tracker -= 4\n        self.wait(0.5)\n        self.play(tracker.animate.set_value(5))\n        self.wait(0.5)\n        self.play(tracker.animate.set_value(3))\n        self.play(tracker.animate.increment_value(-2))\n        self.wait(0.5)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.value_tracker.ValueTracker.html", "original_name": "ValueTracker "}}
{"prompt": "Demonstrate a mathematical function as an arrow field, mapping 2D space to a 2D vector.", "response": "from manim import *\n\nclass BasicUsage(Scene):\n    def construct(self):\n        func = lambda pos: ((pos[0] * UR + pos[1] * LEFT) - pos) / 3\n        self.add(ArrowVectorField(func))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.vector_field.ArrowVectorField.html", "original_name": "ArrowVectorField "}}
{"prompt": "Demonstrate a flowing stream of lines that smoothly move and curve in a diagonal path.", "response": "from manim import *\n\nclass BasicUsage(Scene):\n    def construct(self):\n        func = lambda pos: ((pos[0] * UR + pos[1] * LEFT) - pos) / 3\n        self.add(StreamLines(func))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.vector_field.StreamLines.html", "original_name": "StreamLines "}}
{"prompt": "Demonstrate a 3D vector field with arrows nudging a gray circle and a white dot in sync.", "response": "from manim import *\n\nclass Nudging(Scene):\n    def construct(self):\n        func = lambda pos: np.sin(pos[1] / 2) * RIGHT + np.cos(pos[0] / 2) * UP\n        vector_field = ArrowVectorField(\n            func, x_range=[-7, 7, 1], y_range=[-4, 4, 1], length_func=lambda x: x / 2\n        )\n        self.add(vector_field)\n        circle = Circle(radius=2).shift(LEFT)\n        self.add(circle.copy().set_color(GRAY))\n        dot = Dot().move_to(circle)\n\n        vector_field.nudge(circle, -2, 60, True)\n        vector_field.nudge(dot, -2, 60)\n\n        circle.add_updater(vector_field.get_nudge_updater(pointwise=True))\n        dot.add_updater(vector_field.get_nudge_updater())\n        self.add(circle, dot)\n        self.wait(6)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.mobject.vector_field.VectorField.html", "original_name": "VectorField "}}
{"prompt": "Create a camera animation that scales its width while restoring the original frame size.", "response": "from manim import *\n\nclass ChangingCameraWidthAndRestore(MovingCameraScene):\n    def construct(self):\n        text = Text(\"Hello World\").set_color(BLUE)\n        self.add(text)\n        self.camera.frame.save_state()\n        self.play(self.camera.frame.animate.set(width=text.width * 1.2))\n        self.wait(0.3)\n        self.play(Restore(self.camera.frame))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.scene.moving_camera_scene.html", "original_name": "moving_camera_scene "}}
{"prompt": "Show a \"Hello World!\" text appearing on the screen.", "response": "class MyScene(Scene):\n    def construct(self):\n        self.play(Write(Text(\"Hello World!\")))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.scene.scene.Scene.html", "original_name": "Scene "}}
{"prompt": "Demonstrate a linear transformation on a 2D grid, rotating and scaling a vector by the given matrix.", "response": "from manim import *\n\nclass LinearTransformationSceneExample(LinearTransformationScene):\n    def __init__(self, **kwargs):\n        LinearTransformationScene.__init__(\n            self,\n            show_coordinates=True,\n            leave_ghost_vectors=True,\n            **kwargs\n        )\n\n    def construct(self):\n        matrix = [[1, 1], [0, 1]]\n        self.apply_matrix(matrix)\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.scene.vector_space_scene.LinearTransformationScene.html", "original_name": "LinearTransformationScene "}}
{"prompt": "Create a zoomed scene that starts with a green dot, then suddenly moves to the left.", "response": "from manim import *\n\nclass UseZoomedScene(ZoomedScene):\n    def construct(self):\n        dot = Dot().set_color(GREEN)\n        self.add(dot)\n        self.wait(1)\n        self.activate_zooming(animate=False)\n        self.wait(1)\n        self.play(dot.animate.shift(LEFT))", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.scene.zoomed_scene.html", "original_name": "zoomed_scene "}}
{"prompt": "Demonstrate mathematical formula with color-coded indices and highlighted terms using animated MathTex and index labels.", "response": "from manim import *\n\nclass IndexLabelsExample(Scene):\n    def construct(self):\n        text = MathTex(\n            \"\\\\frac{d}{dx}f(x)g(x)=\",\n            \"f(x)\\\\frac{d}{dx}g(x)\",\n            \"+\",\n            \"g(x)\\\\frac{d}{dx}f(x)\",\n        )\n\n        #index the fist term in the MathTex mob\n        indices = index_labels(text[0])\n\n        text[0][1].set_color(PURPLE_B)\n        text[0][8:12].set_color(DARK_BLUE)\n\n        self.add(text, indices)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.utils.debug.html", "original_name": "debug "}}
{"prompt": "Demonstrate a clockwise transformation of four colored dots along their circular paths.", "response": "from manim import *\n\nclass ClockwisePathExample(Scene):\n    def construct(self):\n        colors = [RED, GREEN, BLUE]\n\n        starting_points = VGroup(\n            *[\n                Dot(LEFT + pos, color=color)\n                for pos, color in zip([UP, DOWN, LEFT], colors)\n            ]\n        )\n\n        finish_points = VGroup(\n            *[\n                Dot(RIGHT + pos, color=color)\n                for pos, color in zip([ORIGIN, UP, DOWN], colors)\n            ]\n        )\n\n        self.add(starting_points)\n        self.add(finish_points)\n        for dot in starting_points:\n            self.add(TracedPath(dot.get_center, stroke_color=dot.get_color()))\n\n        self.wait()\n        self.play(\n            Transform(\n                starting_points,\n                finish_points,\n                path_func=utils.paths.clockwise_path(),\n                run_time=2,\n            )\n        )\n        self.wait()", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.utils.paths.html", "original_name": "paths "}}
{"prompt": "Demonstrate the plotting of various rate functions in a grid layout with titles.", "response": "from manim import *\n\nclass RateFuncExample(Scene):\n    def construct(self):\n        x = VGroup()\n        for k, v in rate_functions.__dict__.items():\n            if \"function\" in str(v):\n                if (\n                    not k.startswith(\"__\")\n                    and not k.startswith(\"sqrt\")\n                    and not k.startswith(\"bezier\")\n                ):\n                    try:\n                        rate_func = v\n                        plot = (\n                            ParametricFunction(\n                                lambda x: [x, rate_func(x), 0],\n                                t_range=[0, 1, .01],\n                                use_smoothing=False,\n                                color=YELLOW,\n                            )\n                            .stretch_to_fit_width(1.5)\n                            .stretch_to_fit_height(1)\n                        )\n                        plot_bg = SurroundingRectangle(plot).set_color(WHITE)\n                        plot_title = (\n                            Text(rate_func.__name__, weight=BOLD)\n                            .scale(0.5)\n                            .next_to(plot_bg, UP, buff=0.1)\n                        )\n                        x.add(VGroup(plot_bg, plot, plot_title))\n                    except: # because functions `not_quite_there`, `function squish_rate_func` are not working.\n                        pass\n        x.arrange_in_grid(cols=8)\n        x.height = config.frame_height\n        x.width = config.frame_width\n        x.move_to(ORIGIN).scale(0.95)\n        self.add(x)", "metadata": {"source": "https://docs.manim.community/en/stable/reference/manim.utils.rate_functions.html", "original_name": "rate_functions "}}
{"prompt": "Show a colorful logo animation consisting of a triangle, square, circle, and math symbol, arranged in a specific order, with smooth transitions.", "response": "class ManimCELogo(Scene):\n    def construct(self):\n        self.camera.background_color = \"#ece6e2\"\n        logo_green = \"#87c2a5\"\n        logo_blue = \"#525893\"\n        logo_red = \"#e07a5f\"\n        logo_black = \"#343434\"\n        ds_m = MathTex(r\"\\mathbb{M}\", fill_color=logo_black).scale(7)\n        ds_m.shift(2.25 * LEFT + 1.5 * UP)\n        circle = Circle(color=logo_green, fill_opacity=1).shift(LEFT)\n        square = Square(color=logo_blue, fill_opacity=1).shift(UP)\n        triangle = Triangle(color=logo_red, fill_opacity=1).shift(RIGHT)\n        logo = VGroup(triangle, square, circle, ds_m)  # order matters\n        logo.move_to(ORIGIN)\n        self.add(logo)", "original_prompt": "ManimCELogo"}
{"prompt": "Show a diagram with an orange line, two dots, and arrows labeled \"Horizontal distance\" and \"x-x_1\".", "response": "class BraceAnnotation(Scene):\n    def construct(self):\n        dot = Dot([-2, -1, 0])\n        dot2 = Dot([2, 1, 0])\n        line = Line(dot.get_center(), dot2.get_center()).set_color(ORANGE)\n        b1 = Brace(line)\n        b1text = b1.get_text(\"Horizontal distance\")\n        b2 = Brace(line, direction=line.copy().rotate(PI / 2).get_unit_vector())\n        b2text = b2.get_tex(\"x-x_1\")\n        self.add(line, dot, dot2, b1, b2, b1text, b2text)", "original_prompt": "BraceAnnotation"}
{"prompt": "Create a diagram showing an arrow with coordinates on a number plane.", "response": "class VectorArrow(Scene):\n    def construct(self):\n        dot = Dot(ORIGIN)\n        arrow = Arrow(ORIGIN, [2, 2, 0], buff=0)\n        numberplane = NumberPlane()\n        origin_text = Text('(0, 0)').next_to(dot, DOWN)\n        tip_text = Text('(2, 2)').next_to(arrow.get_end(), RIGHT)\n        self.add(numberplane, dot, arrow, origin_text, tip_text)", "original_prompt": "VectorArrow"}
{"prompt": "Create a large gradient image by scaling and coloring a 2D array of pixel values.", "response": "class GradientImageFromArray(Scene):\n    def construct(self):\n        n = 256\n        imageArray = np.uint8(\n            [[i * 256 / n for i in range(0, n)] for _ in range(0, n)]\n        )\n        image = ImageMobject(imageArray).scale(2)\n        image.background_rectangle = SurroundingRectangle(image, color=GREEN)\n        self.add(image, image.background_rectangle)", "original_prompt": "GradientImageFromArray"}
{"prompt": "Demonstrate visualizations of basic Boolean operations (intersection, union, exclusion, difference) between two overlapping ellipses.", "response": "class BooleanOperations(Scene):\n    def construct(self):\n        ellipse1 = Ellipse(\n            width=4.0, height=5.0, fill_opacity=0.5, color=BLUE, stroke_width=10\n        ).move_to(LEFT)\n        ellipse2 = ellipse1.copy().set_color(color=RED).move_to(RIGHT)\n        bool_ops_text = MarkupText(\"Boolean Operation\").next_to(ellipse1, UP * 3)\n        ellipse_group = Group(bool_ops_text, ellipse1, ellipse2).move_to(LEFT * 3)\n        self.play(FadeIn(ellipse_group))\n\n        i = Intersection(ellipse1, ellipse2, color=GREEN, fill_opacity=0.5)\n        self.play(i.animate.scale(0.25).move_to(RIGHT * 5 + UP * 2.5))\n        intersection_text = Text(\"Intersection\", font_size=23).next_to(i, UP)\n        self.play(FadeIn(intersection_text))\n\n        u = Union(ellipse1, ellipse2, color=ORANGE, fill_opacity=0.5)\n        union_text = Text(\"Union\", font_size=23)\n        self.play(u.animate.scale(0.3).next_to(i, DOWN, buff=union_text.height * 3))\n        union_text.next_to(u, UP)\n        self.play(FadeIn(union_text))\n\n        e = Exclusion(ellipse1, ellipse2, color=YELLOW, fill_opacity=0.5)\n        exclusion_text = Text(\"Exclusion\", font_size=23)\n        self.play(e.animate.scale(0.3).next_to(u, DOWN, buff=exclusion_text.height * 3.5))\n        exclusion_text.next_to(e, UP)\n        self.play(FadeIn(exclusion_text))\n\n        d = Difference(ellipse1, ellipse2, color=PINK, fill_opacity=0.5)\n        difference_text = Text(\"Difference\", font_size=23)\n        self.play(d.animate.scale(0.3).next_to(u, LEFT, buff=difference_text.height * 3.5))\n        difference_text.next_to(d, UP)\n        self.play(FadeIn(difference_text))", "original_prompt": "BooleanOperations"}
{"prompt": "Show how a blue dot moves and transforms along a growing red circle on the screen.", "response": "class PointMovingOnShapes(Scene):\n    def construct(self):\n        circle = Circle(radius=1, color=BLUE)\n        dot = Dot()\n        dot2 = dot.copy().shift(RIGHT)\n        self.add(dot)\n\n        line = Line([3, 0, 0], [5, 0, 0])\n        self.add(line)\n\n        self.play(GrowFromCenter(circle))\n        self.play(Transform(dot, dot2))\n        self.play(MoveAlongPath(dot, circle), run_time=2, rate_func=linear)\n        self.play(Rotating(dot, about_point=[2, 0, 0]), run_time=1.5)\n        self.wait()", "original_prompt": "PointMovingOnShapes"}
{"prompt": "Demonstrate a square moving around the screen with transformations, including shifting, filling color change, scaling down, and rotation.", "response": "class MovingAround(Scene):\n    def construct(self):\n        square = Square(color=BLUE, fill_opacity=1)\n\n        self.play(square.animate.shift(LEFT))\n        self.play(square.animate.set_fill(ORANGE))\n        self.play(square.animate.scale(0.3))\n        self.play(square.animate.rotate(0.4))", "original_prompt": "MovingAround"}
{"prompt": "Demonstrate a rotating line with changing angle, color, and position on the screen.", "response": "class MovingAngle(Scene):\n    def construct(self):\n        rotation_center = LEFT\n\n        theta_tracker = ValueTracker(110)\n        line1 = Line(LEFT, RIGHT)\n        line_moving = Line(LEFT, RIGHT)\n        line_ref = line_moving.copy()\n        line_moving.rotate(\n            theta_tracker.get_value() * DEGREES, about_point=rotation_center\n        )\n        a = Angle(line1, line_moving, radius=0.5, other_angle=False)\n        tex = MathTex(r\"\\theta\").move_to(\n            Angle(\n                line1, line_moving, radius=0.5 + 3 * SMALL_BUFF, other_angle=False\n            ).point_from_proportion(0.5)\n        )\n\n        self.add(line1, line_moving, a, tex)\n        self.wait()\n\n        line_moving.add_updater(\n            lambda x: x.become(line_ref.copy()).rotate(\n                theta_tracker.get_value() * DEGREES, about_point=rotation_center\n            )\n        )\n\n        a.add_updater(\n            lambda x: x.become(Angle(line1, line_moving, radius=0.5, other_angle=False))\n        )\n        tex.add_updater(\n            lambda x: x.move_to(\n                Angle(\n                    line1, line_moving, radius=0.5 + 3 * SMALL_BUFF, other_angle=False\n                ).point_from_proportion(0.5)\n            )\n        )\n\n        self.play(theta_tracker.animate.set_value(40))\n        self.play(theta_tracker.animate.increment_value(140))\n        self.play(tex.animate.set_color(RED), run_time=0.5)\n        self.play(theta_tracker.animate.set_value(350))", "original_prompt": "MovingAngle"}
{"prompt": "Demonstrate moving dots with a red line connecting them, updating positions over time.", "response": "class MovingDots(Scene):\n    def construct(self):\n        d1,d2=Dot(color=BLUE),Dot(color=GREEN)\n        dg=VGroup(d1,d2).arrange(RIGHT,buff=1)\n        l1=Line(d1.get_center(),d2.get_center()).set_color(RED)\n        x=ValueTracker(0)\n        y=ValueTracker(0)\n        d1.add_updater(lambda z: z.set_x(x.get_value()))\n        d2.add_updater(lambda z: z.set_y(y.get_value()))\n        l1.add_updater(lambda z: z.become(Line(d1.get_center(),d2.get_center())))\n        self.add(d1,d2,l1)\n        self.play(x.animate.set_value(5))\n        self.play(y.animate.set_value(4))\n        self.wait()", "original_prompt": "MovingDots"}
{"prompt": "\"Create a moving dot group that smoothly transitions to a target yellow dot.\"", "response": "class MovingGroupToDestination(Scene):\n    def construct(self):\n        group = VGroup(Dot(LEFT), Dot(ORIGIN), Dot(RIGHT, color=RED), Dot(2 * RIGHT)).scale(1.4)\n        dest = Dot([4, 3, 0], color=YELLOW)\n        self.add(group, dest)\n        self.play(group.animate.shift(dest.get_center() - group[2].get_center()))\n        self.wait(0.5)", "original_prompt": "MovingGroupToDestination"}
{"prompt": "Create a moving animation of a formula with two boxes that transform into each other.", "response": "class MovingFrameBox(Scene):\n    def construct(self):\n        text=MathTex(\n            \"\\\\frac{d}{dx}f(x)g(x)=\",\"f(x)\\\\frac{d}{dx}g(x)\",\"+\",\n            \"g(x)\\\\frac{d}{dx}f(x)\"\n        )\n        self.play(Write(text))\n        framebox1 = SurroundingRectangle(text[1], buff = .1)\n        framebox2 = SurroundingRectangle(text[3], buff = .1)\n        self.play(\n            Create(framebox1),\n        )\n        self.wait()\n        self.play(\n            ReplacementTransform(framebox1,framebox2),\n        )\n        self.wait()", "original_prompt": "MovingFrameBox"}
{"prompt": "Demonstrate a yellow line rotating around the origin with two animations, moving forward and backward.", "response": "class RotationUpdater(Scene):\n    def construct(self):\n        def updater_forth(mobj, dt):\n            mobj.rotate_about_origin(dt)\n        def updater_back(mobj, dt):\n            mobj.rotate_about_origin(-dt)\n        line_reference = Line(ORIGIN, LEFT).set_color(WHITE)\n        line_moving = Line(ORIGIN, LEFT).set_color(YELLOW)\n        line_moving.add_updater(updater_forth)\n        self.add(line_reference, line_moving)\n        self.wait(2)\n        line_moving.remove_updater(updater_forth)\n        line_moving.add_updater(updater_back)\n        self.wait(2)\n        line_moving.remove_updater(updater_back)\n        self.wait(0.5)", "original_prompt": "RotationUpdater"}
{"prompt": "Show a red dot rotating and moving up and left in a circular motion.", "response": "class PointWithTrace(Scene):\n    def construct(self):\n        path = VMobject()\n        dot = Dot()\n        path.set_points_as_corners([dot.get_center(), dot.get_center()])\n        def update_path(path):\n            previous_path = path.copy()\n            previous_path.add_points_as_corners([dot.get_center()])\n            path.become(previous_path)\n        path.add_updater(update_path)\n        self.add(path, dot)\n        self.play(Rotating(dot, radians=PI, about_point=RIGHT, run_time=2))\n        self.wait()\n        self.play(dot.animate.shift(UP))\n        self.play(dot.animate.shift(LEFT))\n        self.wait()", "original_prompt": "PointWithTrace"}
{"prompt": "Demonstrate a sinusoidal function plot with labeled axes and vertical line highlighting key points.", "response": "class SinAndCosFunctionPlot(Scene):\n    def construct(self):\n        axes = Axes(\n            x_range=[-10, 10.3, 1],\n            y_range=[-1.5, 1.5, 1],\n            x_length=10,\n            axis_config={\"color\": GREEN},\n            x_axis_config={\n                \"numbers_to_include\": np.arange(-10, 10.01, 2),\n                \"numbers_with_elongated_ticks\": np.arange(-10, 10.01, 2),\n            },\n            tips=False,\n        )\n        axes_labels = axes.get_axis_labels()\n        sin_graph = axes.plot(lambda x: np.sin(x), color=BLUE)\n        cos_graph = axes.plot(lambda x: np.cos(x), color=RED)\n\n        sin_label = axes.get_graph_label(\n            sin_graph, \"\\\\sin(x)\", x_val=-10, direction=UP / 2\n        )\n        cos_label = axes.get_graph_label(cos_graph, label=\"\\\\cos(x)\")\n\n        vert_line = axes.get_vertical_line(\n            axes.i2gp(TAU, cos_graph), color=YELLOW, line_func=Line\n        )\n        line_label = axes.get_graph_label(\n            cos_graph, r\"x=2\\pi\", x_val=TAU, direction=UR, color=WHITE\n        )\n\n        plot = VGroup(axes, sin_graph, cos_graph, vert_line)\n        labels = VGroup(axes_labels, sin_label, cos_label, line_label)\n        self.add(plot, labels)", "original_prompt": "SinAndCosFunctionPlot"}
{"prompt": "Demonstrate the minimization of a quadratic function by tracking a moving point along its curve and highlighting the minimum value.", "response": "class ArgMinExample(Scene):\n    def construct(self):\n        ax = Axes(\n            x_range=[0, 10], y_range=[0, 100, 10], axis_config={\"include_tip\": False}\n        )\n        labels = ax.get_axis_labels(x_label=\"x\", y_label=\"f(x)\")\n\n        t = ValueTracker(0)\n\n        def func(x):\n            return 2 * (x - 5) ** 2\n        graph = ax.plot(func, color=MAROON)\n\n        initial_point = [ax.coords_to_point(t.get_value(), func(t.get_value()))]\n        dot = Dot(point=initial_point)\n\n        dot.add_updater(lambda x: x.move_to(ax.c2p(t.get_value(), func(t.get_value()))))\n        x_space = np.linspace(*ax.x_range[:2],200)\n        minimum_index = func(x_space).argmin()\n\n        self.add(ax, labels, graph, dot)\n        self.play(t.animate.set_value(x_space[minimum_index]))\n        self.wait()", "original_prompt": "ArgMinExample"}
{"prompt": "Demonstrate a mathematical graph with two curves and a grey shaded area between them.", "response": "class GraphAreaPlot(Scene):\n    def construct(self):\n        ax = Axes(\n            x_range=[0, 5],\n            y_range=[0, 6],\n            x_axis_config={\"numbers_to_include\": [2, 3]},\n            tips=False,\n        )\n\n        labels = ax.get_axis_labels()\n\n        curve_1 = ax.plot(lambda x: 4 * x - x ** 2, x_range=[0, 4], color=BLUE_C)\n        curve_2 = ax.plot(\n            lambda x: 0.8 * x ** 2 - 3 * x + 4,\n            x_range=[0, 4],\n            color=GREEN_B,\n        )\n\n        line_1 = ax.get_vertical_line(ax.input_to_graph_point(2, curve_1), color=YELLOW)\n        line_2 = ax.get_vertical_line(ax.i2gp(3, curve_1), color=YELLOW)\n\n        riemann_area = ax.get_riemann_rectangles(curve_1, x_range=[0.3, 0.6], dx=0.03, color=BLUE, fill_opacity=0.5)\n        area = ax.get_area(curve_2, [2, 3], bounded_graph=curve_1, color=GREY, opacity=0.5)\n\n        self.add(ax, labels, curve_1, curve_2, line_1, line_2, riemann_area, area)", "original_prompt": "GraphAreaPlot"}
{"prompt": "Demonstrate a rotating yellow graph with a blue polygon that follows the graph's curve, animated along an X-axis.", "response": "class PolygonOnAxes(Scene):\n    def get_rectangle_corners(self, bottom_left, top_right):\n        return [\n            (top_right[0], top_right[1]),\n            (bottom_left[0], top_right[1]),\n            (bottom_left[0], bottom_left[1]),\n            (top_right[0], bottom_left[1]),\n        ]\n\n    def construct(self):\n        ax = Axes(\n            x_range=[0, 10],\n            y_range=[0, 10],\n            x_length=6,\n            y_length=6,\n            axis_config={\"include_tip\": False},\n        )\n\n        t = ValueTracker(5)\n        k = 25\n\n        graph = ax.plot(\n            lambda x: k / x,\n            color=YELLOW_D,\n            x_range=[k / 10, 10.0, 0.01],\n            use_smoothing=False,\n        )\n\n        def get_rectangle():\n            polygon = Polygon(\n                *[\n                    ax.c2p(*i)\n                    for i in self.get_rectangle_corners(\n                        (0, 0), (t.get_value(), k / t.get_value())\n                    )\n                ]\n            )\n            polygon.stroke_width = 1\n            polygon.set_fill(BLUE, opacity=0.5)\n            polygon.set_stroke(YELLOW_B)\n            return polygon\n\n        polygon = always_redraw(get_rectangle)\n\n        dot = Dot()\n        dot.add_updater(lambda x: x.move_to(ax.c2p(t.get_value(), k / t.get_value())))\n        dot.set_z_index(10)\n\n        self.add(ax, graph, dot)\n        self.play(Create(polygon))\n        self.play(t.animate.set_value(10))\n        self.play(t.animate.set_value(k / 10))\n        self.play(t.animate.set_value(5))", "original_prompt": "PolygonOnAxes"}
{"prompt": "Show a heat diagram with labeled axes and a specific line graph, demonstrating temperature variations over time.", "response": "class HeatDiagramPlot(Scene):\n    def construct(self):\n        ax = Axes(\n            x_range=[0, 40, 5],\n            y_range=[-8, 32, 5],\n            x_length=9,\n            y_length=6,\n            x_axis_config={\"numbers_to_include\": np.arange(0, 40, 5)},\n            y_axis_config={\"numbers_to_include\": np.arange(-5, 34, 5)},\n            tips=False,\n        )\n        labels = ax.get_axis_labels(\n            x_label=Tex(r\"$\\Delta Q$\"), y_label=Tex(r\"T[$^\\circ C$]\")\n        )\n\n        x_vals = [0, 8, 38, 39]\n        y_vals = [20, 0, 0, -5]\n        graph = ax.plot_line_graph(x_values=x_vals, y_values=y_vals)\n\n        self.add(ax, labels, graph)", "original_prompt": "HeatDiagramPlot"}
{"prompt": "Show a camera scene demonstrating an animated graph with scaling and movement around an orange dot.", "response": "class FollowingGraphCamera(MovingCameraScene):\n    def construct(self):\n        self.camera.frame.save_state()\n\n        # create the axes and the curve\n        ax = Axes(x_range=[-1, 10], y_range=[-1, 10])\n        graph = ax.plot(lambda x: np.sin(x), color=BLUE, x_range=[0, 3 * PI])\n\n        # create dots based on the graph\n        moving_dot = Dot(ax.i2gp(graph.t_min, graph), color=ORANGE)\n        dot_1 = Dot(ax.i2gp(graph.t_min, graph))\n        dot_2 = Dot(ax.i2gp(graph.t_max, graph))\n\n        self.add(ax, graph, dot_1, dot_2, moving_dot)\n        self.play(self.camera.frame.animate.scale(0.5).move_to(moving_dot))\n\n        def update_curve(mob):\n            mob.move_to(moving_dot.get_center())\n\n        self.camera.frame.add_updater(update_curve)\n        self.play(MoveAlongPath(moving_dot, graph, rate_func=linear))\n        self.camera.frame.remove_updater(update_curve)\n\n        self.play(Restore(self.camera.frame))", "original_prompt": "FollowingGraphCamera"}
{"prompt": "Demonstrate a zoomed scene where an image and dot move around while the camera zooms in, scales up, and moves down.", "response": "class MovingZoomedSceneAround(ZoomedScene):\n# contributed by TheoremofBeethoven, www.youtube.com/c/TheoremofBeethoven\n    def __init__(self, **kwargs):\n        ZoomedScene.__init__(\n            self,\n            zoom_factor=0.3,\n            zoomed_display_height=1,\n            zoomed_display_width=6,\n            image_frame_stroke_width=20,\n            zoomed_camera_config={\n                \"default_frame_stroke_width\": 3,\n                },\n            **kwargs\n        )\n\n    def construct(self):\n        dot = Dot().shift(UL * 2)\n        image = ImageMobject(np.uint8([[0, 100, 30, 200],\n                                       [255, 0, 5, 33]]))\n        image.height = 7\n        frame_text = Text(\"Frame\", color=PURPLE, font_size=67)\n        zoomed_camera_text = Text(\"Zoomed camera\", color=RED, font_size=67)\n\n        self.add(image, dot)\n        zoomed_camera = self.zoomed_camera\n        zoomed_display = self.zoomed_display\n        frame = zoomed_camera.frame\n        zoomed_display_frame = zoomed_display.display_frame\n\n        frame.move_to(dot)\n        frame.set_color(PURPLE)\n        zoomed_display_frame.set_color(RED)\n        zoomed_display.shift(DOWN)\n\n        zd_rect = BackgroundRectangle(zoomed_display, fill_opacity=0, buff=MED_SMALL_BUFF)\n        self.add_foreground_mobject(zd_rect)\n\n        unfold_camera = UpdateFromFunc(zd_rect, lambda rect: rect.replace(zoomed_display))\n\n        frame_text.next_to(frame, DOWN)\n\n        self.play(Create(frame), FadeIn(frame_text, shift=UP))\n        self.activate_zooming()\n\n        self.play(self.get_zoomed_display_pop_out_animation(), unfold_camera)\n        zoomed_camera_text.next_to(zoomed_display_frame, DOWN)\n        self.play(FadeIn(zoomed_camera_text, shift=UP))\n        # Scale in        x   y  z\n        scale_factor = [0.5, 1.5, 0]\n        self.play(\n            frame.animate.scale(scale_factor),\n            zoomed_display.animate.scale(scale_factor),\n            FadeOut(zoomed_camera_text),\n            FadeOut(frame_text)\n        )\n        self.wait()\n        self.play(ScaleInPlace(zoomed_display, 2))\n        self.wait()\n        self.play(frame.animate.shift(2.5 * DOWN))\n        self.wait()\n        self.play(self.get_zoomed_display_pop_out_animation(), unfold_camera, rate_func=lambda t: smooth(1 - t))\n        self.play(Uncreate(zoomed_display_frame), FadeOut(frame))\n        self.wait()", "original_prompt": "MovingZoomedSceneAround"}
{"prompt": "\"Create a rotating 3D text element within fixed boundaries of an overlaying axes visualization.\"", "response": "class FixedInFrameMObjectTest(ThreeDScene):\n    def construct(self):\n        axes = ThreeDAxes()\n        self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES)\n        text3d = Text(\"This is a 3D text\")\n        self.add_fixed_in_frame_mobjects(text3d)\n        text3d.to_corner(UL)\n        self.add(axes)\n        self.wait()", "original_prompt": "FixedInFrameMObjectTest"}
{"prompt": "Demonstrate a 3D light source animation with rotating camera and a moving light source positioned at (3,0,0).", "response": "class ThreeDLightSourcePosition(ThreeDScene):\n    def construct(self):\n        axes = ThreeDAxes()\n        sphere = Surface(\n            lambda u, v: np.array([\n                1.5 * np.cos(u) * np.cos(v),\n                1.5 * np.cos(u) * np.sin(v),\n                1.5 * np.sin(u)\n            ]), v_range=[0, TAU], u_range=[-PI / 2, PI / 2],\n            checkerboard_colors=[RED_D, RED_E], resolution=(15, 32)\n        )\n        self.renderer.camera.light_source.move_to(3*IN) # changes the source of the light\n        self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)\n        self.add(axes, sphere)", "original_prompt": "ThreeDLightSourcePosition"}
{"prompt": "Show a rotating 3D camera with animated axes and circle, simulating ambient rotation over time.", "response": "class ThreeDCameraRotation(ThreeDScene):\n    def construct(self):\n        axes = ThreeDAxes()\n        circle=Circle()\n        self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)\n        self.add(circle,axes)\n        self.begin_ambient_camera_rotation(rate=0.1)\n        self.wait()\n        self.stop_ambient_camera_rotation()\n        self.move_camera(phi=75 * DEGREES, theta=30 * DEGREES)\n        self.wait()", "original_prompt": "ThreeDCameraRotation"}
{"prompt": "Demonstrate a 3D camera illusion with rotating axes and circle in a three-dimensional space.", "response": "class ThreeDCameraIllusionRotation(ThreeDScene):\n    def construct(self):\n        axes = ThreeDAxes()\n        circle=Circle()\n        self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)\n        self.add(circle,axes)\n        self.begin_3dillusion_camera_rotation(rate=2)\n        self.wait(PI/2)\n        self.stop_3dillusion_camera_rotation()", "original_prompt": "ThreeDCameraIllusionRotation"}
{"prompt": "Demonstrate a 3D surface plot with a Gaussian distribution, featuring parametric equations, scale transformation, and axes visualization.", "response": "class ThreeDSurfacePlot(ThreeDScene):\n    def construct(self):\n        resolution_fa = 24\n        self.set_camera_orientation(phi=75 * DEGREES, theta=-30 * DEGREES)\n\n        def param_gauss(u, v):\n            x = u\n            y = v\n            sigma, mu = 0.4, [0.0, 0.0]\n            d = np.linalg.norm(np.array([x - mu[0], y - mu[1]]))\n            z = np.exp(-(d ** 2 / (2.0 * sigma ** 2)))\n            return np.array([x, y, z])\n\n        gauss_plane = Surface(\n            param_gauss,\n            resolution=(resolution_fa, resolution_fa),\n            v_range=[-2, +2],\n            u_range=[-2, +2]\n        )\n\n        gauss_plane.scale(2, about_point=ORIGIN)\n        gauss_plane.set_style(fill_opacity=1,stroke_color=GREEN)\n        gauss_plane.set_fill_by_checkerboard(ORANGE, BLUE, opacity=0.5)\n        axes = ThreeDAxes()\n        self.add(axes,gauss_plane)", "original_prompt": "ThreeDSurfacePlot"}
{"prompt": "Create a visual representation by animating a title, a mathematical equation, and a graph, transforming the elements while applying non-linear transformations to the graph.", "response": "class OpeningManim(Scene):\n    def construct(self):\n        title = Tex(r\"This is some \\LaTeX\")\n        basel = MathTex(r\"\\sum_{n=1}^\\infty \\frac{1}{n^2} = \\frac{\\pi^2}{6}\")\n        VGroup(title, basel).arrange(DOWN)\n        self.play(\n            Write(title),\n            FadeIn(basel, shift=DOWN),\n        )\n        self.wait()\n\n        transform_title = Tex(\"That was a transform\")\n        transform_title.to_corner(UP + LEFT)\n        self.play(\n            Transform(title, transform_title),\n            LaggedStart(*[FadeOut(obj, shift=DOWN) for obj in basel]),\n        )\n        self.wait()\n\n        grid = NumberPlane()\n        grid_title = Tex(\"This is a grid\", font_size=72)\n        grid_title.move_to(transform_title)\n\n        self.add(grid, grid_title)  # Make sure title is on top of grid\n        self.play(\n            FadeOut(title),\n            FadeIn(grid_title, shift=UP),\n            Create(grid, run_time=3, lag_ratio=0.1),\n        )\n        self.wait()\n\n        grid_transform_title = Tex(\n            r\"That was a non-linear function \\\\ applied to the grid\"\n        )\n        grid_transform_title.move_to(grid_title, UL)\n        grid.prepare_for_nonlinear_transform()\n        self.play(\n            grid.animate.apply_function(\n                lambda p: p\n                          + np.array(\n                    [\n                        np.sin(p[1]),\n                        np.sin(p[0]),\n                        0,\n                    ]\n                )\n            ),\n            run_time=3,\n        )\n        self.wait()\n        self.play(Transform(grid_title, grid_transform_title))\n        self.wait()", "original_prompt": "OpeningManim"}
{"prompt": "Create a colorful animation of a unit circle with a moving yellow dot tracing a sine curve as it orbits around the center.", "response": "class SineCurveUnitCircle(Scene):\n    # contributed by heejin_park, https://infograph.tistory.com/230\n    def construct(self):\n        self.show_axis()\n        self.show_circle()\n        self.move_dot_and_draw_curve()\n        self.wait()\n\n    def show_axis(self):\n        x_start = np.array([-6,0,0])\n        x_end = np.array([6,0,0])\n\n        y_start = np.array([-4,-2,0])\n        y_end = np.array([-4,2,0])\n\n        x_axis = Line(x_start, x_end)\n        y_axis = Line(y_start, y_end)\n\n        self.add(x_axis, y_axis)\n        self.add_x_labels()\n\n        self.origin_point = np.array([-4,0,0])\n        self.curve_start = np.array([-3,0,0])\n\n    def add_x_labels(self):\n        x_labels = [\n            MathTex(r\"\\pi\"), MathTex(r\"2 \\pi\"),\n            MathTex(r\"3 \\pi\"), MathTex(r\"4 \\pi\"),\n        ]\n\n        for i in range(len(x_labels)):\n            x_labels[i].next_to(np.array([-1 + 2*i, 0, 0]), DOWN)\n            self.add(x_labels[i])\n\n    def show_circle(self):\n        circle = Circle(radius=1)\n        circle.move_to(self.origin_point)\n        self.add(circle)\n        self.circle = circle\n\n    def move_dot_and_draw_curve(self):\n        orbit = self.circle\n        origin_point = self.origin_point\n\n        dot = Dot(radius=0.08, color=YELLOW)\n        dot.move_to(orbit.point_from_proportion(0))\n        self.t_offset = 0\n        rate = 0.25\n\n        def go_around_circle(mob, dt):\n            self.t_offset += (dt * rate)\n            # print(self.t_offset)\n            mob.move_to(orbit.point_from_proportion(self.t_offset % 1))\n\n        def get_line_to_circle():\n            return Line(origin_point, dot.get_center(), color=BLUE)\n\n        def get_line_to_curve():\n            x = self.curve_start[0] + self.t_offset * 4\n            y = dot.get_center()[1]\n            return Line(dot.get_center(), np.array([x,y,0]), color=YELLOW_A, stroke_width=2 )\n\n\n        self.curve = VGroup()\n        self.curve.add(Line(self.curve_start,self.curve_start))\n        def get_curve():\n            last_line = self.curve[-1]\n            x = self.curve_start[0] + self.t_offset * 4\n            y = dot.get_center()[1]\n            new_line = Line(last_line.get_end(),np.array([x,y,0]), color=YELLOW_D)\n            self.curve.add(new_line)\n\n            return self.curve\n\n        dot.add_updater(go_around_circle)\n\n        origin_to_circle_line = always_redraw(get_line_to_circle)\n        dot_to_curve_line = always_redraw(get_line_to_curve)\n        sine_curve_line = always_redraw(get_curve)\n\n        self.add(dot)\n        self.add(orbit, origin_to_circle_line, dot_to_curve_line, sine_curve_line)\n        self.wait(8.5)\n\n        dot.remove_updater(go_around_circle)", "original_prompt": "SineCurveUnitCircle"}
{"prompt": "Create a visual representation of a neural network's forward propagation animation, scaling and moving the layers.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass NeuralNetworkScene(Scene):\n    \"\"\"Test Scene for the Neural Network\"\"\"\n\n    def construct(self):\n        # Make the Layer object\n        layers = [FeedForwardLayer(3), FeedForwardLayer(5), FeedForwardLayer(3)]\n        nn = NeuralNetwork(layers)\n        nn.scale(2)\n        nn.move_to(ORIGIN)\n        # Make Animation\n        self.add(nn)\n        # self.play(Create(nn))\n        forward_propagation_animation = nn.make_forward_pass_animation(\n            run_time=5, passing_flash=True\n        )\n\n        self.play(forward_propagation_animation)\n", "original_prompt": "Basic Neural Network Basic Neural Network"}
{"prompt": "\"Create and animate a neural network diagram, showcasing layer scales and activation, with dynamic flashing\".", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass NeuralNetworkScene(Scene):\n    \"\"\"Test Scene for the Neural Network\"\"\"\n\n    def construct(self):\n        # Make the Layer object\n        layers = [FeedForwardLayer(3), FeedForwardLayer(5), FeedForwardLayer(3)]\n        nn = NeuralNetwork(layers)\n        nn.scale(2)\n        nn.move_to(ORIGIN)\n        # Make Animation\n        self.add(nn)\n        # self.play(Create(nn))\n        forward_propagation_animation = nn.make_forward_pass_animation(\n            run_time=5, passing_flash=True\n        )\n\n        self.play(forward_propagation_animation)\n", "original_prompt": "Basic Neural Network Basic Neural Network"}
{"prompt": "Demonstrate a 3D animated neural network with input and output dots, connections, and code snippet visualization.", "response": "from manim import *\n\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.math_operation_layer import MathOperationLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        nn = NeuralNetwork({\n            \"feed_forward_1\": FeedForwardLayer(3),\n            \"feed_forward_2\": FeedForwardLayer(3, activation_function=\"ReLU\"),\n            \"feed_forward_3\": FeedForwardLayer(3),\n            \"sum_operation\": MathOperationLayer(\"+\", activation_function=\"ReLU\"),\n        })\n        nn.add_connection(\"feed_forward_1\", \"sum_operation\")\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Add the network\n        nn = NeuralNetwork({\n                \"feed_forward_1\": FeedForwardLayer(3),\n                \"feed_forward_2\": FeedForwardLayer(3, activation_function=\"ReLU\"),\n                \"feed_forward_3\": FeedForwardLayer(3),\n                \"sum_operation\": MathOperationLayer(\"+\", activation_function=\"ReLU\"),\n            },\n            layer_spacing=0.38\n        )\n        # Make connections\n        input_blank_dot = Dot(\n            nn.input_layers_dict[\"feed_forward_1\"].get_left() - np.array([0.65, 0.0, 0.0])\n        )\n        nn.add_connection(input_blank_dot, \"feed_forward_1\", arc_direction=\"straight\")\n        nn.add_connection(\"feed_forward_1\", \"sum_operation\")\n        output_blank_dot = Dot(\n            nn.input_layers_dict[\"sum_operation\"].get_right() + np.array([0.65, 0.0, 0.0])\n        )\n        nn.add_connection(\"sum_operation\", output_blank_dot, arc_direction=\"straight\")\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        # Group it all\n        group = Group(nn, code)\n        group.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)", "original_prompt": "Basic Neural Network Residual Block"}
{"prompt": "Demonstrate a 3D neural network visualization with convolutional and feedforward layers, scaling and positioning it on the screen.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork([\n            # ... Layers at start\n            Convolutional2DLayer(3, 5, 3, activation_function=\"ReLU\"),\n            # ... Layers at end\n        ])\n        # Play the animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        font=\"Monospace\",\n        background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.45)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7),\n                Convolutional2DLayer(3, 5, 3, activation_function=\"ReLU\"),\n                Convolutional2DLayer(5, 3, 3, activation_function=\"ReLU\"),\n                FeedForwardLayer(3),\n                FeedForwardLayer(1),\n            ],\n            layer_spacing=0.25,\n        )\n        # nn.scale(0.7)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # nn.move_to(ORIGIN)\n        # Move everything up\n        # Group(nn, code).move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn Activation Functions"}
{"prompt": "Create a visual representation of a neural network's layers and show how its components animate during a forward pass, with key elements including image and convolutional layer transformations.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image, height=1.5),\n            Convolutional2DLayer(1, 7, 3),\n            Convolutional2DLayer(3, 5, 3),\n            Convolutional2DLayer(5, 3, 1),\n            FeedForwardLayer(3),\n            FeedForwardLayer(3),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.50)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # Group it all\n        # group = Group(nn, code)\n        # group.move_to(ORIGIN)\n        nn.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn Cnn"}
{"prompt": "Demonstrate a neural network's 2D convolutional architecture animation, showing layers with filters and spatial transformations over time.", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork([\n            ImageLayer(image),\n            Convolutional2DLayer(1, 8),\n            MaxPooling2DLayer(kernel_size=2),\n            Convolutional2DLayer(3, 2, 3),\n        ])\n        # Play the animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        font=\"Monospace\",\n        background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.4)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 8, filter_spacing=0.32),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(3, 2, 3, filter_spacing=0.32),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        Group(code, nn).move_to(ORIGIN)\n        self.add(code)\n        self.wait(5)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn Cnn Max Pool"}
{"prompt": "Demonstrate a neural network scene with 3D layers and animations, showcasing convolutional layers and image processing.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image, height=1.5),\n            Convolutional2DLayer(1, 5, 5, 1, 1),\n            Convolutional2DLayer(4, 5, 5, 1, 1),\n            Convolutional2DLayer(2, 5, 5),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.50)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 5, 1, filter_spacing=0.32),\n                Convolutional2DLayer(4, 5, 1, filter_spacing=0.32),\n                Convolutional2DLayer(2, 5, 5, filter_spacing=0.32),\n            ],\n            layer_spacing=0.4,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        # Group it all\n        group = Group(nn, code)\n        group.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn One By One Convolution"}
{"prompt": "Demonstrate a 3D neural network visual with multiple layers, including convolutional and feed-forward layers.", "response": "from manim import *\nfrom manim_ml.neural_network.layers.image import ImageLayer\nimport numpy as np\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image),\n            Convolutional2DLayer(1, 6, 1, padding=1),\n            Convolutional2DLayer(3, 6, 3),\n            FeedForwardLayer(3),\n            FeedForwardLayer(1),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(\n                    num_feature_maps=1,\n                    feature_map_size=6,\n                    padding=1,\n                    padding_dashed=True,\n                ),\n                Convolutional2DLayer(\n                    num_feature_maps=3,\n                    feature_map_size=6,\n                    filter_size=3,\n                    padding=0,\n                    padding_dashed=False,\n                ),\n                FeedForwardLayer(3),\n                FeedForwardLayer(1),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        self.add(nn)\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        Group(code, nn).move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass, run_time=20)\n", "original_prompt": "Cnn Padding Example"}
{"prompt": "\"Create a 3D animation of a neural network's forward pass, demonstrating convolutional layers and residual connections.\"", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\nfrom manim_ml.neural_network import Convolutional2DLayer, NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork({\n            \"layer1\": Convolutional2DLayer(1, 5, padding=1),\n            \"layer2\": Convolutional2DLayer(1, 5, 3, padding=1),\n            \"layer3\": Convolutional2DLayer(1, 5, 3, padding=1)\n        })\n        # Add the residual connection\n        nn.add_connection(\"layer1\", \"layer3\")\n        # Make the animation\n        self.play(nn.make_forward_pass_animation())\n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass ConvScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n\n        nn = NeuralNetwork(\n            {\n                \"layer1\": Convolutional2DLayer(1, 5, padding=1),\n                \"layer2\": Convolutional2DLayer(1, 5, 3, padding=1),\n                \"layer3\": Convolutional2DLayer(1, 5, 3, padding=1),\n            },\n            layer_spacing=0.25,\n        )\n\n        nn.add_connection(\"layer1\", \"layer3\")\n\n        self.add(nn)\n\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        Group(code, nn).move_to(ORIGIN)\n\n        self.play(nn.make_forward_pass_animation(), run_time=8)\n", "original_prompt": "Cnn Resnet Block"}
{"prompt": "Create an animation demonstrating a neural network's forward pass, scaling layers and rotating images.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass ImageNeuralNetworkScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make image object\n            image = Image.open('images/image.jpeg')\n            numpy_image = np.asarray(image)\n            # Make Neural Network\n            layers = [\n                ImageLayer(numpy_image, height=1.4),\n                FeedForwardLayer(3), \n                FeedForwardLayer(5),\n                FeedForwardLayer(3)\n            ]\n            nn = NeuralNetwork(layers)\n            self.add(nn)\n            # Play animation\n            self.play(\n                nn.make_forward_pass_animation()\n            )\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            background_stroke_color=WHITE,\n            insert_line_no=False,\n            style=\"monokai\",\n            # background=\"window\",\n            language=\"py\",\n        )\n        code.scale(0.2)\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        layers = [\n            ImageLayer(numpy_image, height=1.4),\n            FeedForwardLayer(3),\n            FeedForwardLayer(5),\n            FeedForwardLayer(3),\n            FeedForwardLayer(6),\n        ]\n        nn = NeuralNetwork(layers)\n        nn.scale(0.9)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.rotate(-PI / 2)\n        nn.layers[0].image_mobject.rotate(PI / 2)\n        nn.layers[0].image_mobject.shift([0, -0.4, 0])\n        nn.shift([1.5, 0.3, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(1.9)\n        code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(run_time=10))\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Feed Forward Neural Network\n    ffnn_scene = FeedForwardNeuralNetworkScene()\n    ffnn_scene.render()\n    # Neural Network\n    nn_scene = NeuralNetworkScene()\n    nn_scene.render()\n", "original_prompt": "Code Snippet Image Nn Code Snippet"}
{"prompt": "Demonstrate a neural network diagram with an animation of forward pass, scaling and positioning layers.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer, EmbeddingLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 720\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass VAECodeSnippetScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make Neural Network\n            nn = NeuralNetwork([\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ], layer_spacing=0.1)\n            # Play animation\n            self.play(nn.make_forward_pass_animation())\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            # background_stroke_color=WHITE,\n            insert_line_no=False,\n            background=\"window\",\n            # font=\"Monospace\",\n            style=\"one-dark\",\n            language=\"py\",\n        )\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\", point_radius=0.04).scale(\n            1.0\n        )\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ],\n            layer_spacing=0.1,\n        )\n\n        nn.scale(1.1)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        # nn.rotate(-PI/2)\n        # nn.all_layers[0].image_mobject.rotate(PI/2)\n        # nn.all_layers[0].image_mobject.shift([0, -0.4, 0])\n        # nn.all_layers[-1].image_mobject.rotate(PI/2)\n        # nn.all_layers[-1].image_mobject.shift([0, -0.4, 0])\n        nn.shift([0, -1.4, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(0.52)\n        code_snippet.shift([0, 1.25, 0])\n        # code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(), run_time=10)\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Neural Network\n    nn_scene = VAECodeSnippetScene()\n    nn_scene.render()\n", "original_prompt": "Code Snippet Vae Code Landscape"}
{"prompt": "Demonstrate a 3D neural network animation, scaling layers and rotating images in a Python code snippet.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer, EmbeddingLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass VAECodeSnippetScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make image object\n            image = Image.open('images/image.jpeg')\n            numpy_image = np.asarray(image)\n            # Make Neural Network\n            nn = NeuralNetwork([\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ], layer_spacing=0.1)\n            self.add(nn)\n            # Play animation\n            self.play(\n                nn.make_forward_pass_animation()\n            )\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            # background_stroke_color=WHITE,\n            insert_line_no=False,\n            background=\"window\",\n            # font=\"Monospace\",\n            style=\"one-dark\",\n            language=\"py\",\n        )\n        code.scale(0.2)\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\", point_radius=0.04).scale(\n            1.0\n        )\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.0),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.0),\n            ],\n            layer_spacing=0.1,\n        )\n\n        nn.scale(0.65)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.rotate(-PI / 2)\n        nn.all_layers[0].image_mobject.rotate(PI / 2)\n        # nn.all_layers[0].image_mobject.shift([0, -0.4, 0])\n        nn.all_layers[-1].image_mobject.rotate(PI / 2)\n        # nn.all_layers[-1].image_mobject.shift([0, -0.4, 0])\n        nn.shift([1.5, 0.0, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(1.9)\n        code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(), run_time=10)\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Neural Network\n    nn_scene = VAECodeSnippetScene()\n    nn_scene.render()\n", "original_prompt": "Code Snippet Vae Nn Code Snippet"}
{"prompt": "Here is a high-quality, readable, and well-documented code for the given animation:\n```python\nimport random\nfrom vpython import *\n\n# Set up the scene\nscene = canvas(title='Image Segmentation', width=800, height=600)\n\n# Define colors\nWHITE = color(1, 1, 1)\nBLACK = color(0, 0, 0)\n\n# Function to create a random image patch\ndef create_image_patch(image_width, image_height):\n    return rect(pos=vector(0, 0, 0), size=vector(image_width, image_height), color=WHITE)\n\n# Define the scene elements\nimage_mobject = sphere(pos=vector(-1, -2, 0), radius=0.05)\ntext_prompt = text(paragraph Prompt, font_size=64)\nprompt_title = text('Prompt', font_size=72)\n\n# Create arrows for visual tokens\ndef create_arrow(label_text, token_position):\n    return arrow(pos=label_text.pos, direction=token_position, color=BROWN)\n\narrow_animations = []\n\n# Define the image segmentation steps\ndef step_1(image_mobject):\n    # Show the image being split into patches\n    patches = [create_image_patch(32, 32) for _ in range(random.randint(10, 20))]\n    \n    self.play(ExpandPatches(patches, expansion_scale=1.2))\n    \n    arrow_animations.append(Create(arrows))\n\ndef step_2(image_mobject):\n    # Show the text prompt and image being tokenized.\n    tokenized_text = TokenizedText(paragraph Prompt, pipe.tokenizer)\n    \n    self.play(FadeIn(tokenized_text))\n    \n    # Plot token label\n    token_label = text('Textual Tokens', font_size=72)\n    \n    arrow_animations.append(Create(token_labels))\n\ndef step_3(image_mobject):\n    # Show the heatmap corresponding to the cross attention map for each image.\n    surrounding_rectangle = SurroundingRectangle(text Prompt, buff=0.1)\n    self.add(surrounding_rectangle)\n    \n    # Show the heatmap for \"astronaut\"\n    astronaut_heatmap = heatmap_dict[\"astronaut\"]\n    astronaut_heatmap.height = image_mobject.get_height()\n    astronaut_heatmap.shift(RIGHT * 7)\n    astronaut_heatmap.shift(DOWN)\n    \n    self.play(FadeIn(astronaut_heatmap))\n    \n    # Show the heatmap for \"horse\"\n    horse_heatmap = heatmap_dict[\"horse\"]\n    horse_heatmap.height = image_mobject.get_height()\n    horse_heatmap.move_to(astronaut_heatmap)\n    self.play(FadeOut(astronaut_heatmap))\n    self.play(FadeIn(horse_heatmap))\n    \n    # Show the heatmap for \"riding\"\n    riding_heatmap = heatmap_dict[\"riding\"]\n    riding_heatmap.height = image_mobject.get_height()\n    riding_heatmap.move_to(horse_heatmap)\n    self.play(FadeOut(horse_heatmap))\n    self.play(FadeIn(riding_heatmap))\n\n# Define the animation steps\ndef create_image_segmentation():\n    # Step 1: Show the image being split into patches\n    step_1(image_mobject)\n    \n    # Wait for a moment to allow the user to see the patching process\n    \n    # Step 2: Show the text prompt and image being tokenized.\n    step_2(image_mobject)\n    \n    # Wait for another moment to allow the user to see the tokenization process\n    \n    # Step 3: Show the heatmap corresponding to the cross attention map for each image.\n    step_3(image_mobject)\n\n# Define the animation\ndef main():\n    create_image_segmentation()\n\n# Call the main function\nmain()\n```\nThis code defines a scene with an image, text prompt, and arrows for visual tokens. It then performs three steps:\n\n1.  **Step 1**: The image is split into patches.\n2.  **Step 2**: The text prompt and image are tokenized.\n3.  **Step 3**: The heatmap corresponding to the cross attention map for each image is shown.\n\nThe animation uses `vpython` functions like `Create`, `ExpandPatches`, `FadeIn`, `FadeOut`, and `arrow` to create the scene elements and animate them.", "response": "\"\"\"\n    Here I thought it would be interesting to visualize the cross attention\n    maps produced by the UNet of a text-to-image diffusion model. \n\n    The key thing I want to show is how images and text are broken up into tokens (patches for images),\n    and those tokens are used to compute a cross attention score, which can be interpreted\n    as a 2D heatmap over the image patches for each text token. \n    \n\n    Necessary operations:\n        1. [X] Split an image into patches and \"expand\" the patches outward to highlight that they are \n            separate. \n        2. Split text into tokens using the tokenizer and display them. \n        3. Compute the cross attention scores (using DAAM) for each word/token and display them as a heatmap.\n        4. Overlay the heatmap over the image patches. Show the overlaying as a transition animation. \n\"\"\"\nimport torch\nimport cv2\nfrom manim import *\nimport numpy as np\nfrom typing import List\nfrom daam import trace, set_seed\nfrom diffusers import StableDiffusionPipeline\nimport torchvision\nimport matplotlib.pyplot as plt\n\nclass ImagePatches(Group):\n    \"\"\"Container object for a grid of ImageMobjects.\"\"\"\n\n    def __init__(self, image: ImageMobject, grid_width=4):\n        \"\"\"\n        Parameters\n        ----------\n        image : ImageMobject\n            image to split into patches\n        grid_width : int, optional\n            number of patches per row, by default 4\n        \"\"\"\n        self.image = image\n        self.grid_width = grid_width\n        super(Group, self).__init__()\n        self.patch_dict = self._split_image_into_patches(image, grid_width)\n\n    def _split_image_into_patches(self, image, grid_width):\n        \"\"\"Splits the images into a set of patches\n\n        Parameters\n        ----------\n        image : ImageMobject\n            image to split into patches\n        grid_width : int\n            number of patches per row\n        \"\"\"\n        patch_dict = {}\n        # Get a pixel array of the image mobject\n        pixel_array = image.pixel_array\n        # Get the height and width of the image\n        height, width = pixel_array.shape[:2]\n        # Split the image into an equal width grid of patches\n        assert height == width, \"Image must be square\"\n        assert height % grid_width == 0, \"Image width must be divisible by grid_width\"\n        patch_width, patch_height = width // grid_width, height // grid_width\n\n        for patch_i in range(self.grid_width):\n            for patch_j in range(self.grid_width):\n                # Get patch pixels\n                i_start, i_end = patch_i * patch_width, (patch_i + 1) * patch_width\n                j_start, j_end = patch_j * patch_height, (patch_j + 1) * patch_height\n                patch_pixels = pixel_array[i_start:i_end, j_start:j_end, :]\n                # Make the patch\n                image_patch = ImageMobject(patch_pixels)\n                # Move the patch to the correct location on the old image\n                image_width = image.get_width()\n                image_center = image.get_center()\n                image_patch.scale(image_width / grid_width / image_patch.get_width())\n                patch_manim_space_width = image_patch.get_width()\n\n                patch_center = image_center\n                patch_center += (patch_j - self.grid_width / 2 + 0.5) * patch_manim_space_width * RIGHT\n                # patch_center = image_center - (patch_i - self.grid_width / 2 + 0.5) * patch_manim_space_width * RIGHT\n                patch_center -= (patch_i - self.grid_width / 2 + 0.5) * patch_manim_space_width * UP\n                # + (patch_j - self.grid_width / 2) * patch_height / 2 * UP\n\n                image_patch.move_to(patch_center)\n\n                self.add(image_patch)\n                patch_dict[(patch_i, patch_j)] = image_patch\n\n        return patch_dict\n\nclass ExpandPatches(Animation):\n\n    def __init__(self, image_patches: ImagePatches, expansion_scale=2.0):\n        \"\"\"\n        Parameters\n        ----------\n        image_patches : ImagePatches\n            set of image patches\n        expansion_scale : float, optional\n            scale factor for expansion, by default 2.0\n        \"\"\"\n        self.image_patches = image_patches\n        self.expansion_scale = expansion_scale\n        super().__init__(image_patches)\n\n    def interpolate_submobject(self, submobject, starting_submobject, alpha):\n        \"\"\"\n        Parameters\n        ----------\n        submobject : ImageMobject\n            current patch\n        starting_submobject : ImageMobject\n            starting patch\n        alpha : float\n            interpolation alpha\n        \"\"\"\n        patch = submobject\n        starting_patch_center = starting_submobject.get_center()\n        image_center = self.image_patches.image.get_center()\n        # Start image vector\n        starting_patch_vector = starting_patch_center - image_center\n        final_image_vector = image_center + starting_patch_vector * self.expansion_scale\n        # Interpolate vectors\n        current_location = alpha * final_image_vector + (1 - alpha) * starting_patch_center\n        # # Need to compute the direction of expansion as the unit vector from the original image center\n        # # and patch center. \n        patch.move_to(current_location)\n\nclass TokenizedText(Group):\n    \"\"\"Tokenizes the given text and displays the tokens as a list of Text Mobjects.\"\"\"\n\n    def __init__(self, text, tokenizer=None):\n        self.text = text\n        if not tokenizer is None:\n            self.tokenizer = tokenizer\n        else:\n            # TODO load default stable diffusion tokenizer here\n            raise NotImplementedError(\"Tokenizer must be provided\")\n\n        self.token_strings = self._tokenize_text(text)\n        self.token_mobjects = self.make_text_mobjects(self.token_strings)\n\n        super(Group, self).__init__()\n        self.add(*self.token_mobjects)\n        self.arrange(RIGHT, buff=-0.05, aligned_edge=UP)\n\n        token_dict = {} \n        for token_index, token_string in enumerate(self.token_strings):\n            token_dict[token_string] = self.token_mobjects[token_index]\n\n        self.token_dict = token_dict\n\n    def _tokenize_text(self, text):\n        \"\"\"Tokenize the text using the tokenizer.\n\n        Parameters\n        ----------\n        text : str\n            text to tokenize\n        \"\"\"\n        tokens = self.tokenizer.tokenize(text)\n        tokens = [token.replace(\"</w>\", \"\") for token in tokens]\n        return tokens\n\n    def make_text_mobjects(self, tokens_list: List[str]):\n        \"\"\"Converts the tokens into a list of TextMobjects.\"\"\"\n        # Make the tokens\n        # Insert phantom\n        tokens_list = [\"l\" + token + \"g\" for token in tokens_list]\n        token_objects = [\n            Text(token, t2c={'[-1:]': '#000000', '[0:1]': \"#000000\"}, font_size=64) \n            for token in tokens_list\n        ]\n\n        return token_objects\n\ndef compute_stable_diffusion_cross_attention_heatmaps(\n    pipe,\n    prompt: str,\n    seed: int = 2,\n    map_resolution=(32, 32)\n):\n    \"\"\"Computes the cross attention heatmaps for the given prompt.\n\n    Parameters\n    ----------\n    prompt : str\n        the prompt\n    seed : int, optional\n        random seed, by default 0\n    map_resolution : tuple, optional\n        resolution to downscale maps to, by default (16, 16)\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    # Get tokens\n    tokens = pipe.tokenizer.tokenize(prompt)\n    tokens = [token.replace(\"</w>\", \"\") for token in tokens]\n    # Set torch seeds\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    gen = set_seed(seed)  # for reproducibility\n\n    heatmap_dict = {}\n    image = None\n\n    with torch.cuda.amp.autocast(dtype=torch.float16), torch.no_grad():\n        with trace(pipe) as tc:\n            out = pipe(prompt, num_inference_steps=30, generator=gen)\n            image = out[0][0]\n            global_heat_map = tc.compute_global_heat_map()\n            for token in tokens: \n                word_heat_map = global_heat_map.compute_word_heat_map(token)\n                heatmap = word_heat_map.heatmap\n\n                # Downscale the heatmap\n                heatmap = heatmap.unsqueeze(0).unsqueeze(0)\n                # Save the heatmap\n                heatmap = torchvision.transforms.Resize(\n                    map_resolution, \n                    interpolation=torchvision.transforms.InterpolationMode.NEAREST\n                )(heatmap)\n                heatmap = heatmap.squeeze(0).squeeze(0)\n                # heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n                # plt.imshow(heatmap)\n                # plt.savefig(f\"{token}.png\")\n                # Convert heatmap to rgb color\n                # normalize\n                heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n                cmap = plt.get_cmap('inferno')\n                heatmap = cmap(heatmap) * 255\n                print(heatmap)\n\n                # print(heatmap)\n                # Make an image mobject for each heatmap\n                print(heatmap.shape)\n                heatmap = ImageMobject(heatmap)\n                heatmap.set_resampling_algorithm(RESAMPLING_ALGORITHMS[\"nearest\"])\n                heatmap_dict[token] = heatmap\n\n    return image, heatmap_dict\n\n# Make the scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 30.0\nconfig.frame_width = 30.0\n\nclass StableDiffusionCrossAttentionScene(Scene):\n\n    def construct(self):\n        # Load the pipeline\n        model_id = 'stabilityai/stable-diffusion-2-base'\n        pipe = StableDiffusionPipeline.from_pretrained(model_id)\n        prompt = \"Astronaut riding a horse on the moon\"\n        # Compute the images and heatmaps\n        image, heatmap_dict = compute_stable_diffusion_cross_attention_heatmaps(pipe, prompt)\n        # 1. Show an image appearing\n        image_mobject = ImageMobject(image)\n        image_mobject.shift(DOWN)\n        image_mobject.scale(0.7)\n        image_mobject.shift(LEFT * 7)\n        self.add(image_mobject)\n        # 1. Show a text prompt and the corresponding generated image\n        paragraph_prompt = '\"Astronaut riding a\\nhorse on the moon\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        text_prompt.next_to(image_mobject, UP, buff=1.5)\n        prompt_title = Text(\"Prompt\", font_size=72)\n        prompt_title.next_to(text_prompt, UP, buff=0.5)\n        self.play(Create(prompt_title))\n        self.play(Create(text_prompt))        \n        # Make an arrow from the text to the image\n        arrow = Arrow(text_prompt.get_bottom(), image_mobject.get_top(), buff=0.5)\n        self.play(GrowArrow(arrow))\n        self.wait(1)\n        self.remove(arrow)\n        # 2. Show the image being split into patches\n        # Make the patches\n        patches = ImagePatches(image_mobject, grid_width=32)\n        # Expand and contract \n        self.remove(image_mobject)\n        self.play(ExpandPatches(patches, expansion_scale=1.2))\n        # Play arrows for visual tokens\n        visual_token_label = Text(\"Visual Tokens\", font_size=72)\n        visual_token_label.next_to(image_mobject, DOWN, buff=1.5)\n        # Draw arrows \n        arrow_animations = []\n        arrows = []\n        for i in range(patches.grid_width):\n            patch = patches.patch_dict[(patches.grid_width - 1, i)]\n            arrow = Line(visual_token_label.get_top(), patch.get_bottom(), buff=0.3)\n            arrow_animations.append(\n                Create(\n                    arrow\n                )\n            )\n            arrows.append(arrow)\n        self.play(AnimationGroup(*arrow_animations, FadeIn(visual_token_label), lag_ratio=0))\n        self.wait(1)\n        self.play(FadeOut(*arrows, visual_token_label))\n        self.play(ExpandPatches(patches, expansion_scale=1/1.2))\n        # 3. Show the text prompt and image being tokenized.\n        tokenized_text = TokenizedText(prompt, pipe.tokenizer)\n        tokenized_text.shift(RIGHT * 7)\n        tokenized_text.shift(DOWN * 7.5)\n        self.play(FadeIn(tokenized_text))\n        # Plot token label\n        token_label = Text(\"Textual Tokens\", font_size=72)\n        token_label.next_to(tokenized_text, DOWN, buff=0.5)\n        arrow_animations = []\n        self.play(Create(token_label)) \n        arrows = []\n        for token_name, token_mobject in tokenized_text.token_dict.items():\n            arrow = Line(token_label.get_top(), token_mobject.get_bottom(), buff=0.3)\n            arrow_animations.append(\n                Create(\n                    arrow\n                )\n            )\n            arrows.append(arrow)\n        self.play(AnimationGroup(*arrow_animations, lag_ratio=0))\n        self.wait(1)\n        self.play(FadeOut(*arrows, token_label))\n        # 4. Show the heatmap corresponding to the cross attention map for each image. \n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"astronaut\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"astronaut\"\n        astronaut_heatmap = heatmap_dict[\"astronaut\"]\n        astronaut_heatmap.height = image_mobject.get_height()\n        astronaut_heatmap.shift(RIGHT * 7)\n        astronaut_heatmap.shift(DOWN)\n        self.play(FadeIn(astronaut_heatmap))\n        self.wait(3)\n        self.remove(surrounding_rectangle)\n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"horse\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"horse\"\n        horse_heatmap = heatmap_dict[\"horse\"]\n        horse_heatmap.height = image_mobject.get_height()\n        horse_heatmap.move_to(astronaut_heatmap)\n        self.play(FadeOut(astronaut_heatmap))\n        self.play(FadeIn(horse_heatmap))\n        self.wait(3)\n        self.remove(surrounding_rectangle)\n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"riding\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"riding\"\n        riding_heatmap = heatmap_dict[\"riding\"]\n        riding_heatmap.height = image_mobject.get_height()\n        riding_heatmap.move_to(horse_heatmap)\n        self.play(FadeOut(horse_heatmap))\n        self.play(FadeIn(riding_heatmap))\n        self.wait(3)\n", "original_prompt": "Cross Attention Vis Cross Attention Vis"}
{"prompt": "Create a visualization of decision areas around a DecisionTreeClassifier using AABB axis-aligned bounding boxes and clipped rectangles, demonstrating key feature transformations and rectangle colors.", "response": "import numpy as np\nfrom collections import deque\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import _tree as ctree\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n\nclass AABB:\n    \"\"\"Axis-aligned bounding box\"\"\"\n\n    def __init__(self, n_features):\n        self.limits = np.array([[-np.inf, np.inf]] * n_features)\n\n    def split(self, f, v):\n        left = AABB(self.limits.shape[0])\n        right = AABB(self.limits.shape[0])\n        left.limits = self.limits.copy()\n        right.limits = self.limits.copy()\n        left.limits[f, 1] = v\n        right.limits[f, 0] = v\n\n        return left, right\n\n\ndef tree_bounds(tree, n_features=None):\n    \"\"\"Compute final decision rule for each node in tree\"\"\"\n    if n_features is None:\n        n_features = np.max(tree.feature) + 1\n    aabbs = [AABB(n_features) for _ in range(tree.node_count)]\n    queue = deque([0])\n    while queue:\n        i = queue.pop()\n        l = tree.children_left[i]\n        r = tree.children_right[i]\n        if l != ctree.TREE_LEAF:\n            aabbs[l], aabbs[r] = aabbs[i].split(tree.feature[i], tree.threshold[i])\n            queue.extend([l, r])\n    return aabbs\n\n\ndef decision_areas(tree_classifier, maxrange, x=0, y=1, n_features=None):\n    \"\"\"Extract decision areas.\n\n    tree_classifier: Instance of a sklearn.tree.DecisionTreeClassifier\n    maxrange: values to insert for [left, right, top, bottom] if the interval is open (+/-inf)\n    x: index of the feature that goes on the x axis\n    y: index of the feature that goes on the y axis\n    n_features: override autodetection of number of features\n    \"\"\"\n    tree = tree_classifier.tree_\n    aabbs = tree_bounds(tree, n_features)\n    maxrange = np.array(maxrange)\n    rectangles = []\n    for i in range(len(aabbs)):\n        if tree.children_left[i] != ctree.TREE_LEAF:\n            continue\n        l = aabbs[i].limits\n        r = [l[x, 0], l[x, 1], l[y, 0], l[y, 1], np.argmax(tree.value[i])]\n        # clip out of bounds indices\n        \"\"\"\n        if r[0] < maxrange[0]:\n            r[0] = maxrange[0]\n        if r[1] > maxrange[1]:\n            r[1] = maxrange[1]\n        if r[2] < maxrange[2]:\n            r[2] = maxrange[2]\n        if r[3] > maxrange[3]:\n            r[3] = maxrange[3]\n        print(r)\n        \"\"\"\n        rectangles.append(r)\n    rectangles = np.array(rectangles)\n    rectangles[:, [0, 2]] = np.maximum(rectangles[:, [0, 2]], maxrange[0::2])\n    rectangles[:, [1, 3]] = np.minimum(rectangles[:, [1, 3]], maxrange[1::2])\n    return rectangles\n\n\ndef plot_areas(rectangles):\n    for rect in rectangles:\n        color = [\"b\", \"r\"][int(rect[4])]\n        print(rect[0], rect[1], rect[2] - rect[0], rect[3] - rect[1])\n        rp = Rectangle(\n            [rect[0], rect[2]],\n            rect[1] - rect[0],\n            rect[3] - rect[2],\n            color=color,\n            alpha=0.3,\n        )\n        plt.gca().add_artist(rp)\n", "original_prompt": "Decision Tree Decision Tree Surface"}
{"prompt": "It appears that you are using the Pygame library to create an animation of a decision tree classifier for the Iris dataset.\n\nHowever, your code is not clear or well-structured. It's also quite long and seems to be missing some crucial details, such as how to actually animate the decision tree surface, highlight the maximum information gain point, and draw the basic tree.\n\nHere are a few suggestions to improve your code:\n\n1.  **Use a more structured approach**: Consider breaking down your code into smaller functions or classes that each handle a specific part of the animation.\n2.  **Follow the Pygame documentation**: Make sure you're using all the necessary functions and variables from the Pygame library, and follow their usage guidelines.\n\nHere's an example of how you could refactor your code to make it more organized and easier to understand:\n\n```python\nimport pygame\n\nclass DecisionTreeSurface(pygame.sprite.Sprite):\n    def __init__(self, classifier, data, axes_group):\n        super().__init__()\n        self.classifier = classifier\n        self.data = data\n        self.axes_group = axes_group\n\n    def draw(self, surface):\n        # Draw the decision tree surface here\n        pass\n\nclass DecisionTreeClassifier:\n    def __init__(self, data):\n        self.tree_ = None  # Initialize an empty tree here\n\n    def make_tree(self):\n        # Make the tree here using your classifier and data\n        pass\n\n    def draw_graph(self, graph):\n        # Draw the decision tree graph here\n        pass\n\nclass IrisDatasetPlot:\n    def __init__(self):\n        self.data = None  # Initialize an empty dataset here\n        self.axes_group = None  # Initialize an empty axes group here\n\n    def all_group(self):\n        # Return a sprite that represents all of your sprites in one group\n        return pygame.sprite.Group()\n\nclass Animation:\n    def __init__(self, classifier, data, iris_dataset_plot):\n        self.classifier = classifier\n        self.data = data\n        self.iris_dataset_plot = iris_dataset_plot\n\n    def make_split_animation(self):\n        # Create the split animation here using your decision tree classifier and dataset\n        pass\n\n    def construct(self):\n        # Construct the animation by calling all the functions above\n        self.make_split_animation()\n```\n\nThis is just one possible way to refactor your code. You may need to modify it further based on your specific requirements.\n\nAs for the actual logic of creating the decision tree surface and drawing the basic tree, that will depend on how you've implemented your classifier in Python (which isn't shown here). But here's a simple example using scikit-learn library:\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier as DT\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# Load the iris dataset\niris = load_iris()\ndata = iris.data[:, :2]  # We only take the first two features.\ntarget = iris.target\ntree_clf = DT()\n\n# Fit the tree classifier to the data\ntree_clf.fit(data, target)\n\n# Create a decision tree surface and draw it\nfig = plt.figure(figsize=(8,6))\nplt.scatter(data[:,0], data[:,1])\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\n\n# Draw the leaf node of the tree (this is the point where we want to highlight)\nx_leaf = []\ny_leaf = []\nfor idx in tree_clf.tree_.feature:\n    if idx == -1:\n        x_leaf.append(data[tree_clf.tree_.threshold[idx], 0])\n        y_leaf.append(data[tree_clf.tree_.threshold[idx], 1])\n\nplt.scatter(x_leaf, y_leaf, color='yellow')\n\n# Draw the line that represents the boundary between the classes\nfor i in range(len(tree_clf.tree_)):\n    threshold = tree_clf.tree_.feature[i]\n    if tree_clf.tree_.feature[i] == -2:\n        continue\n\n    left_idx = np.where(data[:,0] < threshold)[0][0]\n    right_idx = np.where(data[:,0] >= threshold)[0][-1]\n\n    plt.plot([data[left_idx,0], data[right_idx,0]], [data[left_idx,1], data[right_idx,1]], color='black')\n\n# Show the plot\nplt.show()\n```\n\nThis is a very basic example. But it should give you an idea of how to draw a decision tree surface and highlight its leaf node using matplotlib.", "response": "from sklearn import datasets\nfrom decision_tree_surface import *\nfrom manim import *\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import entropy\nimport math\nfrom PIL import Image\n\niris = datasets.load_iris()\nfont = \"Source Han Sans\"\nfont_scale = 0.75\n\nimages = [\n    Image.open(\"iris_dataset/SetosaFlower.jpeg\"),\n    Image.open(\"iris_dataset/VeriscolorFlower.jpeg\"),\n    Image.open(\"iris_dataset/VirginicaFlower.jpeg\"),\n]\n\n\ndef entropy(class_labels, base=2):\n    # compute the class counts\n    unique, counts = np.unique(class_labels, return_counts=True)\n    dictionary = dict(zip(unique, counts))\n    total = 0.0\n    num_samples = len(class_labels)\n    for class_index in range(0, 3):\n        if not class_index in dictionary:\n            continue\n        prob = dictionary[class_index] / num_samples\n        total += prob * math.log(prob, base)\n    # higher set\n    return -total\n\n\ndef merge_overlapping_polygons(all_polygons, colors=[BLUE, GREEN, ORANGE]):\n    # get all polygons of each color\n    polygon_dict = {\n        str(BLUE).lower(): [],\n        str(GREEN).lower(): [],\n        str(ORANGE).lower(): [],\n    }\n    for polygon in all_polygons:\n        print(polygon_dict)\n        polygon_dict[str(polygon.color).lower()].append(polygon)\n\n    return_polygons = []\n    for color in colors:\n        color = str(color).lower()\n        polygons = polygon_dict[color]\n        points = set()\n        for polygon in polygons:\n            vertices = polygon.get_vertices().tolist()\n            vertices = [tuple(vert) for vert in vertices]\n            for pt in vertices:\n                if pt in points:  # Shared vertice, remove it.\n                    points.remove(pt)\n                else:\n                    points.add(pt)\n        points = list(points)\n        sort_x = sorted(points)\n        sort_y = sorted(points, key=lambda x: x[1])\n\n        edges_h = {}\n        edges_v = {}\n\n        i = 0\n        while i < len(points):\n            curr_y = sort_y[i][1]\n            while i < len(points) and sort_y[i][1] == curr_y:\n                edges_h[sort_y[i]] = sort_y[i + 1]\n                edges_h[sort_y[i + 1]] = sort_y[i]\n                i += 2\n        i = 0\n        while i < len(points):\n            curr_x = sort_x[i][0]\n            while i < len(points) and sort_x[i][0] == curr_x:\n                edges_v[sort_x[i]] = sort_x[i + 1]\n                edges_v[sort_x[i + 1]] = sort_x[i]\n                i += 2\n\n        # Get all the polygons.\n        while edges_h:\n            # We can start with any point.\n            polygon = [(edges_h.popitem()[0], 0)]\n            while True:\n                curr, e = polygon[-1]\n                if e == 0:\n                    next_vertex = edges_v.pop(curr)\n                    polygon.append((next_vertex, 1))\n                else:\n                    next_vertex = edges_h.pop(curr)\n                    polygon.append((next_vertex, 0))\n                if polygon[-1] == polygon[0]:\n                    # Closed polygon\n                    polygon.pop()\n                    break\n            # Remove implementation-markers from the polygon.\n            poly = [point for point, _ in polygon]\n            for vertex in poly:\n                if vertex in edges_h:\n                    edges_h.pop(vertex)\n                if vertex in edges_v:\n                    edges_v.pop(vertex)\n            polygon = Polygon(*poly, color=color, fill_opacity=0.3, stroke_opacity=1.0)\n            return_polygons.append(polygon)\n    return return_polygons\n\n\nclass IrisDatasetPlot(VGroup):\n    def __init__(self):\n        points = iris.data[:, 0:2]\n        labels = iris.feature_names\n        targets = iris.target\n        # Make points\n        self.point_group = self._make_point_group(points, targets)\n        # Make axes\n        self.axes_group = self._make_axes_group(points, labels)\n        # Make legend\n        self.legend_group = self._make_legend(\n            [BLUE, ORANGE, GREEN], iris.target_names, self.axes_group\n        )\n        # Make title\n        # title_text = \"Iris Dataset Plot\"\n        # self.title = Text(title_text).match_y(self.axes_group).shift([0.5, self.axes_group.height / 2 + 0.5, 0])\n        # Make all group\n        self.all_group = Group(self.point_group, self.axes_group, self.legend_group)\n        # scale the groups\n        self.point_group.scale(1.6)\n        self.point_group.match_x(self.axes_group)\n        self.point_group.match_y(self.axes_group)\n        self.point_group.shift([0.2, 0, 0])\n        self.axes_group.scale(0.7)\n        self.all_group.shift([0, 0.2, 0])\n\n    @override_animation(Create)\n    def create_animation(self):\n        animation_group = AnimationGroup(\n            # Perform the animations\n            Create(self.point_group, run_time=2),\n            Wait(0.5),\n            Create(self.axes_group, run_time=2),\n            # add title\n            # Create(self.title),\n            Create(self.legend_group),\n        )\n        return animation_group\n\n    def _make_point_group(self, points, targets, class_colors=[BLUE, ORANGE, GREEN]):\n        point_group = VGroup()\n        for point_index, point in enumerate(points):\n            # draw the dot\n            current_target = targets[point_index]\n            color = class_colors[current_target]\n            dot = Dot(point=np.array([point[0], point[1], 0])).set_color(color)\n            dot.scale(0.5)\n            point_group.add(dot)\n        return point_group\n\n    def _make_legend(self, class_colors, feature_labels, axes):\n        legend_group = VGroup()\n        # Make Text\n        setosa = Text(\"Setosa\", color=BLUE)\n        verisicolor = Text(\"Verisicolor\", color=ORANGE)\n        virginica = Text(\"Virginica\", color=GREEN)\n        labels = VGroup(setosa, verisicolor, virginica).arrange(\n            direction=RIGHT, aligned_edge=LEFT, buff=2.0\n        )\n        labels.scale(0.5)\n        legend_group.add(labels)\n        # surrounding rectangle\n        surrounding_rectangle = SurroundingRectangle(labels, color=WHITE)\n        surrounding_rectangle.move_to(labels)\n        legend_group.add(surrounding_rectangle)\n        # shift the legend group\n        legend_group.move_to(axes)\n        legend_group.shift([0, -3.0, 0])\n        legend_group.match_x(axes[0][0])\n\n        return legend_group\n\n    def _make_axes_group(self, points, labels):\n        axes_group = VGroup()\n        # make the axes\n        x_range = [\n            np.amin(points, axis=0)[0] - 0.2,\n            np.amax(points, axis=0)[0] - 0.2,\n            0.5,\n        ]\n        y_range = [np.amin(points, axis=0)[1] - 0.2, np.amax(points, axis=0)[1], 0.5]\n        axes = Axes(\n            x_range=x_range,\n            y_range=y_range,\n            x_length=9,\n            y_length=6.5,\n            # axis_config={\"number_scale_value\":0.75, \"include_numbers\":True},\n            tips=False,\n        ).shift([0.5, 0.25, 0])\n        axes_group.add(axes)\n        # make axis labels\n        # x_label\n        x_label = (\n            Text(labels[0], font=font)\n            .match_y(axes.get_axes()[0])\n            .shift([0.5, -0.75, 0])\n            .scale(font_scale)\n        )\n        axes_group.add(x_label)\n        # y_label\n        y_label = (\n            Text(labels[1], font=font)\n            .match_x(axes.get_axes()[1])\n            .shift([-0.75, 0, 0])\n            .rotate(np.pi / 2)\n            .scale(font_scale)\n        )\n        axes_group.add(y_label)\n\n        return axes_group\n\n\nclass DecisionTreeSurface(VGroup):\n    def __init__(self, tree_clf, data, axes, class_colors=[BLUE, ORANGE, GREEN]):\n        # take the tree and construct the surface from it\n        self.tree_clf = tree_clf\n        self.data = data\n        self.axes = axes\n        self.class_colors = class_colors\n        self.surface_rectangles = self.generate_surface_rectangles()\n\n    def generate_surface_rectangles(self):\n        # compute data bounds\n        left = np.amin(self.data[:, 0]) - 0.2\n        right = np.amax(self.data[:, 0]) - 0.2\n        top = np.amax(self.data[:, 1])\n        bottom = np.amin(self.data[:, 1]) - 0.2\n        maxrange = [left, right, bottom, top]\n        rectangles = decision_areas(self.tree_clf, maxrange, x=0, y=1, n_features=2)\n        # turn the rectangle objects into manim rectangles\n        def convert_rectangle_to_polygon(rect):\n            # get the points for the rectangle in the plot coordinate frame\n            bottom_left = [rect[0], rect[3]]\n            bottom_right = [rect[1], rect[3]]\n            top_right = [rect[1], rect[2]]\n            top_left = [rect[0], rect[2]]\n            # convert those points into the entire manim coordinates\n            bottom_left_coord = self.axes.coords_to_point(*bottom_left)\n            bottom_right_coord = self.axes.coords_to_point(*bottom_right)\n            top_right_coord = self.axes.coords_to_point(*top_right)\n            top_left_coord = self.axes.coords_to_point(*top_left)\n            points = [\n                bottom_left_coord,\n                bottom_right_coord,\n                top_right_coord,\n                top_left_coord,\n            ]\n            # construct a polygon object from those manim coordinates\n            rectangle = Polygon(\n                *points, color=color, fill_opacity=0.3, stroke_opacity=0.0\n            )\n            return rectangle\n\n        manim_rectangles = []\n        for rect in rectangles:\n            color = self.class_colors[int(rect[4])]\n            rectangle = convert_rectangle_to_polygon(rect)\n            manim_rectangles.append(rectangle)\n\n        manim_rectangles = merge_overlapping_polygons(\n            manim_rectangles, colors=[BLUE, GREEN, ORANGE]\n        )\n\n        return manim_rectangles\n\n    @override_animation(Create)\n    def create_override(self):\n        # play a reveal of all of the surface rectangles\n        animations = []\n        for rectangle in self.surface_rectangles:\n            animations.append(Create(rectangle))\n        animation_group = AnimationGroup(*animations)\n\n        return animation_group\n\n    @override_animation(Uncreate)\n    def uncreate_override(self):\n        # play a reveal of all of the surface rectangles\n        animations = []\n        for rectangle in self.surface_rectangles:\n            animations.append(Uncreate(rectangle))\n        animation_group = AnimationGroup(*animations)\n\n        return animation_group\n\n\nclass DecisionTree:\n    \"\"\"\n    Draw a single tree node\n    \"\"\"\n\n    def _make_node(\n        self,\n        feature,\n        threshold,\n        values,\n        is_leaf=False,\n        depth=0,\n        leaf_colors=[BLUE, ORANGE, GREEN],\n    ):\n        if not is_leaf:\n            node_text = f\"{feature}\\n    <= {threshold:.3f} cm\"\n            # draw decision text\n            decision_text = Text(node_text, color=WHITE)\n            # draw a box\n            bounding_box = SurroundingRectangle(decision_text, buff=0.3, color=WHITE)\n            node = VGroup()\n            node.add(bounding_box)\n            node.add(decision_text)\n            # return the node\n        else:\n            # plot the appropriate image\n            class_index = np.argmax(values)\n            # get image\n            pil_image = images[class_index]\n            leaf_group = Group()\n            node = ImageMobject(pil_image)\n            node.scale(1.5)\n            rectangle = Rectangle(\n                width=node.width + 0.05,\n                height=node.height + 0.05,\n                color=leaf_colors[class_index],\n                stroke_width=6,\n            )\n            rectangle.move_to(node.get_center())\n            rectangle.shift([-0.02, 0.02, 0])\n            leaf_group.add(rectangle)\n            leaf_group.add(node)\n            node = leaf_group\n\n        return node\n\n    def _make_connection(self, top, bottom, is_leaf=False):\n        top_node_bottom_location = top.get_center()\n        top_node_bottom_location[1] -= top.height / 2\n        bottom_node_top_location = bottom.get_center()\n        bottom_node_top_location[1] += bottom.height / 2\n        line = Line(top_node_bottom_location, bottom_node_top_location, color=WHITE)\n        return line\n\n    def _make_tree(self, tree, feature_names=[\"Sepal Length\", \"Sepal Width\"]):\n        tree_group = Group()\n        max_depth = tree.max_depth\n        # make the base node\n        feature_name = feature_names[tree.feature[0]]\n        threshold = tree.threshold[0]\n        values = tree.value[0]\n        nodes_map = {}\n        root_node = self._make_node(feature_name, threshold, values, depth=0)\n        nodes_map[0] = root_node\n        tree_group.add(root_node)\n        # save some information\n        node_height = root_node.height\n        node_width = root_node.width\n        scale_factor = 1.0\n        edge_map = {}\n        # tree height\n        tree_height = scale_factor * node_height * max_depth\n        tree_width = scale_factor * 2**max_depth * node_width\n        # traverse tree\n        def recurse(node, depth, direction, parent_object, parent_node):\n            # make sure it is a valid node\n            # make the node object\n            is_leaf = tree.children_left[node] == tree.children_right[node]\n            feature_name = feature_names[tree.feature[node]]\n            threshold = tree.threshold[node]\n            values = tree.value[node]\n            node_object = self._make_node(\n                feature_name, threshold, values, depth=depth, is_leaf=is_leaf\n            )\n            nodes_map[node] = node_object\n            node_height = node_object.height\n            # set the node position\n            direction_factor = -1 if direction == \"left\" else 1\n            shift_right_amount = (\n                0.8 * direction_factor * scale_factor * tree_width / (2**depth) / 2\n            )\n            if is_leaf:\n                shift_down_amount = -1.0 * scale_factor * node_height\n            else:\n                shift_down_amount = -1.8 * scale_factor * node_height\n            node_object.match_x(parent_object).match_y(parent_object).shift(\n                [shift_right_amount, shift_down_amount, 0]\n            )\n            tree_group.add(node_object)\n            # make a connection\n            connection = self._make_connection(\n                parent_object, node_object, is_leaf=is_leaf\n            )\n            edge_name = str(parent_node) + \",\" + str(node)\n            edge_map[edge_name] = connection\n            tree_group.add(connection)\n            # recurse\n            if not is_leaf:\n                recurse(tree.children_left[node], depth + 1, \"left\", node_object, node)\n                recurse(\n                    tree.children_right[node], depth + 1, \"right\", node_object, node\n                )\n\n        recurse(tree.children_left[0], 1, \"left\", root_node, 0)\n        recurse(tree.children_right[0], 1, \"right\", root_node, 0)\n\n        tree_group.scale(0.35)\n        return tree_group, nodes_map, edge_map\n\n    def color_example_path(\n        self, tree_group, nodes_map, tree, edge_map, example, color=YELLOW, thickness=2\n    ):\n        # get decision path\n        decision_path = tree.decision_path(example)[0]\n        path_indices = decision_path.indices\n        # highlight edges\n        for node_index in range(0, len(path_indices) - 1):\n            current_val = path_indices[node_index]\n            next_val = path_indices[node_index + 1]\n            edge_str = str(current_val) + \",\" + str(next_val)\n            edge = edge_map[edge_str]\n            animation_two = AnimationGroup(\n                nodes_map[current_val].animate.set_color(color)\n            )\n            self.play(animation_two, run_time=0.5)\n            animation_one = AnimationGroup(\n                edge.animate.set_color(color),\n                # edge.animate.set_stroke_width(4),\n            )\n            self.play(animation_one, run_time=0.5)\n        # surround the bottom image\n        last_path_index = path_indices[-1]\n        last_path_rectangle = nodes_map[last_path_index][0]\n        self.play(last_path_rectangle.animate.set_color(color))\n\n    def create_sklearn_tree(self, max_tree_depth=1):\n        # learn the decision tree\n        iris = load_iris()\n        tree = learn_iris_decision_tree(iris, max_depth=max_tree_depth)\n        feature_names = iris.feature_names[0:2]\n        return tree.tree_\n\n    def make_tree(self, max_tree_depth=2):\n        sklearn_tree = self.create_sklearn_tree()\n        # make the tree\n        tree_group, nodes_map, edge_map = self._make_tree(\n            sklearn_tree.tree_, feature_names\n        )\n        tree_group.shift([0, 5.5, 0])\n        return tree_group\n        # self.add(tree_group)\n        # self.play(SpinInFromNothing(tree_group), run_time=3)\n        # self.color_example_path(tree_group, nodes_map, tree, edge_map, iris.data[None, 0, 0:2])\n\n\nclass DecisionTreeSplitScene(Scene):\n    def make_decision_tree_classifier(self, max_depth=4):\n        decision_tree = DecisionTreeClassifier(\n            random_state=1, max_depth=max_depth, max_leaf_nodes=8\n        )\n        decision_tree = decision_tree.fit(iris.data[:, :2], iris.target)\n        # output the decisioin tree in some format\n        return decision_tree\n\n    def make_split_animation(self, data, classes, data_labels, main_axes):\n\n        \"\"\"\n        def make_entropy_animation_and_plot(dim=0, num_entropy_values=50):\n            # calculate the entropy values\n            axes_group = VGroup()\n            # make axes\n            range_vals = [np.amin(data, axis=0)[dim], np.amax(data, axis=0)[dim]]\n            axes = Axes(x_range=range_vals,\n                        y_range=[0, 1.0],\n                        x_length=9,\n                        y_length=4,\n            #            axis_config={\"number_scale_value\":0.75, \"include_numbers\":True},\n                        tips=False,\n                    )\n            axes_group.add(axes)\n            # make axis labels\n            # x_label\n            x_label = Text(data_labels[dim], font=font) \\\n                            .match_y(axes.get_axes()[0]) \\\n                            .shift([0.5, -0.75, 0]) \\\n                            .scale(font_scale*1.2)\n            axes_group.add(x_label)\n            # y_label\n            y_label = Text(\"Information Gain\", font=font) \\\n                .match_x(axes.get_axes()[1]) \\\n                .shift([-0.75, 0, 0]) \\\n                .rotate(np.pi / 2) \\\n                .scale(font_scale * 1.2)\n\n            axes_group.add(y_label)\n            # line animation\n            information_gains = []\n            def entropy_function(split_value):\n                # lower entropy\n                lower_set = np.nonzero(data[:, dim] <= split_value)[0]\n                lower_set = classes[lower_set]\n                lower_entropy = entropy(lower_set)\n                # higher entropy\n                higher_set = np.nonzero(data[:, dim] > split_value)[0]\n                higher_set = classes[higher_set]\n                higher_entropy = entropy(higher_set)\n                # calculate entropies\n                all_entropy = entropy(classes, base=2)\n                lower_entropy = entropy(lower_set, base=2)\n                higher_entropy = entropy(higher_set, base=2)\n                mean_entropy = (lower_entropy + higher_entropy) / 2\n                # calculate information gain\n                lower_prob = len(lower_set) / len(data[:, dim])\n                higher_prob = len(higher_set) / len(data[:, dim])\n                info_gain = all_entropy - (lower_prob * lower_entropy + higher_prob * higher_entropy)\n                information_gains.append((split_value, info_gain))\n                return info_gain\n\n            data_range = np.amin(data[:, dim]), np.amax(data[:, dim])\n\n            entropy_graph = axes.get_graph(\n                entropy_function, \n            #    color=RED, \n            #    x_range=data_range\n            )\n            axes_group.add(entropy_graph)\n            axes_group.shift([4.0, 2, 0])\n            axes_group.scale(0.5)\n            dot_animation = Dot(color=WHITE)\n            axes_group.add(dot_animation)\n            # make animations\n            animation_group = AnimationGroup(\n                Create(axes_group, run_time=2),\n                Wait(3),\n                MoveAlongPath(dot_animation, entropy_graph, run_time=20, rate_func=rate_functions.ease_in_out_quad),\n                Wait(2)\n            )\n\n            return axes_group, animation_group, information_gains\n        \"\"\"\n\n        def make_split_line_animation(dim=0):\n            # make a line along one of the dims and move it up and down\n            origin_coord = [\n                np.amin(data, axis=0)[0] - 0.2,\n                np.amin(data, axis=0)[1] - 0.2,\n            ]\n            origin_point = main_axes.coords_to_point(*origin_coord)\n            top_left_coord = [origin_coord[0], np.amax(data, axis=0)[1]]\n            bottom_right_coord = [np.amax(data, axis=0)[0] - 0.2, origin_coord[1]]\n            if dim == 0:\n                other_coord = top_left_coord\n                moving_line_coord = bottom_right_coord\n            else:\n                other_coord = bottom_right_coord\n                moving_line_coord = top_left_coord\n            other_point = main_axes.coords_to_point(*other_coord)\n            moving_line_point = main_axes.coords_to_point(*moving_line_coord)\n            moving_line = Line(origin_point, other_point, color=RED)\n            movement_line = Line(origin_point, moving_line_point)\n            if dim == 0:\n                movement_line.shift([0, moving_line.height / 2, 0])\n            else:\n                movement_line.shift([moving_line.width / 2, 0, 0])\n            # move the moving line along the movement line\n            animation = MoveAlongPath(\n                moving_line,\n                movement_line,\n                run_time=20,\n                rate_func=rate_functions.ease_in_out_quad,\n            )\n            return animation, moving_line\n\n        # plot the line in white then make it invisible\n        # make an animation along the line\n        # make a\n        # axes_one_group, top_animation_group, info_gains = make_entropy_animation_and_plot(dim=0)\n        line_movement, first_moving_line = make_split_line_animation(dim=0)\n        # axes_two_group, bottom_animation_group, _ = make_entropy_animation_and_plot(dim=1)\n        second_line_movement, second_moving_line = make_split_line_animation(dim=1)\n        # axes_two_group.shift([0, -3, 0])\n        animation_group_one = AnimationGroup(\n            #    top_animation_group,\n            line_movement,\n        )\n\n        animation_group_two = AnimationGroup(\n            #    bottom_animation_group,\n            second_line_movement,\n        )\n        \"\"\"\n        both_axes_group = VGroup(\n            axes_one_group,\n            axes_two_group\n        )\n        \"\"\"\n\n        return (\n            animation_group_one,\n            animation_group_two,\n            first_moving_line,\n            second_moving_line,\n            None,\n            None,\n        )\n        #    both_axes_group, \\\n        #    info_gains\n\n    def construct(self):\n        # make the points\n        iris_dataset_plot = IrisDatasetPlot()\n        iris_dataset_plot.all_group.scale(1.0)\n        iris_dataset_plot.all_group.shift([-3, 0.2, 0])\n        # make the entropy line graph\n        # entropy_line_graph = self.draw_entropy_line_graph()\n        # arrange the plots\n        # do animations\n        self.play(Create(iris_dataset_plot))\n        # make the decision tree classifier\n        decision_tree_classifier = self.make_decision_tree_classifier()\n        decision_tree_surface = DecisionTreeSurface(\n            decision_tree_classifier, iris.data, iris_dataset_plot.axes_group[0]\n        )\n        self.play(Create(decision_tree_surface))\n        self.wait(3)\n        self.play(Uncreate(decision_tree_surface))\n        main_axes = iris_dataset_plot.axes_group[0]\n        (\n            split_animation_one,\n            split_animation_two,\n            first_moving_line,\n            second_moving_line,\n            both_axes_group,\n            info_gains,\n        ) = self.make_split_animation(\n            iris.data[:, 0:2], iris.target, iris.feature_names, main_axes\n        )\n        self.play(split_animation_one)\n        self.wait(0.1)\n        self.play(Uncreate(first_moving_line))\n        self.wait(3)\n        self.play(split_animation_two)\n        self.wait(0.1)\n        self.play(Uncreate(second_moving_line))\n        self.wait(0.1)\n        # highlight the maximum on top\n        # sort by second key\n        \"\"\"\n        highest_info_gain = sorted(info_gains, key=lambda x: x[1])[-1]\n        highest_info_gain_point = both_axes_group[0][0].coords_to_point(*highest_info_gain)\n        highlighted_peak = Dot(highest_info_gain_point, color=YELLOW)\n        # get location of highest info gain point\n        highest_info_gain_point_in_iris_graph = iris_dataset_plot.axes_group[0].coords_to_point(*[highest_info_gain[0], 0])\n        first_moving_line.start[0] = highest_info_gain_point_in_iris_graph[0]\n        first_moving_line.end[0] = highest_info_gain_point_in_iris_graph[0]\n        self.play(Create(highlighted_peak))\n        self.play(Create(first_moving_line))\n        text = Text(\"Highest Information Gain\")\n        text.scale(0.4)\n        text.move_to(highlighted_peak)\n        text.shift([0, 0.5, 0])\n        self.play(Create(text))\n        \"\"\"\n        self.wait(1)\n        # draw the basic tree\n        decision_tree_classifier = self.make_decision_tree_classifier(max_depth=1)\n        decision_tree_surface = DecisionTreeSurface(\n            decision_tree_classifier, iris.data, iris_dataset_plot.axes_group[0]\n        )\n        decision_tree_graph, _, _ = DecisionTree()._make_tree(\n            decision_tree_classifier.tree_\n        )\n        decision_tree_graph.match_y(iris_dataset_plot.axes_group)\n        decision_tree_graph.shift([4, 0, 0])\n        self.play(Create(decision_tree_surface))\n        uncreate_animation = AnimationGroup(\n            #    Uncreate(both_axes_group),\n            #    Uncreate(highlighted_peak),\n            Uncreate(second_moving_line),\n            #    Unwrite(text)\n        )\n        self.play(uncreate_animation)\n        self.wait(0.5)\n        self.play(FadeIn(decision_tree_graph))\n        # self.play(FadeIn(highlighted_peak))\n        self.wait(5)\n", "original_prompt": "Decision Tree Split Scene"}
{"prompt": "This code appears to be an interactive Python script using the `manim` library for creating animations. Here's a breakdown of what it does:\n\n1. It first sets up some variables and imports necessary libraries.\n2. It creates a 2D plane (`axes`) where the distribution and chains will be plotted.\n3. It generates two random chains, one forward and one backward, using the `generate_forward_and_reverse_chains` function.\n4. It creates an image with the prompt \"An oil painting of a flaming dragon.\"\n5. It animates the following steps:\n\t* First, it does the forward noise process by adding noise to the image at each timestep. This is done using the `RandomWalk` animation group.\n\t* Second, it does the reverse noise process by removing noise from the image at each timestep. This is also done using the `RandomWalk` animation group.\n6. Finally, it adds some final touches, such as updating the title and fading out the random walk.\n\nHere's a refactored version of the code with some improvements:\n\n```python\nfrom manim import *\n\nclass DiffusionProcess(Scene):\n    def construct(self):\n        # Create the axes\n        axes, axes_group = make_2d_diffusion_space()\n\n        # Set up the prompt\n        paragraph_prompt = '\"An oil painting of\\n a flaming dragon.\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        self.add(text_prompt)\n\n        # Create the image with the prompt\n        stable_diffusion_images = [\n            ImageMobject(\"image1.png\"),\n            ImageMobject(\"image2.png\"),\n            ImageMobject(\"image3.png\"),  # Add more images as needed\n        ]\n        image = stable_diffusion_images[0]\n        self.play(FadeIn(image))\n\n        # Define the forward and backward chains\n        def generate_chain(start_point, target_point, num_steps):\n            forward_chain = []\n            for _ in range(num_steps):\n                forward_chain.append(random_point_in_ellipse(\n                    axes, start_point, target_point, 0.5\n                ))\n                start_point = target_point\n            return forward_chain\n\n        forward_chain = generate_chain([3.0, -3.0], [-7.0, -4.0], 10)\n\n        # Generate the reverse chain\n        def generate_reverse_chain(start_point, num_steps):\n            reverse_chain = []\n            for _ in range(num_steps):\n                reverse_chain.append(random_point_in_ellipse(\n                    axes, start_point, [3.0, -3.0], 0.5\n                ))\n                start_point = [-7.0, -4.0]\n            return reverse_chain\n\n        backward_chain = generate_reverse_chain([3.0, -3.0], 10)\n\n        # Create the forward and backward random walks\n        def create_random_walk(chain):\n            random_walk = RandomWalk(chain, axes=axes)\n            animation_group = []\n            for timestep in range(len(chain)):\n                image = ImageMobject(\"image\" + str(timestep) + \".png\")\n                image.set_height(image.get_height())\n                self.play(FadeOut(self.last_used_image), FadeIn(image))\n                new_image = image\n                if timestep < len(chain) - 1:\n                    replace_image_animation = AnimationGroup(\n                        FadeOut(new_image),\n                        FadeIn(image),\n                        lag_ratio=0.0\n                    )\n                animation_group.append(replace_image_animation)\n            return Succession(*animation_group)\n\n        forward_random_walk = create_random_walk(forward_chain)\n        backward_random_walk = create_random_walk(backward_chain)\n\n        # Play the animations\n        self.play(\n            forward_random_walk,\n            backward_random_walk\n        )\n\n# Helper function to create a 2D plane with axes\ndef make_2d_diffusion_space():\n    axes = NumberPlane(width=10, height=5)\n    return axes\n\nclass RandomWalk(Scene):\n    def construct(self, chain, axes=None):\n        self.chain = chain\n        if axes is None:\n            axes = axes\n        random_walk = []\n        for point in self.chain:\n            image = ImageMobject(\"image\" + str(len(random_walk)) + \".png\")\n            image.set_height(image.get_height())\n            self.play(FadeIn(image))\n            new_image = image\n            if len(self.chain) > 1:\n                replace_image_animation = AnimationGroup(\n                    FadeOut(new_image),\n                    FadeIn(image),\n                    lag_ratio=0.0\n                )\n            random_walk.append(replace_image_animation)\n        return Succession(*random_walk)\n\nclass Point(Scene):\n    def construct(self, point, axes=None):\n        self.point = point\n        if axes is None:\n            axes = axes\n        self.x = axes.get_x_axis()\n        self.y = axes.get_y_axis()\n\n    def draw_point(self, color):\n        line = Line(\n            self.point,\n            Point(self.point + self.x * 0.1),\n            stroke_width=5,\n            color=color,\n        )\n        point = self.point\n        return line\n\n# Helper function to generate a random point in an ellipse\ndef random_point_in_ellipse(axes, center, target_center, radius):\n    x = axes.get_x_axis()\n    y = axes.get_y_axis()\n    theta = uniform_random_in_range(-pi, pi)\n    r = sqrt(radius**2 - (x - center[0])**2)\n    x_coord = radius * cos(theta) + center[0]\n    y_coord = radius * sin(theta) + center[1]\n    return Point(x_coord, y_coord)\n\nclass Scene(Scene):\n    def construct(self):\n        # ... (rest of the code remains the same)\n```\n\nThis refactored version includes:\n\n*   Improved organization and naming conventions\n*   Removed unnecessary comments\n*   Replaced `random_point_in_ellipse` with a more concise version using trigonometry\n*   Removed unused variables and functions\n*   Simplified the creation of random walks and images\n*   Improved error handling by making some parameters optional\n\nNote that this is just one possible way to refactor the code, and there may be other approaches depending on your specific use case.", "response": "\"\"\"\n    Shows video of diffusion process. \n\"\"\"\nimport manim_ml\nfrom manim import *\nfrom PIL import Image\nimport os\nfrom diffusers import StableDiffusionPipeline\nimport numpy as np\nimport scipy\n\nfrom manim_ml.diffusion.mcmc import metropolis_hastings_sampler, gaussian_proposal\nfrom manim_ml.diffusion.random_walk import RandomWalk\nfrom manim_ml.utils.mobjects.probability import make_dist_image_mobject_from_samples\n\ndef generate_stable_diffusion_images(prompt, num_inference_steps=30):\n    \"\"\"Generates a list of progressively denoised images using Stable Diffusion\n\n    Parameters\n    ----------\n    num_inference_steps : int, optional\n        _description_, by default 30\n    \"\"\"\n    pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n    image_list = [] \n    # Save image callback\n    def save_image_callback(step, timestep, latents):\n        print(\"Saving image callback\")\n        # Decode the latents\n        image = pipeline.vae.decode(latents)\n        image_list.append(image)\n\n    # Generate the image\n    pipeline(prompt, num_inference_steps=num_inference_steps, callback=save_image_callback)\n\n    return image_list\n\ndef make_time_schedule_bar(num_time_steps=30):\n    \"\"\"Makes a bar that gets moved back and forth according to the diffusion model time\"\"\"\n    # Draw time bar with initial value\n    time_bar = NumberLine(\n        [0, num_time_steps], length=25, stroke_width=10, include_ticks=False, include_numbers=False,\n        color=manim_ml.config.color_scheme.secondary_color\n\n    )\n    time_bar.shift(4.5 * DOWN)\n    current_time = ValueTracker(0.3)\n    time_point = time_bar.number_to_point(current_time.get_value())\n    time_dot = Dot(time_point, color=manim_ml.config.color_scheme.secondary_color, radius=0.2)\n    label_location = time_bar.number_to_point(1.0)\n    label_location -= DOWN * 0.1\n    label_text = MathTex(\"t\", color=manim_ml.config.color_scheme.text_color).scale(1.5)\n    # label_text = Text(\"time\")\n    label_text.move_to(time_bar.get_center())\n    label_text.shift(DOWN * 0.5)\n    # Make an updater for the dot\n    def dot_updater(time_dot):\n        # Get location on time_bar\n        point_loc = time_bar.number_to_point(current_time.get_value())\n        time_dot.move_to(point_loc)\n\n    time_dot.add_updater(dot_updater)\n\n    return time_bar, time_dot, label_text, current_time\n\ndef make_2d_diffusion_space():\n    \"\"\"Makes a 2D axis where the diffusion random walk happens in. \n    There is also a distribution of points representing the true distribution\n    of images. \n    \"\"\"\n    axes_group = Group()\n    x_range = [-8, 8]\n    y_range = [-8, 8]\n    y_length = 13\n    x_length = 13\n    # Make an axis\n    axes = Axes(\n        x_range=x_range,\n        y_range=y_range,\n        x_length=x_length,\n        y_length=y_length,\n        # x_axis_config={\"stroke_opacity\": 1.0, \"stroke_color\": WHITE},\n        # y_axis_config={\"stroke_opacity\": 1.0, \"stroke_color\": WHITE},\n        # tips=True,\n    )\n    # Make the distribution for the true images as some gaussian mixture in \n    # the bottom right\n    gaussian_a = np.random.multivariate_normal(\n        mean=[3.0, -3.2],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(100)\n    )\n    gaussian_b = np.random.multivariate_normal(\n        mean=[3.0, 3.0],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(100)\n    )\n    gaussian_c = np.random.multivariate_normal(\n        mean=[0.0, -1.6],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(200)\n    )\n    all_gaussian_samples = np.concatenate([gaussian_a, gaussian_b, gaussian_c], axis=0)\n    print(f\"Shape of all gaussian samples: {all_gaussian_samples.shape}\")\n\n    image_mobject = make_dist_image_mobject_from_samples(\n        all_gaussian_samples,\n        xlim=(x_range[0], x_range[1]),\n        ylim=(y_range[0], y_range[1])\n    )\n    image_mobject.scale_to_fit_height(\n        axes.get_height()     \n    )\n    image_mobject.move_to(axes)\n\n    axes_group.add(axes)\n    axes_group.add(image_mobject)\n\n    return axes_group\n\ndef generate_forward_and_reverse_chains(start_point, target_point, num_inference_steps=30):\n    \"\"\"Here basically we want to generate a forward and reverse chain\n    for the diffusion model where the reverse chain starts at the end of the forward\n    chain, and the end of the reverse chain ends somewhere within a certain radius of the \n    reverse chain.\n\n    This can be done in a sortof hacky way by doing metropolis hastings from a start point\n    to a distribution centered about the start point and vica versa.\n\n    Parameters\n    ----------\n    start_point : _type_\n        _description_\n    \"\"\"\n    def start_dist_log_prob_fn(x):\n        \"\"\"Log probability of the start distribution\"\"\"\n        # Make it a gaussian in top left of the 2D space\n        gaussian_pdf = scipy.stats.multivariate_normal(\n            mean=target_point,\n            cov=[1.0, 1.0]\n        ).pdf(x)\n\n        return np.log(gaussian_pdf)\n    \n    def end_dist_log_prob_fn(x):\n        gaussian_pdf = scipy.stats.multivariate_normal(\n            mean=start_point,\n            cov=[1.0, 1.0]\n        ).pdf(x)\n\n        return np.log(gaussian_pdf)\n\n    forward_chain, _, _ = metropolis_hastings_sampler(\n        log_prob_fn=start_dist_log_prob_fn, \n        prop_fn=gaussian_proposal,\n        iterations=num_inference_steps,\n        initial_location=start_point\n    )\n\n    end_point = forward_chain[-1]\n\n    reverse_chain, _, _ = metropolis_hastings_sampler(\n        log_prob_fn=end_dist_log_prob_fn,\n        prop_fn=gaussian_proposal,\n        iterations=num_inference_steps,\n        initial_location=end_point\n    )\n\n    return forward_chain, reverse_chain\n\n# Make the scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 30.0\nconfig.frame_width = 30.0\n\nclass DiffusionProcess(Scene):\n\n    def construct(self):\n        # Parameters\n        num_inference_steps = 50\n        image_save_dir = \"diffusion_images\"\n        prompt = \"An oil painting of a dragon.\"\n        start_time = 0\n        # Compute the stable diffusion images\n        if len(os.listdir(image_save_dir)) < num_inference_steps:\n            stable_diffusion_images = generate_stable_diffusion_images(\n                prompt,\n                num_inference_steps=num_inference_steps\n            )\n            for index, image in enumerate(stable_diffusion_images):\n                image.save(f\"{image_save_dir}/{index}.png\")\n        else:\n            stable_diffusion_images = [Image.open(f\"{image_save_dir}/{i}.png\") for i in range(num_inference_steps)]\n        # Reverse order of list to be in line with theory timesteps\n        stable_diffusion_images = stable_diffusion_images[::-1]\n        # Add the initial location of the first image\n        start_image = stable_diffusion_images[start_time]\n        image_mobject = ImageMobject(start_image)\n        image_mobject.scale(0.55)\n        image_mobject.shift(LEFT * 7.5)\n        self.add(image_mobject)\n        # Make the time schedule bar\n        time_bar, time_dot, time_label, current_time = make_time_schedule_bar(num_time_steps=num_inference_steps)\n        # Place the bar at the bottom of the screen\n        time_bar.move_to(image_mobject.get_bottom() + DOWN * 2)\n        time_bar.set_x(0)\n        self.add(time_bar)\n        # Place the time label below the time bar\n        self.add(time_label)\n        time_label.next_to(time_bar, DOWN, buff=0.5)\n        # Add 0 and T labels above the bar\n        zero_label = Text(\"0\")\n        zero_label.next_to(time_bar.get_left(), UP, buff=0.5)\n        self.add(zero_label)\n        t_label = Text(\"T\")\n        t_label.next_to(time_bar.get_right(), UP, buff=0.5)\n        self.add(t_label)\n        # Move the time dot to zero\n        time_dot.set_value(0)\n        time_dot.move_to(time_bar.number_to_point(0))\n        self.add(time_dot)\n        # Add the prompt above the image\n        paragraph_prompt = '\"An oil painting of\\n a flaming dragon.\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        text_prompt.next_to(image_mobject, UP, buff=0.6)\n        self.add(text_prompt)\n        # Generate the chain data\n        forward_chain, reverse_chain = generate_forward_and_reverse_chains(\n            start_point=[3.0, -3.0],\n            target_point=[-7.0, -4.0],\n            num_inference_steps=num_inference_steps\n        )\n        # Make the axes that the distribution and chains go on\n        axes, axes_group = make_2d_diffusion_space()\n        # axes_group.match_height(image_mobject)\n        axes_group.shift(RIGHT * 7.5)\n        axes.shift(RIGHT * 7.5)\n        self.add(axes_group)\n        # Add title below distribution\n        title = Text(\"Distribution of Real Images\", font_size=48, color=RED)\n        title.move_to(axes_group)\n        title.shift(DOWN * 6)\n        line = Line(\n            title.get_top(),\n            axes_group.get_bottom() - 4 * DOWN,\n        )\n        self.add(title)\n        self.add(line)\n        # Add a title above the plot\n        title = Text(\"Reverse Diffusion Process\", font_size=64)\n        title.move_to(axes)\n        title.match_y(text_prompt)\n        self.add(title)\n        # First do the forward noise process of adding noise to an image\n        forward_random_walk = RandomWalk(forward_chain, axes=axes)\n        # Sync up forward random walk\n        synced_animations = []\n        forward_random_walk_animation = forward_random_walk.animate_random_walk()\n        print(len(forward_random_walk_animation.animations))\n        for timestep, transition_animation in enumerate(forward_random_walk_animation.animations):\n            # Make an animation moving the time bar\n            # time_bar_animation = current_time.animate.set_value(timestep)\n            time_bar_animation = current_time.animate.set_value(timestep)\n            # Make an animation replacing the current image with the timestep image\n            new_image = ImageMobject(stable_diffusion_images[timestep])\n            new_image = new_image.set_height(image_mobject.get_height())\n            new_image.move_to(image_mobject)\n            image_mobject = new_image\n            replace_image_animation = AnimationGroup(\n                FadeOut(image_mobject),\n                FadeIn(new_image),\n                lag_ratio=1.0\n            )\n            # Sync them together\n            # synced_animations.append(\n            self.play(\n                Succession(\n                    transition_animation, \n                    time_bar_animation,\n                    replace_image_animation,\n                    lag_ratio=0.0\n                )\n            )\n\n        # Fade out the random walk\n        self.play(forward_random_walk.fade_out_random_walk())\n        # self.play(\n        # Succession(\n        # *synced_animations,\n        # )\n        # )\n        new_title = Text(\"Forward Diffusion Process\", font_size=64)\n        new_title.move_to(title)\n        self.play(ReplacementTransform(title, new_title))\n        # Second do the reverse noise process of removing noise from the image\n        backward_random_walk = RandomWalk(reverse_chain, axes=axes)\n        # Sync up forward random walk\n        synced_animations = []\n        backward_random_walk_animation = backward_random_walk.animate_random_walk()\n        for timestep, transition_animation in enumerate(backward_random_walk_animation.animations):\n            timestep = num_inference_steps - timestep - 1\n            if timestep == num_inference_steps - 1:\n                continue\n            # Make an animation moving the time bar\n            # time_bar_animation = time_dot.animate.set_value(timestep)\n            time_bar_animation = current_time.animate.set_value(timestep)\n            # Make an animation replacing the current image with the timestep image\n            new_image = ImageMobject(stable_diffusion_images[timestep])\n            new_image = new_image.set_height(image_mobject.get_height())\n            new_image.move_to(image_mobject)\n            image_mobject = new_image\n            replace_image_animation = AnimationGroup(\n                FadeOut(image_mobject),\n                FadeIn(new_image),\n                lag_ratio=1.0\n            )\n            # Sync them together\n            # synced_animations.append(\n            self.play(\n                Succession(\n                    transition_animation, \n                    time_bar_animation,\n                    replace_image_animation,\n                    lag_ratio=0.0\n                )\n            )\n        # self.play(\n        # Succession(\n        # *synced_animations,\n        # )\n        # )\n", "original_prompt": "Diffusion Process Diffusion Process"}
{"prompt": "Show a 3D Gaussian-like embedding with points moving from the center to the edges, accompanied by the creation of a VAE decoder neural network structure.", "response": "\"\"\"This module is dedicated to visualizing VAE disentanglement\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\n\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nimport pickle\n\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef construct_image_mobject(input_image, height=2.3):\n    \"\"\"Constructs an ImageMobject from a numpy grayscale image\"\"\"\n    # Convert image to rgb\n    if len(input_image.shape) == 2:\n        input_image = np.repeat(input_image, 3, axis=0)\n        input_image = np.rollaxis(input_image, 0, start=3)\n    #  Make the ImageMobject\n    image_mobject = ImageMobject(input_image, image_mode=\"RGB\")\n    image_mobject.set_resampling_algorithm(RESAMPLING_ALGORITHMS[\"nearest\"])\n    image_mobject.height = height\n\n    return image_mobject\n\n\nclass DisentanglementVisualization(VGroup):\n    def __init__(\n        self,\n        model_path=ROOT_DIR\n        / \"examples/variational_autoencoder/autoencoder_models/saved_models/model_dim2.pth\",\n        image_height=0.35,\n    ):\n        self.model_path = model_path\n        self.image_height = image_height\n        # Load disentanglement image objects\n        with open(\n            ROOT_DIR\n            / \"examples/variational_autoencoder/autoencoder_models/disentanglement.pkl\",\n            \"rb\",\n        ) as f:\n            self.image_handler = pickle.load(f)\n\n    def make_disentanglement_generation_animation(self):\n        animation_list = []\n        for image_index, image in enumerate(self.image_handler[\"images\"]):\n            image_mobject = construct_image_mobject(image, height=self.image_height)\n            r, c = self.image_handler[\"bin_indices\"][image_index]\n            # Move the image to the correct location\n            r_offset = -1.2\n            c_offset = 0.25\n            image_location = [\n                c_offset + c * self.image_height,\n                r_offset + r * self.image_height,\n                0,\n            ]\n            image_mobject.move_to(image_location)\n            animation_list.append(FadeIn(image_mobject))\n\n        generation_animation = AnimationGroup(*animation_list[::-1], lag_ratio=1.0)\n        return generation_animation\n\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 5.0\nconfig.frame_width = 5.0\n\n\nclass DisentanglementScene(Scene):\n    \"\"\"Disentanglement Scene Object\"\"\"\n\n    def _construct_embedding(self, point_color=BLUE, dot_radius=0.05):\n        \"\"\"Makes a Gaussian-like embedding\"\"\"\n        embedding = VGroup()\n        # Sample points from a Gaussian\n        num_points = 200\n        standard_deviation = [0.6, 0.8]\n        mean = [0, 0]\n        points = np.random.normal(mean, standard_deviation, size=(num_points, 2))\n        # Make an axes\n        embedding.axes = Axes(\n            x_range=[-3, 3],\n            y_range=[-3, 3],\n            x_length=2.2,\n            y_length=2.2,\n            tips=False,\n        )\n        # Add each point to the axes\n        self.point_dots = VGroup()\n        for point in points:\n            point_location = embedding.axes.coords_to_point(*point)\n            dot = Dot(point_location, color=point_color, radius=dot_radius / 2)\n            self.point_dots.add(dot)\n\n        embedding.add(self.point_dots)\n        return embedding\n\n    def construct(self):\n        # Make the VAE decoder\n        vae_decoder = NeuralNetwork(\n            [\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n            ],\n            layer_spacing=0.55,\n        )\n\n        vae_decoder.shift([-0.55, 0, 0])\n        self.play(Create(vae_decoder), run_time=1)\n        # Make the embedding\n        embedding = self._construct_embedding()\n        embedding.scale(0.9)\n        embedding.move_to(vae_decoder.get_left())\n        embedding.shift([-0.85, 0, 0])\n        self.play(Create(embedding))\n        # Make disentanglment visualization\n        disentanglement = DisentanglementVisualization()\n        disentanglement_animation = (\n            disentanglement.make_disentanglement_generation_animation()\n        )\n        self.play(disentanglement_animation, run_time=3)\n        self.play(Wait(2))\n", "original_prompt": "Disentanglement Disentanglement"}
{"prompt": "Show an animation demonstrating how to create a 2D graph of epsilon nearest neighbors using Manim, with dynamic balls representing data points and lines connecting them based on distance thresholds.", "response": "\"\"\"\n    Example where I draw an epsilon nearest neighbor graph animation\n\"\"\"\nfrom cProfile import label\nfrom manim import *\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import SpectralClustering\nimport numpy as np\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1200\nconfig.frame_height = 12.0\nconfig.frame_width = 12.0\n\n\ndef make_moon_points(num_samples=100, noise=0.1, random_seed=1):\n    \"\"\"Make two half moon point shapes\"\"\"\n    # Make sure the points are normalized\n    X, y = make_moons(n_samples=num_samples, noise=noise, random_state=random_seed)\n    X -= np.mean(X, axis=0)\n    X /= np.std(X, axis=0)\n    X[:, 1] += 0.3\n    # X[:, 0] /= 2 # squeeze width\n\n    return X\n\n\ndef make_epsilon_balls(epsilon_value, points, axes, ball_color=RED, opacity=0.0):\n    \"\"\"Draws epsilon balls\"\"\"\n    balls = []\n    for point in points:\n        ball = Circle(epsilon_value, color=ball_color, fill_opacity=opacity)\n        global_location = axes.coords_to_point(*point)\n        ball.move_to(global_location)\n        balls.append(ball)\n\n    return VGroup(*balls)\n\n\ndef make_epsilon_graph(epsilon_value, dots, points, edge_color=ORANGE):\n    \"\"\"Makes an epsilon nearest neighbor graph for the given dots\"\"\"\n    # First compute the adjacency matrix from the epsilon value and the points\n    num_dots = len(dots)\n    adjacency_matrix = np.zeros((num_dots, num_dots))\n    # Note: just doing lower triangular matrix\n    for i in range(num_dots):\n        for j in range(i):\n            dist = np.linalg.norm(dots[i].get_center() - dots[j].get_center())\n            is_connected = 1 if dist < epsilon_value else 0\n            adjacency_matrix[i, j] = is_connected\n    # Draw a graph based on the adjacency matrix\n    edges = []\n    for i in range(num_dots):\n        for j in range(i):\n            is_connected = adjacency_matrix[i, j]\n            if is_connected:\n                # Draw a connection between the corresponding dots\n                dot_a = dots[i]\n                dot_b = dots[j]\n                edge = Line(\n                    dot_a.get_center(),\n                    dot_b.get_center(),\n                    color=edge_color,\n                    stroke_width=3,\n                )\n                edges.append(edge)\n\n    return VGroup(*edges), adjacency_matrix\n\n\ndef perform_spectral_clustering(adjacency_matrix):\n    \"\"\"Performs spectral clustering given adjacency matrix\"\"\"\n    clustering = SpectralClustering(\n        n_clusters=2, affinity=\"precomputed\", random_state=0\n    ).fit(adjacency_matrix)\n    labels = clustering.labels_\n\n    return labels\n\n\ndef make_color_change_animation(labels, dots, colors=[ORANGE, GREEN]):\n    \"\"\"Makes a color change animation\"\"\"\n    anims = []\n\n    for index in range(len(labels)):\n        color = colors[labels[index]]\n        dot = dots[index]\n        anims.append(dot.animate.set_color(color))\n\n    return AnimationGroup(*anims, lag_ratio=0.0)\n\n\nclass EpsilonNearestNeighborScene(Scene):\n    def construct(\n        self,\n        num_points=200,\n        dot_radius=0.1,\n        dot_color=BLUE,\n        ball_color=WHITE,\n        noise=0.1,\n        ball_opacity=0.0,\n        random_seed=2,\n    ):\n        # Make moon shape points\n        # Note: dot is the drawing object and point is the math concept\n        moon_points = make_moon_points(\n            num_samples=num_points, noise=noise, random_seed=random_seed\n        )\n        # Make an axes\n        axes = Axes(\n            x_range=[-6, 6, 1],\n            y_range=[-6, 6, 1],\n            x_length=12,\n            y_length=12,\n            tips=False,\n            axis_config={\"stroke_color\": \"#000000\"},\n        )\n        axes.scale(2.2)\n        self.add(axes)\n        # Draw points\n        dots = []\n        for point in moon_points:\n            axes_location = axes.coords_to_point(*point)\n            dot = Dot(axes_location, color=dot_color, radius=dot_radius, z_index=1)\n            dots.append(dot)\n\n        dots = VGroup(*dots)\n        self.play(Create(dots))\n        # Draw epsilon bar with initial value\n        epsilon_bar = NumberLine(\n            [0, 2], length=8, stroke_width=2, include_ticks=False, include_numbers=False\n        )\n        epsilon_bar.shift(4.5 * DOWN)\n        self.play(Create(epsilon_bar))\n        current_epsilon = ValueTracker(0.3)\n        epsilon_point = epsilon_bar.number_to_point(current_epsilon.get_value())\n        epsilon_dot = Dot(epsilon_point)\n        self.add(epsilon_dot)\n        label_location = epsilon_bar.number_to_point(1.0)\n        label_location -= DOWN * 0.1\n        label_text = MathTex(\"\\epsilon\").scale(1.5)\n        # label_text = Text(\"Epsilon\")\n        label_text.move_to(epsilon_bar.get_center())\n        label_text.shift(DOWN * 0.5)\n        self.add(label_text)\n        # Make an updater for the dot\n        def dot_updater(epsilon_dot):\n            # Get location on epsilon_bar\n            point_loc = epsilon_bar.number_to_point(current_epsilon.get_value())\n            epsilon_dot.move_to(point_loc)\n\n        epsilon_dot.add_updater(dot_updater)\n        # Make the epsilon balls\n        epsilon_balls = make_epsilon_balls(\n            current_epsilon.get_value(),\n            moon_points,\n            axes,\n            ball_color=ball_color,\n            opacity=ball_opacity,\n        )\n        # Set up updater for radius of balls\n        def epsilon_balls_updater(epsilon_balls):\n            for ball in epsilon_balls:\n                ball.set_width(current_epsilon.get_value())\n\n        # Turn epsilon up and down\n        epsilon_balls.add_updater(epsilon_balls_updater)\n        # Fade in the initial balls\n        self.play(FadeIn(epsilon_balls), lag_ratio=0.0)\n        # Iterate through different values of epsilon\n        for value in [1.5, 0.5, 0.9]:\n            self.play(current_epsilon.animate.set_value(value), run_time=2.5)\n        # Perform clustering\n        epsilon_value = 0.9\n        # Show connecting graph\n        epsilon_graph, adjacency_matrix = make_epsilon_graph(\n            current_epsilon.get_value(), dots, moon_points, edge_color=WHITE\n        )\n        self.play(FadeOut(epsilon_balls))\n        self.play(FadeIn(epsilon_graph))\n        # Fade out balls\n        self.play(Wait(1.5))\n        # Perform clustering\n        labels = perform_spectral_clustering(adjacency_matrix)\n        # Change the colors of the dots\n        color_change_animation = make_color_change_animation(labels, dots)\n        self.play(color_change_animation)\n        # Fade out graph edges\n        self.play(FadeOut(epsilon_graph))\n        self.play(Wait(5.0))\n", "original_prompt": "Epsilon Nn Graph Epsilon Nn Graph"}
{"prompt": "Show a Generative Adversarial Network (GAN) animation, highlighting layers and displaying real and fake images with probabilities.", "response": "import random\nfrom pathlib import Path\n\nfrom PIL import Image\nfrom manim import *\nfrom manim_ml.neural_network.layers.embedding import EmbeddingLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.vector import VectorLayer\n\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\nconfig.pixel_height = 1080\nconfig.pixel_width = 1080\nconfig.frame_height = 8.3\nconfig.frame_width = 8.3\n\n\nclass GAN(Mobject):\n    \"\"\"Generative Adversarial Network\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.make_entities()\n        self.place_entities()\n        self.titles = self.make_titles()\n\n    def make_entities(self, image_height=1.2):\n        \"\"\"Makes all of the network entities\"\"\"\n        # Make the fake image layer\n        default_image = Image.open(ROOT_DIR / \"assets/gan/fake_image.png\")\n        numpy_image = np.asarray(default_image)\n        self.fake_image_layer = ImageLayer(\n            numpy_image, height=image_height, show_image_on_create=False\n        )\n        # Make the Generator Network\n        self.generator = NeuralNetwork(\n            [\n                EmbeddingLayer(covariance=np.array([[3.0, 0], [0, 3.0]])).scale(1.3),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                self.fake_image_layer,\n            ],\n            layer_spacing=0.1,\n        )\n\n        self.add(self.generator)\n        # Make the Discriminator\n        self.discriminator = NeuralNetwork(\n            [\n                FeedForwardLayer(5),\n                FeedForwardLayer(1),\n                VectorLayer(1, value_func=lambda: random.uniform(0, 1)),\n            ],\n            layer_spacing=0.1,\n        )\n        self.add(self.discriminator)\n        # Make Ground Truth Dataset\n        default_image = Image.open(ROOT_DIR / \"assets/gan/real_image.jpg\")\n        numpy_image = np.asarray(default_image)\n        self.ground_truth_layer = ImageLayer(numpy_image, height=image_height)\n        self.add(self.ground_truth_layer)\n\n        self.scale(1)\n\n    def place_entities(self):\n        \"\"\"Positions entities in correct places\"\"\"\n        # Place relative to generator\n        # Place the ground_truth image layer\n        self.ground_truth_layer.next_to(self.fake_image_layer, DOWN, 0.8)\n        # Group the images\n        image_group = Group(self.ground_truth_layer, self.fake_image_layer)\n        # Move the discriminator to the right of thee generator\n        self.discriminator.next_to(self.generator, RIGHT, 0.2)\n        self.discriminator.match_y(image_group)\n        # Move the discriminator to the height of the center of the image_group\n        # self.discriminator.match_y(image_group)\n        # self.ground_truth_layer.next_to(self.fake_image_layer, DOWN, 0.5)\n\n    def make_titles(self):\n        \"\"\"Makes titles for the different entities\"\"\"\n        titles = VGroup()\n\n        self.ground_truth_layer_title = Text(\"Real Image\").scale(0.3)\n        self.ground_truth_layer_title.next_to(self.ground_truth_layer, UP, 0.1)\n        self.add(self.ground_truth_layer_title)\n        titles.add(self.ground_truth_layer_title)\n        self.fake_image_layer_title = Text(\"Fake Image\").scale(0.3)\n        self.fake_image_layer_title.next_to(self.fake_image_layer, UP, 0.1)\n        self.add(self.fake_image_layer_title)\n        titles.add(self.fake_image_layer_title)\n        # Overhead title\n        overhead_title = Text(\"Generative Adversarial Network\").scale(0.75)\n        overhead_title.shift(np.array([0, 3.5, 0]))\n        titles.add(overhead_title)\n        # Probability title\n        self.probability_title = Text(\"Probability\").scale(0.5)\n        self.probability_title.move_to(self.discriminator.input_layers[-2])\n        self.probability_title.shift(UP)\n        self.probability_title.shift(RIGHT * 1.05)\n        titles.add(self.probability_title)\n\n        return titles\n\n    def make_highlight_generator_rectangle(self):\n        \"\"\"Returns animation that highlights the generators contents\"\"\"\n        group = VGroup()\n\n        generator_surrounding_group = Group(self.generator, self.fake_image_layer_title)\n\n        generator_surrounding_rectangle = SurroundingRectangle(\n            generator_surrounding_group, buff=0.1, stroke_width=4.0, color=\"#0FFF50\"\n        )\n        group.add(generator_surrounding_rectangle)\n        title = Text(\"Generator\").scale(0.5)\n        title.next_to(generator_surrounding_rectangle, UP, 0.2)\n        group.add(title)\n\n        return group\n\n    def make_highlight_discriminator_rectangle(self):\n        \"\"\"Makes a rectangle for highlighting the discriminator\"\"\"\n        discriminator_group = Group(\n            self.discriminator,\n            self.fake_image_layer,\n            self.ground_truth_layer,\n            self.fake_image_layer_title,\n            self.probability_title,\n        )\n\n        group = VGroup()\n\n        discriminator_surrounding_rectangle = SurroundingRectangle(\n            discriminator_group, buff=0.05, stroke_width=4.0, color=\"#0FFF50\"\n        )\n        group.add(discriminator_surrounding_rectangle)\n        title = Text(\"Discriminator\").scale(0.5)\n        title.next_to(discriminator_surrounding_rectangle, UP, 0.2)\n        group.add(title)\n\n        return group\n\n    def make_generator_forward_pass(self):\n        \"\"\"Makes forward pass of the generator\"\"\"\n\n        forward_pass = self.generator.make_forward_pass_animation(dist_theme=\"ellipse\")\n\n        return forward_pass\n\n    def make_discriminator_forward_pass(self):\n        \"\"\"Makes forward pass of the discriminator\"\"\"\n\n        disc_forward = self.discriminator.make_forward_pass_animation()\n\n        return disc_forward\n\n    @override_animation(Create)\n    def _create_override(self):\n        \"\"\"Overrides create\"\"\"\n        animation_group = AnimationGroup(\n            Create(self.generator),\n            Create(self.discriminator),\n            Create(self.ground_truth_layer),\n            Create(self.titles),\n        )\n        return animation_group\n\n\nclass GANScene(Scene):\n    \"\"\"GAN Scene\"\"\"\n\n    def construct(self):\n        gan = GAN().scale(1.70)\n        gan.move_to(ORIGIN)\n        gan.shift(DOWN * 0.35)\n        gan.shift(LEFT * 0.1)\n        self.play(Create(gan), run_time=3)\n        # Highlight generator\n        highlight_generator_rectangle = gan.make_highlight_generator_rectangle()\n        self.play(Create(highlight_generator_rectangle), run_time=1)\n        # Generator forward pass\n        gen_forward_pass = gan.make_generator_forward_pass()\n        self.play(gen_forward_pass, run_time=5)\n        # Fade out generator highlight\n        self.play(Uncreate(highlight_generator_rectangle), run_time=1)\n        # Highlight discriminator\n        highlight_discriminator_rectangle = gan.make_highlight_discriminator_rectangle()\n        self.play(Create(highlight_discriminator_rectangle), run_time=1)\n        # Discriminator forward pass\n        discriminator_forward_pass = gan.make_discriminator_forward_pass()\n        self.play(discriminator_forward_pass, run_time=5)\n        # Unhighlight discriminator\n        self.play(Uncreate(highlight_discriminator_rectangle), run_time=1)\n", "original_prompt": "Gan Gan"}
{"prompt": "\"Create a 3D animation showing VAE interpolation with zooming and fading in animations on top.\"", "response": "\"\"\"Visualization of VAE Interpolation\"\"\"\nimport sys\nimport os\n\nsys.path.append(os.environ[\"PROJECT_ROOT\"])\nfrom manim import *\nimport pickle\nimport numpy as np\nimport manim_ml.neural_network as neural_network\nimport examples.variational_autoencoder.variational_autoencoder as variational_autoencoder\n\n\"\"\"\n    The VAE Scene for the twitter video. \n\"\"\"\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n# Set random seed so point distribution is constant\nnp.random.seed(1)\n\n\nclass InterpolationScene(MovingCameraScene):\n    \"\"\"Scene object for a Variational Autoencoder and Autoencoder\"\"\"\n\n    def construct(self):\n        # Set Scene config\n        vae = variational_autoencoder.VariationalAutoencoder(\n            dot_radius=0.035, layer_spacing=0.5\n        )\n        vae.move_to(ORIGIN)\n        vae.encoder.shift(LEFT * 0.5)\n        vae.decoder.shift(RIGHT * 0.5)\n        mnist_image_handler = variational_autoencoder.MNISTImageHandler()\n        image_pair = mnist_image_handler.image_pairs[3]\n        # Make forward pass animation and DO NOT run it\n        forward_pass_animation = vae.make_forward_pass_animation(image_pair)\n        # Make the interpolation animation\n        interpolation_images = mnist_image_handler.interpolation_images\n        interpolation_animation = vae.make_interpolation_animation(interpolation_images)\n        embedding_zoom_animation = self.camera.auto_zoom(\n            [vae.embedding, vae.decoder, vae.output_image], margin=0.5\n        )\n        # Make animations\n        forward_pass_animations = []\n        for i in range(7):\n            anim = vae.decoder.make_forward_propagation_animation(run_time=0.5)\n            forward_pass_animations.append(anim)\n        forward_pass_animation_group = AnimationGroup(\n            *forward_pass_animations, lag_ratio=1.0\n        )\n        # Make forward pass animations\n        self.play(Create(vae), run_time=1.5)\n        self.play(FadeOut(vae.encoder), run_time=1.0)\n        self.play(embedding_zoom_animation, run_time=1.5)\n        interpolation_animation = AnimationGroup(\n            forward_pass_animation_group, interpolation_animation\n        )\n        self.play(interpolation_animation, run_time=9.0)\n", "original_prompt": "Interpolation Interpolation"}
{"prompt": "Demonstrate a 3D neural network visualization, showcasing layers with transformations like image input, convolutional and pooling operations.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 20.0\nconfig.frame_width = 20.0\n\nROOT_DIR = Path(__file__).parents[2]\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=4.5),\n                Convolutional2DLayer(1, 28),\n                Convolutional2DLayer(6, 28, 5),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(16, 10, 5),\n                MaxPooling2DLayer(kernel_size=2),\n                FeedForwardLayer(8),\n                FeedForwardLayer(3),\n                FeedForwardLayer(2),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # Group it all\n        # group = Group(nn, code)\n        # group.move_to(ORIGIN)\n        nn.move_to(ORIGIN)\n        # Play animation\n        # forward_pass = nn.make_forward_pass_animation()\n        # self.wait(1)\n        # self.play(forward_pass)\n", "original_prompt": "Lenet Lenet"}
{"prompt": "Demonstrate a Manim animation showcasing a neural network logo with text and an underline, scaled to fit within a 4x4 frame.", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\n\nimport manim_ml\nmanim_ml.config.color_scheme = \"light_mode\"\n\nfrom manim_ml.neural_network.architectures.feed_forward import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 1000\nconfig.pixel_width = 1000\nconfig.frame_height = 4.0\nconfig.frame_width = 4.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.text = Text(\"ManimML\", color=manim_ml.config.color_scheme.text_color)\n        self.text.scale(1.0)\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 6, 3], layer_spacing=0.3, node_color=BLUE\n        )\n        self.neural_network.scale(1.0)\n        self.neural_network.move_to(self.text.get_bottom())\n        self.neural_network.shift(1.25 * DOWN)\n        self.logo_group = Group(self.text, self.neural_network)\n        self.logo_group.scale(1.0)\n        self.logo_group.move_to(ORIGIN)\n        self.play(Write(self.text), run_time=1.0)\n        self.play(Create(self.neural_network), run_time=3.0)\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            # Create(self.surrounding_rectangle)\n        )\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            #    Create(self.surrounding_rectangle)\n        )\n        self.play(animation_group, runtime=5.0)\n        self.wait(5)\n", "original_prompt": "Logo Logo"}
{"prompt": "Create a visually striking logo featuring a scaled-up neural network diagram with animated forward pass.", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\nfrom manim_ml.neural_network.neural_network import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 400\nconfig.pixel_width = 600\nconfig.frame_height = 8.0\nconfig.frame_width = 10.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 5], layer_spacing=0.6, node_color=BLUE, edge_width=6\n        )\n        self.neural_network.scale(3)\n        self.neural_network.move_to(ORIGIN)\n        self.play(Create(self.neural_network))\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n        )\n        self.play(animation_group)\n        self.wait(5)\n", "original_prompt": "Logo Website Logo"}
{"prompt": "Demonstrate a Manim animation showcasing a neural network logo with text and animations of calculations.", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\n\nimport manim_ml\nmanim_ml.config.color_scheme = \"light_mode\"\n\nfrom manim_ml.neural_network.architectures.feed_forward import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 1000\nconfig.pixel_width = 2000\nconfig.frame_height = 4.0\nconfig.frame_width = 8.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.text = Text(\"ManimML\", color=manim_ml.config.color_scheme.text_color)\n        self.text.scale(1.0)\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 6, 3], layer_spacing=0.3, node_color=BLUE\n        )\n        self.neural_network.scale(0.8)\n        self.neural_network.next_to(self.text, RIGHT, buff=0.5)\n        # self.neural_network.move_to(self.text.get_right())\n        # self.neural_network.shift(1.25 * DOWN)\n        self.logo_group = Group(self.text, self.neural_network)\n        self.logo_group.scale(1.0)\n        self.logo_group.move_to(ORIGIN)\n        self.play(Write(self.text), run_time=1.0)\n        self.play(Create(self.neural_network), run_time=3.0)\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            # Create(self.surrounding_rectangle)\n        )\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            #    Create(self.surrounding_rectangle)\n        )\n        self.play(animation_group, runtime=5.0)\n        self.wait(5)\n", "original_prompt": "Logo Wide Logo"}
{"prompt": "Demonstrate a Gaussian Mixture Model with animated MCMC chain sampling and visualization on a dark background.", "response": "from manim import *\n\nimport scipy.stats\nfrom manim_ml.diffusion.mcmc import MCMCAxes\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use('dark_background')\n\n# Make the specific scene\nconfig.pixel_height = 720\nconfig.pixel_width = 720\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\nclass MCMCWarmupScene(Scene):\n\n    def construct(self):\n        # Define the Gaussian Mixture likelihood\n        def gaussian_mm_logpdf(x):\n            \"\"\"Gaussian Mixture Model Log PDF\"\"\"\n            # Choose two arbitrary Gaussians\n            # Big Gaussian\n            big_gaussian_pdf = scipy.stats.multivariate_normal(\n                mean=[-0.5, -0.5],\n                cov=[1.0, 1.0]\n            ).pdf(x)\n            # Little Gaussian\n            little_gaussian_pdf = scipy.stats.multivariate_normal(\n                mean=[2.3, 1.9],\n                cov=[0.3, 0.3]\n            ).pdf(x)\n            # Sum their likelihoods and take the log\n            logpdf = np.log(big_gaussian_pdf + little_gaussian_pdf)\n\n            return logpdf\n\n        # Generate a bunch of true samples\n        true_samples = []\n        # Generate samples for little gaussian\n        little_gaussian_samples = np.random.multivariate_normal(\n            mean=[2.3, 1.9],\n            cov=[[0.3, 0.0], [0.0, 0.3]],\n            size=(10000)\n        )\n        big_gaussian_samples = np.random.multivariate_normal(\n            mean=[-0.5, -0.5],\n            cov=[[1.0, 0.0], [0.0, 1.0]],\n            size=(10000)\n        )\n        true_samples = np.concatenate((little_gaussian_samples, big_gaussian_samples))\n        # Make the MCMC axes\n        axes = MCMCAxes(\n            x_range=[-5, 5],\n            y_range=[-5, 5],\n            x_length=7.0,\n            y_length=7.0\n        )\n        axes.move_to(ORIGIN)\n        self.play(\n            Create(axes)\n        )\n        # Make the chain sampling animation \n        chain_sampling_animation = axes.visualize_metropolis_hastings_chain_sampling(\n            log_prob_fn=gaussian_mm_logpdf, \n            true_samples=true_samples,\n            sampling_kwargs={\n                \"iterations\": 2000,\n                \"warm_up\": 50,\n                \"initial_location\": np.array([-3.5, 3.5]),\n                \"sampling_seed\": 4\n            },\n        )\n        self.play(chain_sampling_animation)\n        self.wait(3)\n", "original_prompt": "Mcmc Warmup Mcmc"}
{"prompt": "The code snippet you provided appears to be a part of a larger program that creates and animates a 3D scene. The specific functionality being asked is not explicitly stated, but based on the context, it seems like you're looking for suggestions or improvements on how to optimize the animation of the Neocognitron model.\n\nHere are a few observations and potential suggestions:\n\n1. **Performance optimization**: If the animation is slow, you might want to consider using `VGroup` to group the objects in your scene together, which can reduce the number of object updates during each frame.\n2. **Camera orientation**: The `set_camera_orientation` method seems like it could be improved by making it a separate function or by allowing users to input specific angles for each component (e.g., phi, theta, gamma).\n3. **Scene layout**: The use of `shift` and `next_to` methods to position objects in the scene can be flexible but might become unwieldy as the number of layers increases. Consider using a more structured approach, such as creating subplots or sections for each layer.\n4. **Animation timing**: It's not clear what the desired animation speed is. You could consider adding an `animation_time` parameter to your animations to control how long they take.\n\nHere's a sample code snippet that demonstrates some of these suggestions:\n\n```python\nclass Scene(ThreeDScene):\n    def construct(self):\n        neocognitron = Neocognitron(self.camera)\n        neocognitron.shift(DOWN*0.5)\n\n        # Create separate subplots for the layers to reduce clutter\n        layer_spacing = 1.5\n        self.add_layer(neocognitron, layer_spacing)\n\n        # Create an animation of the forward pass\n        self.play(neocognitron.make_forward_pass_animation())\n\n    def add_layer(self, layer, spacing):\n        # Group all objects from this layer together\n        group = VGroup()\n        for obj in layer.layers:\n            if isinstance(obj, NeocognitronLayer):\n                group.add(obj.plane_label)\n                group.add(obj)\n        group.shift(spacing * ORIGIN)\n\n        # Position the layer and add it to the scene\n        layer.move_to(ORIGIN)\n        self.add(group)\n```\n\nThis code uses `VGroup` to simplify the animation of individual layers and improves the structure of the scene. The `add_layer` method allows for easy extension or modification of each layer's position.\n\nThese are just a few potential suggestions, and you may have different requirements or preferences depending on your specific use case.", "response": "from manim import *\n\nfrom manim_ml.neural_network import NeuralNetwork\nfrom manim_ml.neural_network.layers.parent_layers import NeuralNetworkLayer, ConnectiveLayer, ThreeDLayer\nimport manim_ml\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 10.5\nconfig.frame_width = 10.5\n\nclass NeocognitronFilter(VGroup):\n    \"\"\"\n        Filter connecting the S and C Cells of a neocognitron layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_cell, \n        output_cell, \n        receptive_field_radius, \n        outline_color=BLUE,\n        active_color=ORANGE, \n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.outline_color = outline_color\n        self.active_color = active_color\n        self.input_cell = input_cell\n        self.output_cell = output_cell\n        # Draw the receptive field\n        # Make the points of a equilateral triangle in the plane of the \n        # cell about a random center\n        # center_point = input_cell.get_center()\n        shift_amount_x = np.random.uniform(\n            -(input_cell.get_height()/2 - receptive_field_radius - 0.01),\n            input_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        shift_amount_y = np.random.uniform(\n            -(input_cell.get_height()/2 - receptive_field_radius - 0.01),\n            input_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        # center_point += np.array([shift_amount_x, shift_amount_y, 0])\n        # # Make the triangle points\n        # side_length = np.sqrt(3) * receptive_field_radius\n        # normal_vector = np.cross(\n        #     input_cell.get_left() - input_cell.get_center(),\n        #     input_cell.get_top() - input_cell.get_center(),\n        # )\n        # Get vector in the plane of the cell\n        # vector_in_plane = input_cell.get_left() - input_cell.get_center()\n        # point_a = center_point + vector_in_plane * receptive_field_radius\n        # # rotate the vector 120 degrees about the vector normal to the cell\n        # vector_in_plane = rotate_vector(vector_in_plane, PI/3, normal_vector)\n        # point_b = center_point + vector_in_plane * receptive_field_radius\n        # # rotate the vector 120 degrees about the vector normal to the cell\n        # vector_in_plane = rotate_vector(vector_in_plane, PI/3, normal_vector)\n        # point_c = center_point + vector_in_plane * receptive_field_radius\n        # self.receptive_field = Circle.from_three_points(\n        #     point_a, \n        #     point_b,\n        #     point_c,\n        #     color=outline_color,\n        #     stroke_width=2.0,\n        # )\n        self.receptive_field = Circle(\n            radius=receptive_field_radius,\n            color=outline_color,\n            stroke_width=3.0,\n        )\n        self.add(self.receptive_field)\n        # Move receptive field to a random point within the cell\n        # minus the radius of the receptive field\n        self.receptive_field.move_to(\n            input_cell\n        )\n        # Shift a random amount in the x and y direction within \n        self.receptive_field.shift(\n            np.array([shift_amount_x, shift_amount_y, 0])\n        )\n        # Choose a random point on the c_cell\n        shift_amount_x = np.random.uniform(\n            -(output_cell.get_height()/2 - receptive_field_radius - 0.01),\n            output_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        shift_amount_y = np.random.uniform(\n            -(output_cell.get_height()/2 - receptive_field_radius - 0.01),\n            output_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        self.dot = Dot(\n            color=outline_color,\n            radius=0.04\n        )\n        self.dot.move_to(output_cell.get_center() + np.array([shift_amount_x, shift_amount_y, 0]))\n        self.add(self.dot)\n        # Make lines connecting receptive field to the dot\n        self.lines = VGroup()\n        self.lines.add(\n            Line(\n                self.receptive_field.get_center() + np.array([0, receptive_field_radius, 0]),\n                self.dot,\n                color=outline_color,\n                stroke_width=3.0,\n            )\n        )\n        self.lines.add(\n            Line(\n                self.receptive_field.get_center() - np.array([0, receptive_field_radius, 0]),\n                self.dot,\n                color=outline_color,\n                stroke_width=3.0,\n            )\n        )\n        self.add(self.lines)\n\n    def make_filter_pulse_animation(self, **kwargs):\n        succession = Succession(\n            ApplyMethod(\n                self.receptive_field.set_color, \n                self.active_color, \n                run_time=0.25\n            ),\n            ApplyMethod(\n                self.receptive_field.set_color, \n                self.outline_color, \n                run_time=0.25\n            ),\n            ShowPassingFlash(\n                self.lines.copy().set_color(self.active_color),\n                time_width=0.5,\n            ),\n            ApplyMethod(\n                self.dot.set_color, \n                self.active_color, \n                run_time=0.25\n            ),\n            ApplyMethod(\n                self.dot.set_color, \n                self.outline_color, \n                run_time=0.25\n            ),\n        )\n\n        return succession\n\nclass NeocognitronLayer(NeuralNetworkLayer, ThreeDLayer):\n    \n    def __init__(\n        self, \n        num_cells, \n        cell_height,\n        receptive_field_radius,\n        layer_name,\n        active_color=manim_ml.config.color_scheme.active_color,\n        cell_color=manim_ml.config.color_scheme.secondary_color,\n        outline_color=manim_ml.config.color_scheme.primary_color,\n        show_outline=True,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.num_cells = num_cells\n        self.cell_height = cell_height\n        self.receptive_field_radius = receptive_field_radius\n        self.layer_name = layer_name\n        self.active_color = active_color\n        self.cell_color = cell_color\n        self.outline_color = outline_color\n        self.show_outline = show_outline\n\n        self.plane_label = Text(layer_name).scale(0.4)\n\n    def make_cell_plane(self, layer_name, cell_buffer=0.1, stroke_width=2.0):\n        \"\"\"Makes a plane of cells. \n        \"\"\"\n        cell_plane = VGroup()\n        for cell_index in range(self.num_cells):\n            # Draw the cell box\n            cell_box = Rectangle(\n                width=self.cell_height,\n                height=self.cell_height,\n                color=self.cell_color,\n                fill_color=self.cell_color,\n                fill_opacity=0.3,\n                stroke_width=stroke_width,\n            )\n            if cell_index > 0:\n                cell_box.next_to(cell_plane[-1], DOWN, buff=cell_buffer)\n            cell_plane.add(cell_box)\n        # Draw the outline\n        if self.show_outline:\n            self.plane_outline = SurroundingRectangle(\n                cell_plane,\n                color=self.cell_color,\n                buff=cell_buffer,\n                stroke_width=stroke_width,\n            )\n            cell_plane.add(\n                self.plane_outline\n            )\n        # Draw a label above the container box\n        self.plane_label.next_to(cell_plane, UP, buff=0.2)\n        cell_plane.add(self.plane_label)\n\n        return cell_plane\n\n    def construct_layer(self, input_layer, output_layer, **kwargs):\n        # Make the Cell layer\n        self.cell_plane = self.make_cell_plane(self.layer_name)\n        self.add(self.cell_plane)\n        \n    def make_forward_pass_animation(self, layer_args={}, **kwargs):\n        \"\"\"Forward pass for query\"\"\"\n        # # Pulse and un pulse the cell plane rectangle\n        flash_outline_animations = []\n        for cell in self.cell_plane:\n            flash_outline_animations.append(\n                Succession(\n                    ApplyMethod(\n                        cell.set_stroke_color,\n                        self.active_color,\n                        run_time=0.25\n                    ),\n                    ApplyMethod(\n                        cell.set_stroke_color,\n                        self.outline_color,\n                        run_time=0.25\n                    )\n                )\n            )\n        \n        return AnimationGroup(\n            *flash_outline_animations,\n            lag_ratio=0.0\n        )\n\nclass NeocognitronToNeocognitronLayer(ConnectiveLayer):\n    input_class = NeocognitronLayer\n    output_class = NeocognitronLayer\n\n    def __init__(self, input_layer, output_layer, **kwargs):\n        super().__init__(input_layer, output_layer, **kwargs)\n\n    def construct_layer(self, input_layer, output_layer, **kwargs):\n        self.filters = []\n        for cell_index in range(input_layer.num_cells):\n            input_cell = input_layer.cell_plane[cell_index]\n            # Randomly choose a cell from the output layer\n            output_cell = output_layer.cell_plane[\n                np.random.randint(0, output_layer.num_cells)\n            ]\n            # Make the filter\n            filter = NeocognitronFilter(\n                input_cell=input_cell,\n                output_cell=output_cell,\n                receptive_field_radius=input_layer.receptive_field_radius,\n                outline_color=self.input_layer.outline_color\n            )\n            # filter = NeocognitronFilter(\n            #     outline_color=input_layer.outline_color\n            # )\n            self.filters.append(filter)\n\n        self.add(VGroup(*self.filters))\n\n    def make_forward_pass_animation(self, layer_args={}, **kwargs):\n        \"\"\"Forward pass for query\"\"\"\n        filter_pulses = []\n        for filter in self.filters:\n            filter_pulses.append(\n                filter.make_filter_pulse_animation()\n            )\n        return AnimationGroup(\n            *filter_pulses\n        )\n\nmanim_ml.neural_network.layers.util.register_custom_connective_layer(\n    NeocognitronToNeocognitronLayer,\n)\n\nclass Neocognitron(NeuralNetwork):\n\n    def __init__(\n        self, \n        camera,\n        cells_per_layer=[4, 5, 4, 4, 3, 3, 5, 5],\n        cell_heights=[0.8, 0.8, 0.8*0.75, 0.8*0.75, 0.8*0.75**2, 0.8*0.75**2, 0.8*0.75**3, 0.02],\n        layer_names=[\"S1\", \"C1\", \"S2\", \"C2\", \"S3\", \"C3\", \"S4\", \"C4\"],\n        receptive_field_sizes=[0.2, 0.2, 0.2*0.75, 0.2*0.75, 0.2*0.75**2, 0.2*0.75**2, 0.2*0.75**3, 0.0],\n    ):\n        self.cells_per_layer = cells_per_layer\n        self.cell_heights = cell_heights\n        self.layer_names = layer_names\n        self.receptive_field_sizes = receptive_field_sizes\n        # Make the neo-cognitron input layer\n        input_layers = []\n        input_layers.append(\n            NeocognitronLayer(\n                1,\n                1.5,\n                0.2,\n                \"U0\",\n                show_outline=False,\n            )\n        )\n        # Make the neo-cognitron layers\n        for i in range(len(cells_per_layer)):\n            layer = NeocognitronLayer(\n                cells_per_layer[i],\n                cell_heights[i],\n                receptive_field_sizes[i],\n                layer_names[i]\n            )\n            input_layers.append(layer)\n        \n        # Make all of the layer labels fixed in frame\n        for layer in input_layers:\n            if isinstance(layer, NeocognitronLayer):\n                # camera.add_fixed_orientation_mobjects(layer.plane_label)\n                pass\n\n        all_layers = []\n        for layer_index in range(len(input_layers) - 1):\n            input_layer = input_layers[layer_index]\n            all_layers.append(input_layer)\n            output_layer = input_layers[layer_index + 1]\n            connective_layer = NeocognitronToNeocognitronLayer(\n                input_layer,\n                output_layer\n            )\n            all_layers.append(connective_layer)\n        all_layers.append(input_layers[-1])\n\n        def custom_layout_func(neural_network):\n            # Insert the connective layers\n            # Pass the layers to neural network super class\n            # Rotate pairs of layers\n            layer_index = 1\n            while layer_index < len(input_layers):\n                prev_layer = input_layers[layer_index - 1]\n                s_layer = input_layers[layer_index]\n                s_layer.next_to(prev_layer, RIGHT, buff=0.0)\n                s_layer.shift(RIGHT * 0.4)\n                c_layer = input_layers[layer_index + 1]\n                c_layer.next_to(s_layer, RIGHT, buff=0.0)\n                c_layer.shift(RIGHT * 0.2)\n                # Rotate the pair of layers\n                group = Group(s_layer, c_layer)\n                group.move_to(np.array([group.get_center()[0], 0, 0]))\n\n                # group.shift(layer_index * 3 * np.array([0, 0, 1.0]))\n                # group.rotate(40 * DEGREES, axis=UP, about_point=group.get_center())\n                # c_layer.rotate(40*DEGREES, axis=UP, about_point=group.get_center())\n                # s_layer.shift(\n                #     layer_index // 2 * np.array([0, 0, 0.3])\n                # )\n                # c_layer.shift(\n                #     layer_index // 2 * np.array([0, 0, 0.3])\n                # )\n                layer_index += 2\n            neural_network.move_to(ORIGIN)\n\n        super().__init__(\n            all_layers,\n            layer_spacing=0.5,\n            custom_layout_func=custom_layout_func\n        )\n\nclass Scene(ThreeDScene):\n\n    def construct(self):\n        neocognitron = Neocognitron(self.camera)\n        neocognitron.shift(DOWN*0.5)\n        self.add(neocognitron)\n        title = Text(\"Neocognitron\").scale(1)\n        self.add_fixed_in_frame_mobjects(title)\n        title.next_to(neocognitron, UP, buff=0.4)\n        self.add(title)\n        \"\"\"\n        self.play(\n            neocognitron.make_forward_pass_animation()\n        )\n        \"\"\"\n        print(self.camera.gamma)\n        print(self.camera.theta)\n        print(self.camera.phi)\n        neocognitron.rotate(90*DEGREES, axis=RIGHT)\n        neocognitron.shift(np.array([0, 0, -0.2]))\n        # title.rotate(90*DEGREES, axis=RIGHT)\n        # self.set_camera_orientation(phi=-15*DEGREES)\n        # vec = np.array([0.0, 0.2, 0.0])\n        # vec /= np.linalg.norm(vec)\n        # x, y, z = vec[0], vec[1], vec[2]\n        # theta = np.arccos(z)\n        # phi = np.arctan(y / x)\n        self.set_camera_orientation(phi=90 * DEGREES, theta=-70*DEGREES, gamma=0*DEGREES)\n        # self.set_camera_orientation(theta=theta, phi=phi)\n\n        forward_pass = neocognitron.make_forward_pass_animation()\n        self.play(forward_pass)\n\n", "original_prompt": "Neocognitron Neocognitron"}
{"prompt": "It appears that the code snippet you provided is a Python script for creating a visualization of an oracle-guided image synthesis process using VAE (Variational Autoencoder). The script uses a combination of classes and functions to create the architecture, perform localization procedure, training procedure, and generate estimate images.\n\nHere's a high-level overview of how the script works:\n\n1. **Architecture Creation**: The `construct` method creates the neural network architecture, which includes the encoder, decoder, and triplet layer.\n2. **Localization Procedure**: The `make_localization_time_step` method performs a single iteration of the localization procedure, which involves calculating the posterior distribution and updating it based on the prior distribution.\n3. **Training Procedure**: The `make_triplet_forward_animation` method performs an unsupervised image reconstruction forward pass using the encoder.\n4. **Estimate Generation**: The `make_generate_estimate_animation` method generates an estimate image by sampling from the posterior distribution.\n\nSome key classes and functions in the script include:\n\n*   `Create`: A class that creates a visualization of a VAE architecture.\n*   `TripletLayer`: A class that represents a triplet layer, which is used for unsupervised image reconstruction.\n*   `Encoder` and `Decoder`: Classes that represent the encoder and decoder components of the neural network architecture.\n*   `Localizer`: A class that performs the localization procedure.\n\nThe script uses various visualization tools and libraries, such as Matplotlib, to create interactive visualizations of the VAE architecture, localization procedure, and estimate generation process.", "response": "\"\"\"\n    Here is a animated explanatory figure for the \"Oracle Guided Image Synthesis with Relative Queries\" paper. \n\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\nfrom manim_ml.neural_network.layers import triplet\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.paired_query import PairedQueryLayer\nfrom manim_ml.neural_network.layers.triplet import TripletLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom manim_ml.neural_network.layers import FeedForwardLayer, EmbeddingLayer\nfrom manim_ml.neural_network.layers.util import get_connective_layer\nimport os\n\nfrom manim_ml.utils.mobjects.probability import GaussianDistribution\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\nROOT_DIR = Path(__file__).parents[3]\n\n\nclass Localizer:\n    \"\"\"\n    Holds the localizer object, which contains the queries, images, etc.\n    needed to represent a localization run.\n    \"\"\"\n\n    def __init__(self, axes):\n        # Set dummy values for these\n        self.index = -1\n        self.axes = axes\n        self.num_queries = 3\n        self.assets_path = ROOT_DIR / \"assets/oracle_guidance\"\n        self.ground_truth_image_path = self.assets_path / \"ground_truth.jpg\"\n        self.ground_truth_location = np.array([2, 3])\n        # Prior distribution\n        print(\"initial gaussian\")\n        self.prior_distribution = GaussianDistribution(\n            self.axes,\n            mean=np.array([0.0, 0.0]),\n            cov=np.array([[3, 0], [0, 3]]),\n            dist_theme=\"ellipse\",\n            color=GREEN,\n        )\n        # Define the query images and embedded locations\n        # Contains image paths [(positive_path, negative_path), ...]\n        self.query_image_paths = [\n            (\n                os.path.join(self.assets_path, \"positive_1.jpg\"),\n                os.path.join(self.assets_path, \"negative_1.jpg\"),\n            ),\n            (\n                os.path.join(self.assets_path, \"positive_2.jpg\"),\n                os.path.join(self.assets_path, \"negative_2.jpg\"),\n            ),\n            (\n                os.path.join(self.assets_path, \"positive_3.jpg\"),\n                os.path.join(self.assets_path, \"negative_3.jpg\"),\n            ),\n        ]\n        # Contains 2D locations for each image [([2, 3], [2, 4]), ...]\n        self.query_locations = [\n            (np.array([-1, -1]), np.array([1, 1])),\n            (np.array([1, -1]), np.array([-1, 1])),\n            (np.array([0.3, -0.6]), np.array([-0.5, 0.7])),\n        ]\n        # Make the covariances for each query\n        self.query_covariances = [\n            (np.array([[0.3, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n            (np.array([[0.2, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n            (np.array([[0.2, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n        ]\n        # Posterior distributions over time GaussianDistribution objects\n        self.posterior_distributions = [\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([-0.3, -0.3]),\n                cov=np.array([[5, -4], [-4, 6]]),\n            ).scale(0.6),\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([0.25, -0.25]),\n                cov=np.array([[3, -2], [-2, 4]]),\n            ).scale(0.35),\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([0.4, -0.35]),\n                cov=np.array([[1, 0], [0, 1]]),\n            ).scale(0.3),\n        ]\n        # Some assumptions\n        assert len(self.query_locations) == len(self.query_image_paths)\n        assert len(self.query_locations) == len(self.posterior_distributions)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"Steps through each localization time instance\"\"\"\n        if self.index < len(self.query_image_paths):\n            self.index += 1\n        else:\n            raise StopIteration\n\n        # Return query_paths, query_locations, posterior\n        out_tuple = (\n            self.query_image_paths[self.index],\n            self.query_locations[self.index],\n            self.posterior_distributions[self.index],\n            self.query_covariances[self.index],\n        )\n\n        return out_tuple\n\n\nclass OracleGuidanceVisualization(Scene):\n    def __init__(self):\n        super().__init__()\n        self.neural_network, self.embedding_layer = self.make_vae()\n        self.localizer = iter(Localizer(self.embedding_layer.axes))\n        self.subtitle = None\n        self.title = None\n        # Set image paths\n        # VAE embedding animation image paths\n        self.assets_path = ROOT_DIR / \"assets/oracle_guidance\"\n        self.input_embed_image_path = os.path.join(self.assets_path, \"input_image.jpg\")\n        self.output_embed_image_path = os.path.join(\n            self.assets_path, \"output_image.jpg\"\n        )\n\n    def make_vae(self):\n        \"\"\"Makes a simple VAE architecture\"\"\"\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\")\n        self.encoder = NeuralNetwork(\n            [\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n            ]\n        )\n\n        self.decoder = NeuralNetwork(\n            [\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n            ]\n        )\n\n        neural_network = NeuralNetwork([self.encoder, self.decoder])\n\n        neural_network.shift(DOWN * 0.4)\n        return neural_network, embedding_layer\n\n    @override_animation(Create)\n    def _create_animation(self):\n        animation_group = AnimationGroup(Create(self.neural_network))\n\n        return animation_group\n\n    def insert_at_start(self, layer, create=True):\n        \"\"\"Inserts a layer at the beggining of the network\"\"\"\n        # Note: Does not move the rest of the network\n        current_first_layer = self.encoder.all_layers[0]\n        # Get connective layer\n        connective_layer = get_connective_layer(layer, current_first_layer)\n        # Insert both layers\n        self.encoder.all_layers.insert(0, layer)\n        self.encoder.all_layers.insert(1, connective_layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        layer.shift(DOWN * 0.4)\n        layer.shift(LEFT * 2.35)\n        # Make insert animation\n        if not create:\n            animation_group = AnimationGroup(Create(connective_layer))\n        else:\n            animation_group = AnimationGroup(Create(layer), Create(connective_layer))\n        self.play(animation_group)\n\n    def remove_start_layer(self):\n        \"\"\"Removes the first layer of the network\"\"\"\n        first_layer = self.encoder.all_layers.remove_at_index(0)\n        first_connective = self.encoder.all_layers.remove_at_index(0)\n        # Make remove animations\n        animation_group = AnimationGroup(\n            FadeOut(first_layer), FadeOut(first_connective)\n        )\n\n        self.play(animation_group)\n\n    def insert_at_end(self, layer):\n        \"\"\"Inserts the given layer at the end of the network\"\"\"\n        current_last_layer = self.decoder.all_layers[-1]\n        # Get connective layer\n        connective_layer = get_connective_layer(current_last_layer, layer)\n        # Insert both layers\n        self.decoder.all_layers.add(connective_layer)\n        self.decoder.all_layers.add(layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        layer.shift(DOWN * 0.4)\n        layer.shift(RIGHT * 2.35)\n        # Make insert animation\n        animation_group = AnimationGroup(Create(layer), Create(connective_layer))\n        self.play(animation_group)\n\n    def remove_end_layer(self):\n        \"\"\"Removes the lsat layer of the network\"\"\"\n        first_layer = self.decoder.all_layers.remove_at_index(-1)\n        first_connective = self.decoder.all_layers.remove_at_index(-1)\n        # Make remove animations\n        animation_group = AnimationGroup(\n            FadeOut(first_layer), FadeOut(first_connective)\n        )\n\n        self.play(animation_group)\n\n    def change_title(self, text, title_location=np.array([0, 1.25, 0]), font_size=24):\n        \"\"\"Changes title to the given text\"\"\"\n        if self.title is None:\n            self.title = Text(text, font_size=font_size)\n            self.title.move_to(title_location)\n            self.add(self.title)\n            self.play(Write(self.title), run_time=1)\n            self.wait(1)\n            return\n\n        self.play(Unwrite(self.title))\n        new_title = Text(text, font_size=font_size)\n        new_title.move_to(self.title)\n        self.title = new_title\n        self.wait(0.1)\n        self.play(Write(self.title))\n\n    def change_subtitle(self, text, title_location=np.array([0, 0.9, 0]), font_size=20):\n        \"\"\"Changes subtitle to the next algorithm step\"\"\"\n        if self.subtitle is None:\n            self.subtitle = Text(text, font_size=font_size)\n            self.subtitle.move_to(title_location)\n            self.play(Write(self.subtitle))\n            return\n\n        self.play(Unwrite(self.subtitle))\n        new_title = Text(text, font_size=font_size)\n        new_title.move_to(title_location)\n        self.subtitle = new_title\n        self.wait(0.1)\n        self.play(Write(self.subtitle))\n\n    def make_embed_input_image_animation(self, input_image_path, output_image_path):\n        \"\"\"Makes embed input image animation\"\"\"\n        # insert the input image at the begginging\n        input_image_layer = ImageLayer.from_path(input_image_path)\n        input_image_layer.scale(0.6)\n        current_first_layer = self.encoder.all_layers[0]\n        # Get connective layer\n        connective_layer = get_connective_layer(input_image_layer, current_first_layer)\n        # Insert both layers\n        self.encoder.all_layers.insert(0, input_image_layer)\n        self.encoder.all_layers.insert(1, connective_layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        input_image_layer.shift(DOWN * 0.4)\n        input_image_layer.shift(LEFT * 2.35)\n        # Play full forward pass\n        forward_pass = self.neural_network.make_forward_pass_animation(\n            layer_args={\n                self.encoder: {\n                    self.embedding_layer: {\n                        \"dist_args\": {\n                            \"cov\": np.array([[1.5, 0], [0, 1.5]]),\n                            \"mean\": np.array([0.5, 0.5]),\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": ORANGE,\n                        }\n                    }\n                }\n            }\n        )\n        self.play(forward_pass)\n        # insert the output image at the end\n        output_image_layer = ImageLayer.from_path(output_image_path)\n        output_image_layer.scale(0.6)\n        self.insert_at_end(output_image_layer)\n        # Remove the input and output layers\n        self.remove_start_layer()\n        self.remove_end_layer()\n        # Remove the latent distribution\n        self.play(FadeOut(self.embedding_layer.latent_distribution))\n\n    def make_localization_time_step(self, old_posterior):\n        \"\"\"\n        Performs one query update for the localization procedure\n\n        Procedure:\n        a. Embed query input images\n        b. Oracle is asked a query\n        c. Query is embedded\n        d. Show posterior update\n        e. Show current recomendation\n        \"\"\"\n        # Helper functions\n        def embed_query_to_latent_space(query_locations, query_covariance):\n            \"\"\"Makes animation for a paired query\"\"\"\n            # Assumes first layer of neural network is a PairedQueryLayer\n            # Make the embedding animation\n            # Wait\n            self.play(Wait(1))\n            # Embed the query to latent space\n            self.change_subtitle(\"3. Embed the Query in Latent Space\")\n            # Make forward pass animation\n            self.embedding_layer.paired_query_mode = True\n            # Make embedding embed query animation\n            embed_query_animation = self.encoder.make_forward_pass_animation(\n                run_time=5,\n                layer_args={\n                    self.embedding_layer: {\n                        \"positive_dist_args\": {\n                            \"cov\": query_covariance[0],\n                            \"mean\": query_locations[0],\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": BLUE,\n                        },\n                        \"negative_dist_args\": {\n                            \"cov\": query_covariance[1],\n                            \"mean\": query_locations[1],\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": RED,\n                        },\n                    }\n                },\n            )\n            self.play(embed_query_animation)\n\n        # Access localizer information\n        query_paths, query_locations, posterior_distribution, query_covariances = next(\n            self.localizer\n        )\n        positive_path, negative_path = query_paths\n        # Make subtitle for present user with query\n        self.change_subtitle(\"2. Present User with Query\")\n        # Insert the layer into the encoder\n        query_layer = PairedQueryLayer.from_paths(\n            positive_path, negative_path, grayscale=False\n        )\n        query_layer.scale(0.5)\n        self.insert_at_start(query_layer)\n        # Embed query to latent space\n        query_to_latent_space_animation = embed_query_to_latent_space(\n            query_locations, query_covariances\n        )\n        # Wait\n        self.play(Wait(1))\n        # Update the posterior\n        self.change_subtitle(\"4. Update the Posterior\")\n        # Remove the old posterior\n        self.play(ReplacementTransform(old_posterior, posterior_distribution))\n        \"\"\"\n        self.play(\n            self.embedding_layer.remove_gaussian_distribution(self.localizer.posterior_distribution)\n        )\n        \"\"\"\n        # self.embedding_layer.add_gaussian_distribution(posterior_distribution)\n        # self.localizer.posterior_distribution = posterior_distribution\n        # Remove query layer\n        self.remove_start_layer()\n        # Remove query ellipses\n\n        fade_outs = []\n        for dist in self.embedding_layer.gaussian_distributions:\n            self.embedding_layer.gaussian_distributions.remove(dist)\n            fade_outs.append(FadeOut(dist))\n\n        if not len(fade_outs) == 0:\n            fade_outs = AnimationGroup(*fade_outs)\n            self.play(fade_outs)\n\n        return posterior_distribution\n\n    def make_generate_estimate_animation(self, estimate_image_path):\n        \"\"\"Makes the generate estimate animation\"\"\"\n        # Change embedding layer mode\n        self.embedding_layer.paired_query_mode = False\n        # Sample from posterior distribution\n        # self.embedding_layer.latent_distribution = self.localizer.posterior_distribution\n        emb_to_ff_ind = self.neural_network.all_layers.index_of(self.encoder)\n        embedding_to_ff = self.neural_network.all_layers[emb_to_ff_ind + 1]\n        self.play(embedding_to_ff.make_forward_pass_animation())\n        # Pass through decoder\n        self.play(self.decoder.make_forward_pass_animation(), run_time=1)\n        # Create Image layer after the decoder\n        output_image_layer = ImageLayer.from_path(estimate_image_path)\n        output_image_layer.scale(0.5)\n        self.insert_at_end(output_image_layer)\n        # Wait\n        self.wait(1)\n        # Remove the image at the end\n        print(self.neural_network)\n        self.remove_end_layer()\n\n    def make_triplet_forward_animation(self):\n        \"\"\"Make triplet forward animation\"\"\"\n        # Make triplet layer\n        anchor_path = os.path.join(self.assets_path, \"anchor.jpg\")\n        positive_path = os.path.join(self.assets_path, \"positive.jpg\")\n        negative_path = os.path.join(self.assets_path, \"negative.jpg\")\n        triplet_layer = TripletLayer.from_paths(\n            anchor_path,\n            positive_path,\n            negative_path,\n            grayscale=False,\n            font_size=100,\n            buff=1.05,\n        )\n        triplet_layer.scale(0.10)\n        self.insert_at_start(triplet_layer)\n        # Make latent triplet animation\n        self.play(\n            self.encoder.make_forward_pass_animation(\n                layer_args={\n                    self.embedding_layer: {\n                        \"triplet_args\": {\n                            \"anchor_dist\": {\n                                \"cov\": np.array([[0.3, 0], [0, 0.3]]),\n                                \"mean\": np.array([0.7, 1.4]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": BLUE,\n                            },\n                            \"positive_dist\": {\n                                \"cov\": np.array([[0.2, 0], [0, 0.2]]),\n                                \"mean\": np.array([0.8, -0.4]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": GREEN,\n                            },\n                            \"negative_dist\": {\n                                \"cov\": np.array([[0.4, 0], [0, 0.25]]),\n                                \"mean\": np.array([-1, -1.2]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": RED,\n                            },\n                        }\n                    }\n                },\n                run_time=3,\n            )\n        )\n\n    def construct(self):\n        \"\"\"\n        Makes the whole visualization.\n\n        1. Create the Architecture\n            a. Create the traditional VAE architecture with images\n        2. The Localization Procedure\n        3. The Training Procedure\n        \"\"\"\n        # 1. Create the Architecture\n        self.neural_network.scale(1.2)\n        create_vae = Create(self.neural_network)\n        self.play(create_vae, run_time=3)\n        # Make changing title\n        self.change_title(\"Oracle Guided Image Synthesis\\n      with Relative Queries\")\n        # 2. The Localization Procedure\n        self.change_title(\"The Localization Procedure\")\n        # Make algorithm subtitle\n        self.change_subtitle(\"Algorithm Steps\")\n        # Wait\n        self.play(Wait(1))\n        # Make prior distribution subtitle\n        self.change_subtitle(\"1. Calculate Prior Distribution\")\n        # Draw the prior distribution\n        self.play(Create(self.localizer.prior_distribution))\n        old_posterior = self.localizer.prior_distribution\n        # For N queries update the posterior\n        for query_index in range(self.localizer.num_queries):\n            # Make localization iteration\n            old_posterior = self.make_localization_time_step(old_posterior)\n            self.play(Wait(1))\n            if not query_index == self.localizer.num_queries - 1:\n                # Repeat\n                self.change_subtitle(\"5. Repeat\")\n                # Wait a second\n                self.play(Wait(1))\n        # Generate final estimate\n        self.change_subtitle(\"5. Generate Estimate Image\")\n        # Generate an estimate image\n        estimate_image_path = os.path.join(self.assets_path, \"estimate_image.jpg\")\n        self.make_generate_estimate_animation(estimate_image_path)\n        self.wait(1)\n        # Remove old posterior\n        self.play(FadeOut(old_posterior))\n        # 3. The Training Procedure\n        self.change_title(\"The Training Procedure\")\n        # Make training animation\n        # Do an Image forward pass\n        self.change_subtitle(\"1. Unsupervised Image Reconstruction\")\n        self.make_embed_input_image_animation(\n            self.input_embed_image_path, self.output_embed_image_path\n        )\n        self.wait(1)\n        # Do triplet forward pass\n        self.change_subtitle(\"2. Triplet Loss in Latent Space\")\n        self.make_triplet_forward_animation()\n        self.wait(1)\n", "original_prompt": "Paper Visualizations Oracle Guidance Oracle Guidance"}
{"prompt": "Show a 3D neural network diagram with layers and translations.", "response": "from manim import *\n\nfrom manim_ml.neural_network import FeedForwardLayer, NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1200\nconfig.frame_height = 4.0\nconfig.frame_width = 4.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                FeedForwardLayer(num_nodes=3),\n                FeedForwardLayer(num_nodes=5),\n                FeedForwardLayer(num_nodes=3),\n            ]\n        )\n        self.add(nn)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n", "original_prompt": "Readme Example A Simple Feed Forward Network"}
{"prompt": "Demonstrate a 3D neural network model composed of convolutional and feed-forward layers, with animated forward pass.", "response": "from manim import *\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, filter_spacing=0.32),\n                Convolutional2DLayer(\n                    3, 5, 3, filter_spacing=0.32, activation_function=\"ReLU\"\n                ),\n                FeedForwardLayer(3, activation_function=\"Sigmoid\"),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "original_prompt": "Readme Example Activation Functions"}
{"prompt": "Show a 3D neural network with convolutional and feedforward layers, animated in a forward pass.", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "original_prompt": "Readme Example Animating The Forward Pass"}
{"prompt": "Create a 3D neural network animation with images, convolutional layers and two feedforward layers, moving to the center of the screen.", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n    ImageLayer,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "original_prompt": "Readme Example Convolutional Neural Network With Images"}
{"prompt": "Show a 3D neural network with layered convolutional and feedforward layers transforming over time.", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n", "original_prompt": "Readme Example Convolutional Neural Networks"}
{"prompt": "Demonstrate a 3D neural network with multiple convolutional and feed-forward layers, animating its movement in space.", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "original_prompt": "Readme Example Example"}
{"prompt": "Demonstrate a 3D neural network model with convolutional and feedforward layers, then animate a forward pass.", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "original_prompt": "Readme Example First Neural Network"}
{"prompt": "Show a neural network with 4 layers (convolutions and max pooling) animating a forward pass over 10 seconds.", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass MaxPoolingScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 8),\n                Convolutional2DLayer(3, 6, 3),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(5, 2, 2),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "original_prompt": "Readme Example Max Pooling"}
{"prompt": "Demonstrate a 5-layer neural network with alternating input and hidden layer sizes, showcasing random weights animation.", "response": "from manim import *\nfrom manim_ml.neural_network.animations.dropout import (\n    make_neural_network_dropout_animation,\n)\nfrom manim_ml.neural_network import FeedForwardLayer, NeuralNetwork\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 5.0\nconfig.frame_width = 5.0\n\n\nclass DropoutNeuralNetworkScene(Scene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                FeedForwardLayer(3, rectangle_color=BLUE),\n                FeedForwardLayer(5, rectangle_color=BLUE),\n                FeedForwardLayer(3, rectangle_color=BLUE),\n                FeedForwardLayer(5, rectangle_color=BLUE),\n                FeedForwardLayer(4, rectangle_color=BLUE),\n            ],\n            layer_spacing=0.4,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        self.play(\n            make_neural_network_dropout_animation(\n                nn, dropout_rate=0.25, do_forward_pass=True\n            )\n        )\n        self.wait(1)\n", "original_prompt": "Readme Example Neural Network Dropout"}
{"prompt": "Demonstrate a 2D convolutional neural network's architecture, showcasing layered transformations with images and filters.", "response": "from manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass ConvolutionalNetworkScene(Scene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        self.play(nn.make_forward_pass_animation())\n", "original_prompt": "Readme Example Old Example"}
{"prompt": "\"Create a 3D animation that displays a single, animated text message in the scene.\"", "response": "from manim import *\n\n# Import modules here\n\n\nclass BasicScene(ThreeDScene):\n    def construct(self):\n        # Your code goes here\n        text = Text(\"Your first scene!\")\n        self.add(text)\n", "original_prompt": "Readme Example Setting Up A Scene"}
{"prompt": "Demonstrate a 3D neural network visualization with image input, two convolutional layers, and rotation.", "response": "from manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.parent_layers import ThreeDLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 800\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/doggo.jpeg\")\n        numpy_image = np.asarray(image)\n        # Rotate the Three D layer position\n        ThreeDLayer.rotation_angle = 15 * DEGREES\n        ThreeDLayer.rotation_axis = [1, -1.0, 0]\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, 7, 3, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 5, 1, 1, filter_spacing=0.18),\n            ],\n            layer_spacing=0.25,\n            layout_direction=\"top_to_bottom\",\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.scale(1.5)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation(\n            highlight_active_feature_map=True,\n        )\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Translation Equivariance Translation Equivariance"}
{"prompt": "Demonstrate visualization of digit 2 dataset's latent space, creating a grid of images with corresponding points in a lower-dimensional space.", "response": "import pickle\nimport sys\nimport os\n\nsys.path.append(os.environ[\"PROJECT_ROOT\"])\nfrom autoencoder_models.variational_autoencoder import (\n    VAE,\n    load_dataset,\n    load_vae_from_path,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport scipy\nimport scipy.stats\nimport cv2\n\n\ndef binned_images(model_path, num_x_bins=6, plot=False):\n    latent_dim = 2\n    model = load_vae_from_path(model_path, latent_dim)\n    image_dataset = load_dataset(digit=2)\n    # Compute embedding\n    num_images = 500\n    embedding = []\n    images = []\n    for i in range(num_images):\n        image, _ = image_dataset[i]\n        mean, _, recon, _ = model.forward(image)\n        mean = mean.detach().numpy()\n        recon = recon.detach().numpy()\n        recon = recon.reshape(32, 32)\n        images.append(recon.squeeze())\n        if latent_dim > 2:\n            mean = mean[:2]\n        embedding.append(mean)\n    images = np.stack(images)\n    tsne_points = np.array(embedding)\n    tsne_points = (tsne_points - tsne_points.mean(axis=0)) / (tsne_points.std(axis=0))\n    # make vis\n    num_points = np.shape(tsne_points)[0]\n    x_min = np.amin(tsne_points.T[0])\n    y_min = np.amin(tsne_points.T[1])\n    y_max = np.amax(tsne_points.T[1])\n    x_max = np.amax(tsne_points.T[0])\n    # make the bins from the ranges\n    # to keep it square the same width is used for x and y dim\n    x_bins, step = np.linspace(x_min, x_max, num_x_bins, retstep=True)\n    x_bins = x_bins.astype(float)\n    num_y_bins = np.absolute(np.ceil((y_max - y_min) / step)).astype(int)\n    y_bins = np.linspace(y_min, y_max, num_y_bins)\n    # sort the tsne_points into a 2d histogram\n    tsne_points = tsne_points.squeeze()\n    hist_obj = scipy.stats.binned_statistic_dd(\n        tsne_points,\n        np.arange(num_points),\n        statistic=\"count\",\n        bins=[x_bins, y_bins],\n        expand_binnumbers=True,\n    )\n    # sample one point from each bucket\n    binnumbers = hist_obj.binnumber\n    num_x_bins = np.amax(binnumbers[0]) + 1\n    num_y_bins = np.amax(binnumbers[1]) + 1\n    binnumbers = binnumbers.T\n    # some places have no value in a region\n    used_mask = np.zeros((num_y_bins, num_x_bins))\n    image_bins = np.zeros(\n        (num_y_bins, num_x_bins, 3, np.shape(images)[2], np.shape(images)[2])\n    )\n    for i, bin_num in enumerate(list(binnumbers)):\n        used_mask[bin_num[1], bin_num[0]] = 1\n        image_bins[bin_num[1], bin_num[0]] = images[i]\n    # plot a grid of the images\n    fig, axs = plt.subplots(\n        nrows=np.shape(y_bins)[0],\n        ncols=np.shape(x_bins)[0],\n        constrained_layout=False,\n        dpi=50,\n    )\n    images = []\n    bin_indices = []\n    for y in range(num_y_bins):\n        for x in range(num_x_bins):\n            if used_mask[y, x] > 0.0:\n                image = np.uint8(image_bins[y][x].squeeze() * 255)\n                image = np.rollaxis(image, 0, 3)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                axs[num_y_bins - 1 - y][x].imshow(image)\n                images.append(image)\n                bin_indices.append((y, x))\n            axs[y, x].axis(\"off\")\n    if plot:\n        plt.axis(\"off\")\n        plt.show()\n    else:\n        return images, bin_indices\n\n\ndef generate_disentanglement(model_path=\"saved_models/model_dim2.pth\"):\n    \"\"\"Generates disentanglement visualization and serializes it\"\"\"\n    # Disentanglement object\n    disentanglement_object = {}\n    # Make Disentanglement\n    images, bin_indices = binned_images(model_path)\n    disentanglement_object[\"images\"] = images\n    disentanglement_object[\"bin_indices\"] = bin_indices\n    # Serialize Images\n    with open(\"disentanglement.pkl\", \"wb\") as f:\n        pickle.dump(disentanglement_object, f)\n\n\nif __name__ == \"__main__\":\n    plot = False\n    if plot:\n        model_path = \"saved_models/model_dim2.pth\"\n        # uniform_image_sample(model_path)\n        binned_images(model_path)\n    else:\n        generate_disentanglement()\n", "original_prompt": "Variational Autoencoder Autoencoder Models Generate Disentanglement"}
{"prompt": "Create a side-by-side visualization of original and reconstructed MNIST images.", "response": "import torch\nfrom variational_autoencoder import VAE\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport numpy as np\nimport pickle\n\n# Load model\nvae = VAE(latent_dim=16)\nvae.load_state_dict(torch.load(\"saved_models/model.pth\"))\n# Transforms images to a PyTorch Tensor\ntensor_transform = transforms.ToTensor()\n# Download the MNIST Dataset\ndataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=tensor_transform\n)\n# Generate reconstructions\nnum_recons = 10\nfig, axs = plt.subplots(num_recons, 2, figsize=(2, num_recons))\nimage_pairs = []\nfor i in range(num_recons):\n    base_image, _ = dataset[i]\n    base_image = base_image.reshape(-1, 28 * 28)\n    _, _, recon_image, _ = vae.forward(base_image)\n    base_image = base_image.detach().numpy()\n    base_image = np.reshape(base_image, (28, 28)) * 255\n    recon_image = recon_image.detach().numpy()\n    recon_image = np.reshape(recon_image, (28, 28)) * 255\n    # Add to plot\n    axs[i][0].imshow(base_image)\n    axs[i][1].imshow(recon_image)\n    # image pairs\n    image_pairs.append((base_image, recon_image))\n\nwith open(\"image_pairs.pkl\", \"wb\") as f:\n    pickle.dump(image_pairs, f)\n\nplt.show()\n", "original_prompt": "Variational Autoencoder Autoencoder Models Generate Images"}
{"prompt": "Demonstrate a sequence of images generated by interpolating between two input images using a VAE model.", "response": "import torch\nfrom variational_autoencoder import VAE, load_dataset\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport numpy as np\nimport pickle\n\n# Load model\nvae = VAE(latent_dim=16)\nvae.load_state_dict(torch.load(\"saved_models/model.pth\"))\ndataset = load_dataset()\n# Generate reconstructions\nnum_images = 50\nimage_pairs = []\nsave_object = {\"interpolation_path\": [], \"interpolation_images\": []}\n\n# Make interpolation path\nimage_a, image_b = dataset[0][0], dataset[1][0]\nimage_a = image_a.view(32 * 32)\nimage_b = image_b.view(32 * 32)\nz_a, _, _, _ = vae.forward(image_a)\nz_a = z_a.detach().cpu().numpy()\nz_b, _, _, _ = vae.forward(image_b)\nz_b = z_b.detach().cpu().numpy()\ninterpolation_path = np.linspace(z_a, z_b, num=num_images)\n# interpolation_path[:, 4] = np.linspace(-3, 3, num=num_images)\nsave_object[\"interpolation_path\"] = interpolation_path\n\nfor i in range(num_images):\n    # Generate\n    z = torch.Tensor(interpolation_path[i]).unsqueeze(0)\n    gen_image = vae.decode(z).detach().numpy()\n    gen_image = np.reshape(gen_image, (32, 32)) * 255\n    save_object[\"interpolation_images\"].append(gen_image)\n\nfig, axs = plt.subplots(num_images, 1, figsize=(1, num_images))\nimage_pairs = []\nfor i in range(num_images):\n    recon_image = save_object[\"interpolation_images\"][i]\n    # Add to plot\n    axs[i].imshow(recon_image)\n\n# Perform intrpolations\nwith open(\"interpolations.pkl\", \"wb\") as f:\n    pickle.dump(save_object, f)\n\nplt.show()\n", "original_prompt": "Variational Autoencoder Autoencoder Models Generate Interpolation"}
{"prompt": "Based on the provided code, here is a high-quality, readable, and well-documented version of the code with explanations:\n\n**Variational Autoencoder (VAE) Implementation**\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport os\n\n# Define the VAE model\nclass VAE(nn.Module):\n    def __init__(self, latent_dim, input_dim, hidden_dim):\n        super(VAE, self).__init__()\n        self.latent_dim = latent_dim\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        # Encoder (Bottleneck layer)\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Decoder (Bottleneck layer)\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.ConvTranspose2d(hidden_dim, 1, kernel_size=4, stride=2, padding=1),\n            nn.Sigmoid()\n        )\n\n    def encode(self, x):\n        # Get the mean and log variance of the latent space\n        z = self.encoder(x.view(-1, self.input_dim))\n        return z[:, :self.latent_dim], z[:, self.latent_dim:]\n\n    def decode(self, z):\n        # Generate an image from the latent space\n        reconstructed = self.decoder(z)\n        return reconstructed\n\n    def forward(self, x):\n        mean, log_var = self.encode(x)\n        eps = torch.randn_like(mean)\n        z = mean + torch.exp(log_var / 2) * eps\n        reconstructed = self.decode(z)\n        return mean, log_var, reconstructed, x\n\n# Define the loss function\ndef loss_function(mean, log_var, reconstructed, original, kl_beta=0.0001):\n    # Calculate the Kullback-Leibler (KL) divergence term\n    kl = torch.mean(-0.5 * torch.sum(1 + log_var - mean**2 - log_var.exp(), dim=1), dim=0)\n    # Calculate the Reconstruction loss term\n    recon = torch.nn.functional.mse_loss(reconstructed, original)\n    return kl_beta * kl + recon\n\n# Train the VAE model\ndef train_model(latent_dim, input_dim, hidden_dim, epochs, plot=True):\n    # Load the MNIST dataset\n    transform = transforms.Compose([transforms.ToTensor()])\n    dataset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n\n    # Initialize the VAE model and optimizer\n    model = VAE(latent_dim=latent_dim, input_dim=input_dim, hidden_dim=hidden_dim)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)\n\n    outputs = []\n    losses = []\n    for epoch in range(epochs):\n        for (image, _) in loader:\n            # Forward pass\n            mean, log_var, reconstructed, _ = model(image)\n            loss = loss_function(mean, log_var, reconstructed, image)\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # Store the losses and outputs\n            if torch.isnan(loss):\n                raise Exception()\n            losses.append(loss.detach().cpu())\n            outputs.append((epoch, image, reconstructed))\n\n        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n\n    # Save the trained model\n    torch.save(model.state_dict(), os.path.join(os.environ[\"PROJECT_ROOT\"], f\"examples/variational_autoencoder/autoencoder_model/saved_models/model_dim{latent_dim}.pth\"))\n\n    if plot:\n        plt.style.use(\"fivethirtyeight\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        # Plot the last 100 values\n        plt.plot(losses)\n        plt.show()\n\n# Run the training script\nif __name__ == \"__main__\":\n    latent_dim = 2\n    input_dim = 784\n    hidden_dim = 128\n    epochs = 40\n    train_model(latent_dim=latent_dim, input_dim=input_dim, hidden_dim=hidden_dim, epochs=epochs)\n```\n\n**Explanation**\n\nThis code implements a Variational Autoencoder (VAE) in PyTorch. The VAE is a type of deep learning model that can be used for unsupervised learning and dimensionality reduction.\n\nThe `VAE` class defines the VAE architecture, including the encoder and decoder networks. The `encode` method computes the mean and log variance of the latent space, while the `decode` method generates an image from the latent space. The `forward` method defines the forward pass through the network.\n\nThe `loss_function` function defines the loss function used to train the VAE. It calculates the Kullback-Leibler (KL) divergence term and the Reconstruction loss term.\n\nThe `train_model` function trains the VAE model using stochastic gradient descent (SGD). It loads the MNIST dataset, initializes the VAE model and optimizer, and trains the network for a specified number of epochs.\n\nFinally, the code saves the trained model to disk using PyTorch's `torch.save` method. If the `plot` argument is set to `True`, it plots the loss values over time using Matplotlib.\n\n**Note**\n\nThis code assumes that you have PyTorch and NumPy installed on your system. You can install them using pip: `pip install torch numpy matplotlib`.", "response": "import torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport math\n\n\"\"\"\n    These are utility functions that help to calculate the input and output\n    sizes of convolutional neural networks\n\"\"\"\n\n\ndef num2tuple(num):\n    return num if isinstance(num, tuple) else (num, num)\n\n\ndef conv2d_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    h_w, kernel_size, stride, pad, dilation = (\n        num2tuple(h_w),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(pad),\n        num2tuple(dilation),\n    )\n    pad = num2tuple(pad[0]), num2tuple(pad[1])\n\n    h = math.floor(\n        (h_w[0] + sum(pad[0]) - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1\n    )\n    w = math.floor(\n        (h_w[1] + sum(pad[1]) - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1\n    )\n\n    return h, w\n\n\ndef convtransp2d_output_shape(\n    h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0\n):\n    h_w, kernel_size, stride, pad, dilation, out_pad = (\n        num2tuple(h_w),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(pad),\n        num2tuple(dilation),\n        num2tuple(out_pad),\n    )\n    pad = num2tuple(pad[0]), num2tuple(pad[1])\n\n    h = (\n        (h_w[0] - 1) * stride[0]\n        - sum(pad[0])\n        + dialation[0] * (kernel_size[0] - 1)\n        + out_pad[0]\n        + 1\n    )\n    w = (\n        (h_w[1] - 1) * stride[1]\n        - sum(pad[1])\n        + dialation[1] * (kernel_size[1] - 1)\n        + out_pad[1]\n        + 1\n    )\n\n    return h, w\n\n\ndef conv2d_get_padding(h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1):\n    h_w_in, h_w_out, kernel_size, stride, dilation = (\n        num2tuple(h_w_in),\n        num2tuple(h_w_out),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(dilation),\n    )\n\n    p_h = (\n        (h_w_out[0] - 1) * stride[0]\n        - h_w_in[0]\n        + dilation[0] * (kernel_size[0] - 1)\n        + 1\n    )\n    p_w = (\n        (h_w_out[1] - 1) * stride[1]\n        - h_w_in[1]\n        + dilation[1] * (kernel_size[1] - 1)\n        + 1\n    )\n\n    return (math.floor(p_h / 2), math.ceil(p_h / 2)), (\n        math.floor(p_w / 2),\n        math.ceil(p_w / 2),\n    )\n\n\ndef convtransp2d_get_padding(\n    h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1, out_pad=0\n):\n    h_w_in, h_w_out, kernel_size, stride, dilation, out_pad = (\n        num2tuple(h_w_in),\n        num2tuple(h_w_out),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(dilation),\n        num2tuple(out_pad),\n    )\n\n    p_h = (\n        -(\n            h_w_out[0]\n            - 1\n            - out_pad[0]\n            - dilation[0] * (kernel_size[0] - 1)\n            - (h_w_in[0] - 1) * stride[0]\n        )\n        / 2\n    )\n    p_w = (\n        -(\n            h_w_out[1]\n            - 1\n            - out_pad[1]\n            - dilation[1] * (kernel_size[1] - 1)\n            - (h_w_in[1] - 1) * stride[1]\n        )\n        / 2\n    )\n\n    return (math.floor(p_h / 2), math.ceil(p_h / 2)), (\n        math.floor(p_w / 2),\n        math.ceil(p_w / 2),\n    )\n\n\ndef load_dataset(train=True, digit=None):\n    # Transforms images to a PyTorch Tensor\n    tensor_transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()])\n\n    # Download the MNIST Dataset\n    dataset = datasets.MNIST(\n        root=\"./data\", train=train, download=True, transform=tensor_transform\n    )\n    # Load specific image\n    if not digit is None:\n        idx = dataset.train_labels == digit\n        dataset.targets = dataset.targets[idx]\n        dataset.data = dataset.data[idx]\n\n    return dataset\n\n\ndef load_vae_from_path(path, latent_dim):\n    model = VAE(latent_dim)\n    model.load_state_dict(torch.load(path))\n\n    return model\n\n\n# Creating a PyTorch class\n# 28*28 ==> 9 ==> 28*28\nclass VAE(torch.nn.Module):\n    def __init__(self, latent_dim=5, layer_count=4, channels=1):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.in_shape = 32\n        self.layer_count = layer_count\n        self.channels = channels\n        self.d = 128\n        mul = 1\n        inputs = self.channels\n        out_sizes = [(self.in_shape, self.in_shape)]\n        for i in range(self.layer_count):\n            setattr(self, \"conv%d\" % (i + 1), nn.Conv2d(inputs, self.d * mul, 4, 2, 1))\n            setattr(self, \"conv%d_bn\" % (i + 1), nn.BatchNorm2d(self.d * mul))\n            h_w = (out_sizes[-1][-1], out_sizes[-1][-1])\n            out_sizes.append(\n                conv2d_output_shape(h_w, kernel_size=4, stride=2, pad=1, dilation=1)\n            )\n            inputs = self.d * mul\n            mul *= 2\n\n        self.d_max = inputs\n        self.last_size = out_sizes[-1][-1]\n        self.num_linear = self.last_size**2 * self.d_max\n        # Encoder linear layers\n        self.encoder_mean_linear = nn.Linear(self.num_linear, self.latent_dim)\n        self.encoder_logvar_linear = nn.Linear(self.num_linear, self.latent_dim)\n        # Decoder linear layer\n        self.decoder_linear = nn.Linear(self.latent_dim, self.num_linear)\n\n        mul = inputs // self.d // 2\n\n        for i in range(1, self.layer_count):\n            setattr(\n                self,\n                \"deconv%d\" % (i + 1),\n                nn.ConvTranspose2d(inputs, self.d * mul, 4, 2, 1),\n            )\n            setattr(self, \"deconv%d_bn\" % (i + 1), nn.BatchNorm2d(self.d * mul))\n            inputs = self.d * mul\n            mul //= 2\n\n        setattr(\n            self,\n            \"deconv%d\" % (self.layer_count + 1),\n            nn.ConvTranspose2d(inputs, self.channels, 4, 2, 1),\n        )\n\n    def encode(self, x):\n        if len(x.shape) < 3:\n            x = x.unsqueeze(0)\n        if len(x.shape) < 4:\n            x = x.unsqueeze(1)\n        batch_size = x.shape[0]\n\n        for i in range(self.layer_count):\n            x = F.relu(\n                getattr(self, \"conv%d_bn\" % (i + 1))(\n                    getattr(self, \"conv%d\" % (i + 1))(x)\n                )\n            )\n\n        x = x.view(batch_size, -1)\n\n        mean = self.encoder_mean_linear(x)\n        logvar = self.encoder_logvar_linear(x)\n\n        return mean, logvar\n\n    def decode(self, x):\n        x = x.view(x.shape[0], self.latent_dim)\n        x = self.decoder_linear(x)\n        x = x.view(x.shape[0], self.d_max, self.last_size, self.last_size)\n        # x = self.deconv1_bn(x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(1, self.layer_count):\n            x = F.leaky_relu(\n                getattr(self, \"deconv%d_bn\" % (i + 1))(\n                    getattr(self, \"deconv%d\" % (i + 1))(x)\n                ),\n                0.2,\n            )\n        x = getattr(self, \"deconv%d\" % (self.layer_count + 1))(x)\n        x = torch.sigmoid(x)\n        return x\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        mean, logvar = self.encode(x)\n        eps = torch.randn(batch_size, self.latent_dim)\n        z = mean + torch.exp(logvar / 2) * eps\n        reconstructed = self.decode(z)\n        return mean, logvar, reconstructed, x\n\n\ndef train_model(latent_dim=16, plot=True, digit=1, epochs=200):\n    dataset = load_dataset(train=True, digit=digit)\n    # DataLoader is used to load the dataset\n    # for training\n    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n    # Model Initialization\n    model = VAE(latent_dim=latent_dim)\n    # Validation using MSE Loss function\n    def loss_function(mean, log_var, reconstructed, original, kl_beta=0.0001):\n        kl = torch.mean(\n            -0.5 * torch.sum(1 + log_var - mean**2 - log_var.exp(), dim=1), dim=0\n        )\n        recon = torch.nn.functional.mse_loss(reconstructed, original)\n        # print(f\"KL Error {kl}, Recon Error {recon}\")\n        return kl_beta * kl + recon\n\n    # Using an Adam Optimizer with lr = 0.1\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0e-8)\n\n    outputs = []\n    losses = []\n    for epoch in tqdm(range(epochs)):\n        for (image, _) in loader:\n            # Output of Autoencoder\n            mean, log_var, reconstructed, image = model(image)\n            # Calculating the loss function\n            loss = loss_function(mean, log_var, reconstructed, image)\n            # The gradients are set to zero,\n            # the the gradient is computed and stored.\n            # .step() performs parameter update\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # Storing the losses in a list for plotting\n            if torch.isnan(loss):\n                raise Exception()\n            losses.append(loss.detach().cpu())\n            outputs.append((epochs, image, reconstructed))\n\n    torch.save(\n        model.state_dict(),\n        os.path.join(\n            os.environ[\"PROJECT_ROOT\"],\n            f\"examples/variational_autoencoder/autoencoder_model/saved_models/model_dim{latent_dim}.pth\",\n        ),\n    )\n\n    if plot:\n        # Defining the Plot Style\n        plt.style.use(\"fivethirtyeight\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        # Plotting the last 100 values\n        plt.plot(losses)\n        plt.show()\n\n\nif __name__ == \"__main__\":\n    train_model(latent_dim=2, digit=2, epochs=40)\n", "original_prompt": "Variational Autoencoder Autoencoder Models Variational Autoencoder"}
{"prompt": "Demonstrate a Variational Autoencoder (VAE) neural network animation with images and layers transformations.", "response": "\"\"\"Autoencoder Manim Visualizations\n\nIn this module I define Manim visualizations for Variational Autoencoders\nand Traditional Autoencoders.\n\n\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\nimport numpy as np\nfrom PIL import Image\nfrom manim_ml.neural_network.layers import EmbeddingLayer\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.layers import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass VAEScene(Scene):\n    \"\"\"Scene object for a Variational Autoencoder and Autoencoder\"\"\"\n\n    def construct(self):\n        numpy_image = np.asarray(Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\"))\n        vae = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.4),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(dist_theme=\"ellipse\"),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.4),\n            ]\n        )\n\n        self.play(Create(vae))\n        self.play(vae.make_forward_pass_animation(run_time=15))\n", "original_prompt": "Variational Autoencoder Variational Autoencoder"}
{"prompt": "Create a visual representation of a neural network's forward propagation animation, scaling and moving the layers.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass NeuralNetworkScene(Scene):\n    \"\"\"Test Scene for the Neural Network\"\"\"\n\n    def construct(self):\n        # Make the Layer object\n        layers = [FeedForwardLayer(3), FeedForwardLayer(5), FeedForwardLayer(3)]\n        nn = NeuralNetwork(layers)\n        nn.scale(2)\n        nn.move_to(ORIGIN)\n        # Make Animation\n        self.add(nn)\n        # self.play(Create(nn))\n        forward_propagation_animation = nn.make_forward_pass_animation(\n            run_time=5, passing_flash=True\n        )\n\n        self.play(forward_propagation_animation)\n", "original_prompt": "Basic Neural Network Basic Neural Network"}
{"prompt": "\"Create and animate a neural network diagram, showcasing layer scales and activation, with dynamic flashing\".", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass NeuralNetworkScene(Scene):\n    \"\"\"Test Scene for the Neural Network\"\"\"\n\n    def construct(self):\n        # Make the Layer object\n        layers = [FeedForwardLayer(3), FeedForwardLayer(5), FeedForwardLayer(3)]\n        nn = NeuralNetwork(layers)\n        nn.scale(2)\n        nn.move_to(ORIGIN)\n        # Make Animation\n        self.add(nn)\n        # self.play(Create(nn))\n        forward_propagation_animation = nn.make_forward_pass_animation(\n            run_time=5, passing_flash=True\n        )\n\n        self.play(forward_propagation_animation)\n", "original_prompt": "Basic Neural Network Basic Neural Network"}
{"prompt": "Demonstrate a 3D animated neural network with input and output dots, connections, and code snippet visualization.", "response": "from manim import *\n\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.math_operation_layer import MathOperationLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        nn = NeuralNetwork({\n            \"feed_forward_1\": FeedForwardLayer(3),\n            \"feed_forward_2\": FeedForwardLayer(3, activation_function=\"ReLU\"),\n            \"feed_forward_3\": FeedForwardLayer(3),\n            \"sum_operation\": MathOperationLayer(\"+\", activation_function=\"ReLU\"),\n        })\n        nn.add_connection(\"feed_forward_1\", \"sum_operation\")\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Add the network\n        nn = NeuralNetwork({\n                \"feed_forward_1\": FeedForwardLayer(3),\n                \"feed_forward_2\": FeedForwardLayer(3, activation_function=\"ReLU\"),\n                \"feed_forward_3\": FeedForwardLayer(3),\n                \"sum_operation\": MathOperationLayer(\"+\", activation_function=\"ReLU\"),\n            },\n            layer_spacing=0.38\n        )\n        # Make connections\n        input_blank_dot = Dot(\n            nn.input_layers_dict[\"feed_forward_1\"].get_left() - np.array([0.65, 0.0, 0.0])\n        )\n        nn.add_connection(input_blank_dot, \"feed_forward_1\", arc_direction=\"straight\")\n        nn.add_connection(\"feed_forward_1\", \"sum_operation\")\n        output_blank_dot = Dot(\n            nn.input_layers_dict[\"sum_operation\"].get_right() + np.array([0.65, 0.0, 0.0])\n        )\n        nn.add_connection(\"sum_operation\", output_blank_dot, arc_direction=\"straight\")\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        # Group it all\n        group = Group(nn, code)\n        group.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)", "original_prompt": "Basic Neural Network Residual Block"}
{"prompt": "Demonstrate a 3D neural network visualization with convolutional and feedforward layers, scaling and positioning it on the screen.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork([\n            # ... Layers at start\n            Convolutional2DLayer(3, 5, 3, activation_function=\"ReLU\"),\n            # ... Layers at end\n        ])\n        # Play the animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        font=\"Monospace\",\n        background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.45)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7),\n                Convolutional2DLayer(3, 5, 3, activation_function=\"ReLU\"),\n                Convolutional2DLayer(5, 3, 3, activation_function=\"ReLU\"),\n                FeedForwardLayer(3),\n                FeedForwardLayer(1),\n            ],\n            layer_spacing=0.25,\n        )\n        # nn.scale(0.7)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # nn.move_to(ORIGIN)\n        # Move everything up\n        # Group(nn, code).move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn Activation Functions"}
{"prompt": "Create a visual representation of a neural network's layers and show how its components animate during a forward pass, with key elements including image and convolutional layer transformations.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image, height=1.5),\n            Convolutional2DLayer(1, 7, 3),\n            Convolutional2DLayer(3, 5, 3),\n            Convolutional2DLayer(5, 3, 1),\n            FeedForwardLayer(3),\n            FeedForwardLayer(3),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.50)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # Group it all\n        # group = Group(nn, code)\n        # group.move_to(ORIGIN)\n        nn.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn Cnn"}
{"prompt": "Demonstrate a neural network's 2D convolutional architecture animation, showing layers with filters and spatial transformations over time.", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork([\n            ImageLayer(image),\n            Convolutional2DLayer(1, 8),\n            MaxPooling2DLayer(kernel_size=2),\n            Convolutional2DLayer(3, 2, 3),\n        ])\n        # Play the animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        font=\"Monospace\",\n        background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.4)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 8, filter_spacing=0.32),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(3, 2, 3, filter_spacing=0.32),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        Group(code, nn).move_to(ORIGIN)\n        self.add(code)\n        self.wait(5)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn Cnn Max Pool"}
{"prompt": "Demonstrate a neural network scene with 3D layers and animations, showcasing convolutional layers and image processing.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image, height=1.5),\n            Convolutional2DLayer(1, 5, 5, 1, 1),\n            Convolutional2DLayer(4, 5, 5, 1, 1),\n            Convolutional2DLayer(2, 5, 5),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.50)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 5, 1, filter_spacing=0.32),\n                Convolutional2DLayer(4, 5, 1, filter_spacing=0.32),\n                Convolutional2DLayer(2, 5, 5, filter_spacing=0.32),\n            ],\n            layer_spacing=0.4,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        # Group it all\n        group = Group(nn, code)\n        group.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn One By One Convolution"}
{"prompt": "Demonstrate a 3D neural network visual with multiple layers, including convolutional and feed-forward layers.", "response": "from manim import *\nfrom manim_ml.neural_network.layers.image import ImageLayer\nimport numpy as np\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image),\n            Convolutional2DLayer(1, 6, 1, padding=1),\n            Convolutional2DLayer(3, 6, 3),\n            FeedForwardLayer(3),\n            FeedForwardLayer(1),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(\n                    num_feature_maps=1,\n                    feature_map_size=6,\n                    padding=1,\n                    padding_dashed=True,\n                ),\n                Convolutional2DLayer(\n                    num_feature_maps=3,\n                    feature_map_size=6,\n                    filter_size=3,\n                    padding=0,\n                    padding_dashed=False,\n                ),\n                FeedForwardLayer(3),\n                FeedForwardLayer(1),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        self.add(nn)\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        Group(code, nn).move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass, run_time=20)\n", "original_prompt": "Cnn Padding Example"}
{"prompt": "\"Create a 3D animation of a neural network's forward pass, demonstrating convolutional layers and residual connections.\"", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\nfrom manim_ml.neural_network import Convolutional2DLayer, NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork({\n            \"layer1\": Convolutional2DLayer(1, 5, padding=1),\n            \"layer2\": Convolutional2DLayer(1, 5, 3, padding=1),\n            \"layer3\": Convolutional2DLayer(1, 5, 3, padding=1)\n        })\n        # Add the residual connection\n        nn.add_connection(\"layer1\", \"layer3\")\n        # Make the animation\n        self.play(nn.make_forward_pass_animation())\n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass ConvScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n\n        nn = NeuralNetwork(\n            {\n                \"layer1\": Convolutional2DLayer(1, 5, padding=1),\n                \"layer2\": Convolutional2DLayer(1, 5, 3, padding=1),\n                \"layer3\": Convolutional2DLayer(1, 5, 3, padding=1),\n            },\n            layer_spacing=0.25,\n        )\n\n        nn.add_connection(\"layer1\", \"layer3\")\n\n        self.add(nn)\n\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        Group(code, nn).move_to(ORIGIN)\n\n        self.play(nn.make_forward_pass_animation(), run_time=8)\n", "original_prompt": "Cnn Resnet Block"}
{"prompt": "Create an animation demonstrating a neural network's forward pass, scaling layers and rotating images.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass ImageNeuralNetworkScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make image object\n            image = Image.open('images/image.jpeg')\n            numpy_image = np.asarray(image)\n            # Make Neural Network\n            layers = [\n                ImageLayer(numpy_image, height=1.4),\n                FeedForwardLayer(3), \n                FeedForwardLayer(5),\n                FeedForwardLayer(3)\n            ]\n            nn = NeuralNetwork(layers)\n            self.add(nn)\n            # Play animation\n            self.play(\n                nn.make_forward_pass_animation()\n            )\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            background_stroke_color=WHITE,\n            insert_line_no=False,\n            style=\"monokai\",\n            # background=\"window\",\n            language=\"py\",\n        )\n        code.scale(0.2)\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        layers = [\n            ImageLayer(numpy_image, height=1.4),\n            FeedForwardLayer(3),\n            FeedForwardLayer(5),\n            FeedForwardLayer(3),\n            FeedForwardLayer(6),\n        ]\n        nn = NeuralNetwork(layers)\n        nn.scale(0.9)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.rotate(-PI / 2)\n        nn.layers[0].image_mobject.rotate(PI / 2)\n        nn.layers[0].image_mobject.shift([0, -0.4, 0])\n        nn.shift([1.5, 0.3, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(1.9)\n        code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(run_time=10))\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Feed Forward Neural Network\n    ffnn_scene = FeedForwardNeuralNetworkScene()\n    ffnn_scene.render()\n    # Neural Network\n    nn_scene = NeuralNetworkScene()\n    nn_scene.render()\n", "original_prompt": "Code Snippet Image Nn Code Snippet"}
{"prompt": "Demonstrate a neural network diagram with an animation of forward pass, scaling and positioning layers.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer, EmbeddingLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 720\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass VAECodeSnippetScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make Neural Network\n            nn = NeuralNetwork([\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ], layer_spacing=0.1)\n            # Play animation\n            self.play(nn.make_forward_pass_animation())\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            # background_stroke_color=WHITE,\n            insert_line_no=False,\n            background=\"window\",\n            # font=\"Monospace\",\n            style=\"one-dark\",\n            language=\"py\",\n        )\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\", point_radius=0.04).scale(\n            1.0\n        )\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ],\n            layer_spacing=0.1,\n        )\n\n        nn.scale(1.1)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        # nn.rotate(-PI/2)\n        # nn.all_layers[0].image_mobject.rotate(PI/2)\n        # nn.all_layers[0].image_mobject.shift([0, -0.4, 0])\n        # nn.all_layers[-1].image_mobject.rotate(PI/2)\n        # nn.all_layers[-1].image_mobject.shift([0, -0.4, 0])\n        nn.shift([0, -1.4, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(0.52)\n        code_snippet.shift([0, 1.25, 0])\n        # code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(), run_time=10)\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Neural Network\n    nn_scene = VAECodeSnippetScene()\n    nn_scene.render()\n", "original_prompt": "Code Snippet Vae Code Landscape"}
{"prompt": "Demonstrate a 3D neural network animation, scaling layers and rotating images in a Python code snippet.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer, EmbeddingLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass VAECodeSnippetScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make image object\n            image = Image.open('images/image.jpeg')\n            numpy_image = np.asarray(image)\n            # Make Neural Network\n            nn = NeuralNetwork([\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ], layer_spacing=0.1)\n            self.add(nn)\n            # Play animation\n            self.play(\n                nn.make_forward_pass_animation()\n            )\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            # background_stroke_color=WHITE,\n            insert_line_no=False,\n            background=\"window\",\n            # font=\"Monospace\",\n            style=\"one-dark\",\n            language=\"py\",\n        )\n        code.scale(0.2)\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\", point_radius=0.04).scale(\n            1.0\n        )\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.0),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.0),\n            ],\n            layer_spacing=0.1,\n        )\n\n        nn.scale(0.65)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.rotate(-PI / 2)\n        nn.all_layers[0].image_mobject.rotate(PI / 2)\n        # nn.all_layers[0].image_mobject.shift([0, -0.4, 0])\n        nn.all_layers[-1].image_mobject.rotate(PI / 2)\n        # nn.all_layers[-1].image_mobject.shift([0, -0.4, 0])\n        nn.shift([1.5, 0.0, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(1.9)\n        code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(), run_time=10)\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Neural Network\n    nn_scene = VAECodeSnippetScene()\n    nn_scene.render()\n", "original_prompt": "Code Snippet Vae Nn Code Snippet"}
{"prompt": "Here is a high-quality, readable, and well-documented code for the given animation:\n```python\nimport random\nfrom vpython import *\n\n# Set up the scene\nscene = canvas(title='Image Segmentation', width=800, height=600)\n\n# Define colors\nWHITE = color(1, 1, 1)\nBLACK = color(0, 0, 0)\n\n# Function to create a random image patch\ndef create_image_patch(image_width, image_height):\n    return rect(pos=vector(0, 0, 0), size=vector(image_width, image_height), color=WHITE)\n\n# Define the scene elements\nimage_mobject = sphere(pos=vector(-1, -2, 0), radius=0.05)\ntext_prompt = text(paragraph Prompt, font_size=64)\nprompt_title = text('Prompt', font_size=72)\n\n# Create arrows for visual tokens\ndef create_arrow(label_text, token_position):\n    return arrow(pos=label_text.pos, direction=token_position, color=BROWN)\n\narrow_animations = []\n\n# Define the image segmentation steps\ndef step_1(image_mobject):\n    # Show the image being split into patches\n    patches = [create_image_patch(32, 32) for _ in range(random.randint(10, 20))]\n    \n    self.play(ExpandPatches(patches, expansion_scale=1.2))\n    \n    arrow_animations.append(Create(arrows))\n\ndef step_2(image_mobject):\n    # Show the text prompt and image being tokenized.\n    tokenized_text = TokenizedText(paragraph Prompt, pipe.tokenizer)\n    \n    self.play(FadeIn(tokenized_text))\n    \n    # Plot token label\n    token_label = text('Textual Tokens', font_size=72)\n    \n    arrow_animations.append(Create(token_labels))\n\ndef step_3(image_mobject):\n    # Show the heatmap corresponding to the cross attention map for each image.\n    surrounding_rectangle = SurroundingRectangle(text Prompt, buff=0.1)\n    self.add(surrounding_rectangle)\n    \n    # Show the heatmap for \"astronaut\"\n    astronaut_heatmap = heatmap_dict[\"astronaut\"]\n    astronaut_heatmap.height = image_mobject.get_height()\n    astronaut_heatmap.shift(RIGHT * 7)\n    astronaut_heatmap.shift(DOWN)\n    \n    self.play(FadeIn(astronaut_heatmap))\n    \n    # Show the heatmap for \"horse\"\n    horse_heatmap = heatmap_dict[\"horse\"]\n    horse_heatmap.height = image_mobject.get_height()\n    horse_heatmap.move_to(astronaut_heatmap)\n    self.play(FadeOut(astronaut_heatmap))\n    self.play(FadeIn(horse_heatmap))\n    \n    # Show the heatmap for \"riding\"\n    riding_heatmap = heatmap_dict[\"riding\"]\n    riding_heatmap.height = image_mobject.get_height()\n    riding_heatmap.move_to(horse_heatmap)\n    self.play(FadeOut(horse_heatmap))\n    self.play(FadeIn(riding_heatmap))\n\n# Define the animation steps\ndef create_image_segmentation():\n    # Step 1: Show the image being split into patches\n    step_1(image_mobject)\n    \n    # Wait for a moment to allow the user to see the patching process\n    \n    # Step 2: Show the text prompt and image being tokenized.\n    step_2(image_mobject)\n    \n    # Wait for another moment to allow the user to see the tokenization process\n    \n    # Step 3: Show the heatmap corresponding to the cross attention map for each image.\n    step_3(image_mobject)\n\n# Define the animation\ndef main():\n    create_image_segmentation()\n\n# Call the main function\nmain()\n```\nThis code defines a scene with an image, text prompt, and arrows for visual tokens. It then performs three steps:\n\n1.  **Step 1**: The image is split into patches.\n2.  **Step 2**: The text prompt and image are tokenized.\n3.  **Step 3**: The heatmap corresponding to the cross attention map for each image is shown.\n\nThe animation uses `vpython` functions like `Create`, `ExpandPatches`, `FadeIn`, `FadeOut`, and `arrow` to create the scene elements and animate them.", "response": "\"\"\"\n    Here I thought it would be interesting to visualize the cross attention\n    maps produced by the UNet of a text-to-image diffusion model. \n\n    The key thing I want to show is how images and text are broken up into tokens (patches for images),\n    and those tokens are used to compute a cross attention score, which can be interpreted\n    as a 2D heatmap over the image patches for each text token. \n    \n\n    Necessary operations:\n        1. [X] Split an image into patches and \"expand\" the patches outward to highlight that they are \n            separate. \n        2. Split text into tokens using the tokenizer and display them. \n        3. Compute the cross attention scores (using DAAM) for each word/token and display them as a heatmap.\n        4. Overlay the heatmap over the image patches. Show the overlaying as a transition animation. \n\"\"\"\nimport torch\nimport cv2\nfrom manim import *\nimport numpy as np\nfrom typing import List\nfrom daam import trace, set_seed\nfrom diffusers import StableDiffusionPipeline\nimport torchvision\nimport matplotlib.pyplot as plt\n\nclass ImagePatches(Group):\n    \"\"\"Container object for a grid of ImageMobjects.\"\"\"\n\n    def __init__(self, image: ImageMobject, grid_width=4):\n        \"\"\"\n        Parameters\n        ----------\n        image : ImageMobject\n            image to split into patches\n        grid_width : int, optional\n            number of patches per row, by default 4\n        \"\"\"\n        self.image = image\n        self.grid_width = grid_width\n        super(Group, self).__init__()\n        self.patch_dict = self._split_image_into_patches(image, grid_width)\n\n    def _split_image_into_patches(self, image, grid_width):\n        \"\"\"Splits the images into a set of patches\n\n        Parameters\n        ----------\n        image : ImageMobject\n            image to split into patches\n        grid_width : int\n            number of patches per row\n        \"\"\"\n        patch_dict = {}\n        # Get a pixel array of the image mobject\n        pixel_array = image.pixel_array\n        # Get the height and width of the image\n        height, width = pixel_array.shape[:2]\n        # Split the image into an equal width grid of patches\n        assert height == width, \"Image must be square\"\n        assert height % grid_width == 0, \"Image width must be divisible by grid_width\"\n        patch_width, patch_height = width // grid_width, height // grid_width\n\n        for patch_i in range(self.grid_width):\n            for patch_j in range(self.grid_width):\n                # Get patch pixels\n                i_start, i_end = patch_i * patch_width, (patch_i + 1) * patch_width\n                j_start, j_end = patch_j * patch_height, (patch_j + 1) * patch_height\n                patch_pixels = pixel_array[i_start:i_end, j_start:j_end, :]\n                # Make the patch\n                image_patch = ImageMobject(patch_pixels)\n                # Move the patch to the correct location on the old image\n                image_width = image.get_width()\n                image_center = image.get_center()\n                image_patch.scale(image_width / grid_width / image_patch.get_width())\n                patch_manim_space_width = image_patch.get_width()\n\n                patch_center = image_center\n                patch_center += (patch_j - self.grid_width / 2 + 0.5) * patch_manim_space_width * RIGHT\n                # patch_center = image_center - (patch_i - self.grid_width / 2 + 0.5) * patch_manim_space_width * RIGHT\n                patch_center -= (patch_i - self.grid_width / 2 + 0.5) * patch_manim_space_width * UP\n                # + (patch_j - self.grid_width / 2) * patch_height / 2 * UP\n\n                image_patch.move_to(patch_center)\n\n                self.add(image_patch)\n                patch_dict[(patch_i, patch_j)] = image_patch\n\n        return patch_dict\n\nclass ExpandPatches(Animation):\n\n    def __init__(self, image_patches: ImagePatches, expansion_scale=2.0):\n        \"\"\"\n        Parameters\n        ----------\n        image_patches : ImagePatches\n            set of image patches\n        expansion_scale : float, optional\n            scale factor for expansion, by default 2.0\n        \"\"\"\n        self.image_patches = image_patches\n        self.expansion_scale = expansion_scale\n        super().__init__(image_patches)\n\n    def interpolate_submobject(self, submobject, starting_submobject, alpha):\n        \"\"\"\n        Parameters\n        ----------\n        submobject : ImageMobject\n            current patch\n        starting_submobject : ImageMobject\n            starting patch\n        alpha : float\n            interpolation alpha\n        \"\"\"\n        patch = submobject\n        starting_patch_center = starting_submobject.get_center()\n        image_center = self.image_patches.image.get_center()\n        # Start image vector\n        starting_patch_vector = starting_patch_center - image_center\n        final_image_vector = image_center + starting_patch_vector * self.expansion_scale\n        # Interpolate vectors\n        current_location = alpha * final_image_vector + (1 - alpha) * starting_patch_center\n        # # Need to compute the direction of expansion as the unit vector from the original image center\n        # # and patch center. \n        patch.move_to(current_location)\n\nclass TokenizedText(Group):\n    \"\"\"Tokenizes the given text and displays the tokens as a list of Text Mobjects.\"\"\"\n\n    def __init__(self, text, tokenizer=None):\n        self.text = text\n        if not tokenizer is None:\n            self.tokenizer = tokenizer\n        else:\n            # TODO load default stable diffusion tokenizer here\n            raise NotImplementedError(\"Tokenizer must be provided\")\n\n        self.token_strings = self._tokenize_text(text)\n        self.token_mobjects = self.make_text_mobjects(self.token_strings)\n\n        super(Group, self).__init__()\n        self.add(*self.token_mobjects)\n        self.arrange(RIGHT, buff=-0.05, aligned_edge=UP)\n\n        token_dict = {} \n        for token_index, token_string in enumerate(self.token_strings):\n            token_dict[token_string] = self.token_mobjects[token_index]\n\n        self.token_dict = token_dict\n\n    def _tokenize_text(self, text):\n        \"\"\"Tokenize the text using the tokenizer.\n\n        Parameters\n        ----------\n        text : str\n            text to tokenize\n        \"\"\"\n        tokens = self.tokenizer.tokenize(text)\n        tokens = [token.replace(\"</w>\", \"\") for token in tokens]\n        return tokens\n\n    def make_text_mobjects(self, tokens_list: List[str]):\n        \"\"\"Converts the tokens into a list of TextMobjects.\"\"\"\n        # Make the tokens\n        # Insert phantom\n        tokens_list = [\"l\" + token + \"g\" for token in tokens_list]\n        token_objects = [\n            Text(token, t2c={'[-1:]': '#000000', '[0:1]': \"#000000\"}, font_size=64) \n            for token in tokens_list\n        ]\n\n        return token_objects\n\ndef compute_stable_diffusion_cross_attention_heatmaps(\n    pipe,\n    prompt: str,\n    seed: int = 2,\n    map_resolution=(32, 32)\n):\n    \"\"\"Computes the cross attention heatmaps for the given prompt.\n\n    Parameters\n    ----------\n    prompt : str\n        the prompt\n    seed : int, optional\n        random seed, by default 0\n    map_resolution : tuple, optional\n        resolution to downscale maps to, by default (16, 16)\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    # Get tokens\n    tokens = pipe.tokenizer.tokenize(prompt)\n    tokens = [token.replace(\"</w>\", \"\") for token in tokens]\n    # Set torch seeds\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    gen = set_seed(seed)  # for reproducibility\n\n    heatmap_dict = {}\n    image = None\n\n    with torch.cuda.amp.autocast(dtype=torch.float16), torch.no_grad():\n        with trace(pipe) as tc:\n            out = pipe(prompt, num_inference_steps=30, generator=gen)\n            image = out[0][0]\n            global_heat_map = tc.compute_global_heat_map()\n            for token in tokens: \n                word_heat_map = global_heat_map.compute_word_heat_map(token)\n                heatmap = word_heat_map.heatmap\n\n                # Downscale the heatmap\n                heatmap = heatmap.unsqueeze(0).unsqueeze(0)\n                # Save the heatmap\n                heatmap = torchvision.transforms.Resize(\n                    map_resolution, \n                    interpolation=torchvision.transforms.InterpolationMode.NEAREST\n                )(heatmap)\n                heatmap = heatmap.squeeze(0).squeeze(0)\n                # heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n                # plt.imshow(heatmap)\n                # plt.savefig(f\"{token}.png\")\n                # Convert heatmap to rgb color\n                # normalize\n                heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n                cmap = plt.get_cmap('inferno')\n                heatmap = cmap(heatmap) * 255\n                print(heatmap)\n\n                # print(heatmap)\n                # Make an image mobject for each heatmap\n                print(heatmap.shape)\n                heatmap = ImageMobject(heatmap)\n                heatmap.set_resampling_algorithm(RESAMPLING_ALGORITHMS[\"nearest\"])\n                heatmap_dict[token] = heatmap\n\n    return image, heatmap_dict\n\n# Make the scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 30.0\nconfig.frame_width = 30.0\n\nclass StableDiffusionCrossAttentionScene(Scene):\n\n    def construct(self):\n        # Load the pipeline\n        model_id = 'stabilityai/stable-diffusion-2-base'\n        pipe = StableDiffusionPipeline.from_pretrained(model_id)\n        prompt = \"Astronaut riding a horse on the moon\"\n        # Compute the images and heatmaps\n        image, heatmap_dict = compute_stable_diffusion_cross_attention_heatmaps(pipe, prompt)\n        # 1. Show an image appearing\n        image_mobject = ImageMobject(image)\n        image_mobject.shift(DOWN)\n        image_mobject.scale(0.7)\n        image_mobject.shift(LEFT * 7)\n        self.add(image_mobject)\n        # 1. Show a text prompt and the corresponding generated image\n        paragraph_prompt = '\"Astronaut riding a\\nhorse on the moon\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        text_prompt.next_to(image_mobject, UP, buff=1.5)\n        prompt_title = Text(\"Prompt\", font_size=72)\n        prompt_title.next_to(text_prompt, UP, buff=0.5)\n        self.play(Create(prompt_title))\n        self.play(Create(text_prompt))        \n        # Make an arrow from the text to the image\n        arrow = Arrow(text_prompt.get_bottom(), image_mobject.get_top(), buff=0.5)\n        self.play(GrowArrow(arrow))\n        self.wait(1)\n        self.remove(arrow)\n        # 2. Show the image being split into patches\n        # Make the patches\n        patches = ImagePatches(image_mobject, grid_width=32)\n        # Expand and contract \n        self.remove(image_mobject)\n        self.play(ExpandPatches(patches, expansion_scale=1.2))\n        # Play arrows for visual tokens\n        visual_token_label = Text(\"Visual Tokens\", font_size=72)\n        visual_token_label.next_to(image_mobject, DOWN, buff=1.5)\n        # Draw arrows \n        arrow_animations = []\n        arrows = []\n        for i in range(patches.grid_width):\n            patch = patches.patch_dict[(patches.grid_width - 1, i)]\n            arrow = Line(visual_token_label.get_top(), patch.get_bottom(), buff=0.3)\n            arrow_animations.append(\n                Create(\n                    arrow\n                )\n            )\n            arrows.append(arrow)\n        self.play(AnimationGroup(*arrow_animations, FadeIn(visual_token_label), lag_ratio=0))\n        self.wait(1)\n        self.play(FadeOut(*arrows, visual_token_label))\n        self.play(ExpandPatches(patches, expansion_scale=1/1.2))\n        # 3. Show the text prompt and image being tokenized.\n        tokenized_text = TokenizedText(prompt, pipe.tokenizer)\n        tokenized_text.shift(RIGHT * 7)\n        tokenized_text.shift(DOWN * 7.5)\n        self.play(FadeIn(tokenized_text))\n        # Plot token label\n        token_label = Text(\"Textual Tokens\", font_size=72)\n        token_label.next_to(tokenized_text, DOWN, buff=0.5)\n        arrow_animations = []\n        self.play(Create(token_label)) \n        arrows = []\n        for token_name, token_mobject in tokenized_text.token_dict.items():\n            arrow = Line(token_label.get_top(), token_mobject.get_bottom(), buff=0.3)\n            arrow_animations.append(\n                Create(\n                    arrow\n                )\n            )\n            arrows.append(arrow)\n        self.play(AnimationGroup(*arrow_animations, lag_ratio=0))\n        self.wait(1)\n        self.play(FadeOut(*arrows, token_label))\n        # 4. Show the heatmap corresponding to the cross attention map for each image. \n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"astronaut\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"astronaut\"\n        astronaut_heatmap = heatmap_dict[\"astronaut\"]\n        astronaut_heatmap.height = image_mobject.get_height()\n        astronaut_heatmap.shift(RIGHT * 7)\n        astronaut_heatmap.shift(DOWN)\n        self.play(FadeIn(astronaut_heatmap))\n        self.wait(3)\n        self.remove(surrounding_rectangle)\n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"horse\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"horse\"\n        horse_heatmap = heatmap_dict[\"horse\"]\n        horse_heatmap.height = image_mobject.get_height()\n        horse_heatmap.move_to(astronaut_heatmap)\n        self.play(FadeOut(astronaut_heatmap))\n        self.play(FadeIn(horse_heatmap))\n        self.wait(3)\n        self.remove(surrounding_rectangle)\n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"riding\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"riding\"\n        riding_heatmap = heatmap_dict[\"riding\"]\n        riding_heatmap.height = image_mobject.get_height()\n        riding_heatmap.move_to(horse_heatmap)\n        self.play(FadeOut(horse_heatmap))\n        self.play(FadeIn(riding_heatmap))\n        self.wait(3)\n", "original_prompt": "Cross Attention Vis Cross Attention Vis"}
{"prompt": "Create a visualization of decision areas around a DecisionTreeClassifier using AABB axis-aligned bounding boxes and clipped rectangles, demonstrating key feature transformations and rectangle colors.", "response": "import numpy as np\nfrom collections import deque\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import _tree as ctree\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n\nclass AABB:\n    \"\"\"Axis-aligned bounding box\"\"\"\n\n    def __init__(self, n_features):\n        self.limits = np.array([[-np.inf, np.inf]] * n_features)\n\n    def split(self, f, v):\n        left = AABB(self.limits.shape[0])\n        right = AABB(self.limits.shape[0])\n        left.limits = self.limits.copy()\n        right.limits = self.limits.copy()\n        left.limits[f, 1] = v\n        right.limits[f, 0] = v\n\n        return left, right\n\n\ndef tree_bounds(tree, n_features=None):\n    \"\"\"Compute final decision rule for each node in tree\"\"\"\n    if n_features is None:\n        n_features = np.max(tree.feature) + 1\n    aabbs = [AABB(n_features) for _ in range(tree.node_count)]\n    queue = deque([0])\n    while queue:\n        i = queue.pop()\n        l = tree.children_left[i]\n        r = tree.children_right[i]\n        if l != ctree.TREE_LEAF:\n            aabbs[l], aabbs[r] = aabbs[i].split(tree.feature[i], tree.threshold[i])\n            queue.extend([l, r])\n    return aabbs\n\n\ndef decision_areas(tree_classifier, maxrange, x=0, y=1, n_features=None):\n    \"\"\"Extract decision areas.\n\n    tree_classifier: Instance of a sklearn.tree.DecisionTreeClassifier\n    maxrange: values to insert for [left, right, top, bottom] if the interval is open (+/-inf)\n    x: index of the feature that goes on the x axis\n    y: index of the feature that goes on the y axis\n    n_features: override autodetection of number of features\n    \"\"\"\n    tree = tree_classifier.tree_\n    aabbs = tree_bounds(tree, n_features)\n    maxrange = np.array(maxrange)\n    rectangles = []\n    for i in range(len(aabbs)):\n        if tree.children_left[i] != ctree.TREE_LEAF:\n            continue\n        l = aabbs[i].limits\n        r = [l[x, 0], l[x, 1], l[y, 0], l[y, 1], np.argmax(tree.value[i])]\n        # clip out of bounds indices\n        \"\"\"\n        if r[0] < maxrange[0]:\n            r[0] = maxrange[0]\n        if r[1] > maxrange[1]:\n            r[1] = maxrange[1]\n        if r[2] < maxrange[2]:\n            r[2] = maxrange[2]\n        if r[3] > maxrange[3]:\n            r[3] = maxrange[3]\n        print(r)\n        \"\"\"\n        rectangles.append(r)\n    rectangles = np.array(rectangles)\n    rectangles[:, [0, 2]] = np.maximum(rectangles[:, [0, 2]], maxrange[0::2])\n    rectangles[:, [1, 3]] = np.minimum(rectangles[:, [1, 3]], maxrange[1::2])\n    return rectangles\n\n\ndef plot_areas(rectangles):\n    for rect in rectangles:\n        color = [\"b\", \"r\"][int(rect[4])]\n        print(rect[0], rect[1], rect[2] - rect[0], rect[3] - rect[1])\n        rp = Rectangle(\n            [rect[0], rect[2]],\n            rect[1] - rect[0],\n            rect[3] - rect[2],\n            color=color,\n            alpha=0.3,\n        )\n        plt.gca().add_artist(rp)\n", "original_prompt": "Decision Tree Decision Tree Surface"}
{"prompt": "It appears that you are using the Pygame library to create an animation of a decision tree classifier for the Iris dataset.\n\nHowever, your code is not clear or well-structured. It's also quite long and seems to be missing some crucial details, such as how to actually animate the decision tree surface, highlight the maximum information gain point, and draw the basic tree.\n\nHere are a few suggestions to improve your code:\n\n1.  **Use a more structured approach**: Consider breaking down your code into smaller functions or classes that each handle a specific part of the animation.\n2.  **Follow the Pygame documentation**: Make sure you're using all the necessary functions and variables from the Pygame library, and follow their usage guidelines.\n\nHere's an example of how you could refactor your code to make it more organized and easier to understand:\n\n```python\nimport pygame\n\nclass DecisionTreeSurface(pygame.sprite.Sprite):\n    def __init__(self, classifier, data, axes_group):\n        super().__init__()\n        self.classifier = classifier\n        self.data = data\n        self.axes_group = axes_group\n\n    def draw(self, surface):\n        # Draw the decision tree surface here\n        pass\n\nclass DecisionTreeClassifier:\n    def __init__(self, data):\n        self.tree_ = None  # Initialize an empty tree here\n\n    def make_tree(self):\n        # Make the tree here using your classifier and data\n        pass\n\n    def draw_graph(self, graph):\n        # Draw the decision tree graph here\n        pass\n\nclass IrisDatasetPlot:\n    def __init__(self):\n        self.data = None  # Initialize an empty dataset here\n        self.axes_group = None  # Initialize an empty axes group here\n\n    def all_group(self):\n        # Return a sprite that represents all of your sprites in one group\n        return pygame.sprite.Group()\n\nclass Animation:\n    def __init__(self, classifier, data, iris_dataset_plot):\n        self.classifier = classifier\n        self.data = data\n        self.iris_dataset_plot = iris_dataset_plot\n\n    def make_split_animation(self):\n        # Create the split animation here using your decision tree classifier and dataset\n        pass\n\n    def construct(self):\n        # Construct the animation by calling all the functions above\n        self.make_split_animation()\n```\n\nThis is just one possible way to refactor your code. You may need to modify it further based on your specific requirements.\n\nAs for the actual logic of creating the decision tree surface and drawing the basic tree, that will depend on how you've implemented your classifier in Python (which isn't shown here). But here's a simple example using scikit-learn library:\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier as DT\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# Load the iris dataset\niris = load_iris()\ndata = iris.data[:, :2]  # We only take the first two features.\ntarget = iris.target\ntree_clf = DT()\n\n# Fit the tree classifier to the data\ntree_clf.fit(data, target)\n\n# Create a decision tree surface and draw it\nfig = plt.figure(figsize=(8,6))\nplt.scatter(data[:,0], data[:,1])\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1])\n\n# Draw the leaf node of the tree (this is the point where we want to highlight)\nx_leaf = []\ny_leaf = []\nfor idx in tree_clf.tree_.feature:\n    if idx == -1:\n        x_leaf.append(data[tree_clf.tree_.threshold[idx], 0])\n        y_leaf.append(data[tree_clf.tree_.threshold[idx], 1])\n\nplt.scatter(x_leaf, y_leaf, color='yellow')\n\n# Draw the line that represents the boundary between the classes\nfor i in range(len(tree_clf.tree_)):\n    threshold = tree_clf.tree_.feature[i]\n    if tree_clf.tree_.feature[i] == -2:\n        continue\n\n    left_idx = np.where(data[:,0] < threshold)[0][0]\n    right_idx = np.where(data[:,0] >= threshold)[0][-1]\n\n    plt.plot([data[left_idx,0], data[right_idx,0]], [data[left_idx,1], data[right_idx,1]], color='black')\n\n# Show the plot\nplt.show()\n```\n\nThis is a very basic example. But it should give you an idea of how to draw a decision tree surface and highlight its leaf node using matplotlib.", "response": "from sklearn import datasets\nfrom decision_tree_surface import *\nfrom manim import *\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import entropy\nimport math\nfrom PIL import Image\n\niris = datasets.load_iris()\nfont = \"Source Han Sans\"\nfont_scale = 0.75\n\nimages = [\n    Image.open(\"iris_dataset/SetosaFlower.jpeg\"),\n    Image.open(\"iris_dataset/VeriscolorFlower.jpeg\"),\n    Image.open(\"iris_dataset/VirginicaFlower.jpeg\"),\n]\n\n\ndef entropy(class_labels, base=2):\n    # compute the class counts\n    unique, counts = np.unique(class_labels, return_counts=True)\n    dictionary = dict(zip(unique, counts))\n    total = 0.0\n    num_samples = len(class_labels)\n    for class_index in range(0, 3):\n        if not class_index in dictionary:\n            continue\n        prob = dictionary[class_index] / num_samples\n        total += prob * math.log(prob, base)\n    # higher set\n    return -total\n\n\ndef merge_overlapping_polygons(all_polygons, colors=[BLUE, GREEN, ORANGE]):\n    # get all polygons of each color\n    polygon_dict = {\n        str(BLUE).lower(): [],\n        str(GREEN).lower(): [],\n        str(ORANGE).lower(): [],\n    }\n    for polygon in all_polygons:\n        print(polygon_dict)\n        polygon_dict[str(polygon.color).lower()].append(polygon)\n\n    return_polygons = []\n    for color in colors:\n        color = str(color).lower()\n        polygons = polygon_dict[color]\n        points = set()\n        for polygon in polygons:\n            vertices = polygon.get_vertices().tolist()\n            vertices = [tuple(vert) for vert in vertices]\n            for pt in vertices:\n                if pt in points:  # Shared vertice, remove it.\n                    points.remove(pt)\n                else:\n                    points.add(pt)\n        points = list(points)\n        sort_x = sorted(points)\n        sort_y = sorted(points, key=lambda x: x[1])\n\n        edges_h = {}\n        edges_v = {}\n\n        i = 0\n        while i < len(points):\n            curr_y = sort_y[i][1]\n            while i < len(points) and sort_y[i][1] == curr_y:\n                edges_h[sort_y[i]] = sort_y[i + 1]\n                edges_h[sort_y[i + 1]] = sort_y[i]\n                i += 2\n        i = 0\n        while i < len(points):\n            curr_x = sort_x[i][0]\n            while i < len(points) and sort_x[i][0] == curr_x:\n                edges_v[sort_x[i]] = sort_x[i + 1]\n                edges_v[sort_x[i + 1]] = sort_x[i]\n                i += 2\n\n        # Get all the polygons.\n        while edges_h:\n            # We can start with any point.\n            polygon = [(edges_h.popitem()[0], 0)]\n            while True:\n                curr, e = polygon[-1]\n                if e == 0:\n                    next_vertex = edges_v.pop(curr)\n                    polygon.append((next_vertex, 1))\n                else:\n                    next_vertex = edges_h.pop(curr)\n                    polygon.append((next_vertex, 0))\n                if polygon[-1] == polygon[0]:\n                    # Closed polygon\n                    polygon.pop()\n                    break\n            # Remove implementation-markers from the polygon.\n            poly = [point for point, _ in polygon]\n            for vertex in poly:\n                if vertex in edges_h:\n                    edges_h.pop(vertex)\n                if vertex in edges_v:\n                    edges_v.pop(vertex)\n            polygon = Polygon(*poly, color=color, fill_opacity=0.3, stroke_opacity=1.0)\n            return_polygons.append(polygon)\n    return return_polygons\n\n\nclass IrisDatasetPlot(VGroup):\n    def __init__(self):\n        points = iris.data[:, 0:2]\n        labels = iris.feature_names\n        targets = iris.target\n        # Make points\n        self.point_group = self._make_point_group(points, targets)\n        # Make axes\n        self.axes_group = self._make_axes_group(points, labels)\n        # Make legend\n        self.legend_group = self._make_legend(\n            [BLUE, ORANGE, GREEN], iris.target_names, self.axes_group\n        )\n        # Make title\n        # title_text = \"Iris Dataset Plot\"\n        # self.title = Text(title_text).match_y(self.axes_group).shift([0.5, self.axes_group.height / 2 + 0.5, 0])\n        # Make all group\n        self.all_group = Group(self.point_group, self.axes_group, self.legend_group)\n        # scale the groups\n        self.point_group.scale(1.6)\n        self.point_group.match_x(self.axes_group)\n        self.point_group.match_y(self.axes_group)\n        self.point_group.shift([0.2, 0, 0])\n        self.axes_group.scale(0.7)\n        self.all_group.shift([0, 0.2, 0])\n\n    @override_animation(Create)\n    def create_animation(self):\n        animation_group = AnimationGroup(\n            # Perform the animations\n            Create(self.point_group, run_time=2),\n            Wait(0.5),\n            Create(self.axes_group, run_time=2),\n            # add title\n            # Create(self.title),\n            Create(self.legend_group),\n        )\n        return animation_group\n\n    def _make_point_group(self, points, targets, class_colors=[BLUE, ORANGE, GREEN]):\n        point_group = VGroup()\n        for point_index, point in enumerate(points):\n            # draw the dot\n            current_target = targets[point_index]\n            color = class_colors[current_target]\n            dot = Dot(point=np.array([point[0], point[1], 0])).set_color(color)\n            dot.scale(0.5)\n            point_group.add(dot)\n        return point_group\n\n    def _make_legend(self, class_colors, feature_labels, axes):\n        legend_group = VGroup()\n        # Make Text\n        setosa = Text(\"Setosa\", color=BLUE)\n        verisicolor = Text(\"Verisicolor\", color=ORANGE)\n        virginica = Text(\"Virginica\", color=GREEN)\n        labels = VGroup(setosa, verisicolor, virginica).arrange(\n            direction=RIGHT, aligned_edge=LEFT, buff=2.0\n        )\n        labels.scale(0.5)\n        legend_group.add(labels)\n        # surrounding rectangle\n        surrounding_rectangle = SurroundingRectangle(labels, color=WHITE)\n        surrounding_rectangle.move_to(labels)\n        legend_group.add(surrounding_rectangle)\n        # shift the legend group\n        legend_group.move_to(axes)\n        legend_group.shift([0, -3.0, 0])\n        legend_group.match_x(axes[0][0])\n\n        return legend_group\n\n    def _make_axes_group(self, points, labels):\n        axes_group = VGroup()\n        # make the axes\n        x_range = [\n            np.amin(points, axis=0)[0] - 0.2,\n            np.amax(points, axis=0)[0] - 0.2,\n            0.5,\n        ]\n        y_range = [np.amin(points, axis=0)[1] - 0.2, np.amax(points, axis=0)[1], 0.5]\n        axes = Axes(\n            x_range=x_range,\n            y_range=y_range,\n            x_length=9,\n            y_length=6.5,\n            # axis_config={\"number_scale_value\":0.75, \"include_numbers\":True},\n            tips=False,\n        ).shift([0.5, 0.25, 0])\n        axes_group.add(axes)\n        # make axis labels\n        # x_label\n        x_label = (\n            Text(labels[0], font=font)\n            .match_y(axes.get_axes()[0])\n            .shift([0.5, -0.75, 0])\n            .scale(font_scale)\n        )\n        axes_group.add(x_label)\n        # y_label\n        y_label = (\n            Text(labels[1], font=font)\n            .match_x(axes.get_axes()[1])\n            .shift([-0.75, 0, 0])\n            .rotate(np.pi / 2)\n            .scale(font_scale)\n        )\n        axes_group.add(y_label)\n\n        return axes_group\n\n\nclass DecisionTreeSurface(VGroup):\n    def __init__(self, tree_clf, data, axes, class_colors=[BLUE, ORANGE, GREEN]):\n        # take the tree and construct the surface from it\n        self.tree_clf = tree_clf\n        self.data = data\n        self.axes = axes\n        self.class_colors = class_colors\n        self.surface_rectangles = self.generate_surface_rectangles()\n\n    def generate_surface_rectangles(self):\n        # compute data bounds\n        left = np.amin(self.data[:, 0]) - 0.2\n        right = np.amax(self.data[:, 0]) - 0.2\n        top = np.amax(self.data[:, 1])\n        bottom = np.amin(self.data[:, 1]) - 0.2\n        maxrange = [left, right, bottom, top]\n        rectangles = decision_areas(self.tree_clf, maxrange, x=0, y=1, n_features=2)\n        # turn the rectangle objects into manim rectangles\n        def convert_rectangle_to_polygon(rect):\n            # get the points for the rectangle in the plot coordinate frame\n            bottom_left = [rect[0], rect[3]]\n            bottom_right = [rect[1], rect[3]]\n            top_right = [rect[1], rect[2]]\n            top_left = [rect[0], rect[2]]\n            # convert those points into the entire manim coordinates\n            bottom_left_coord = self.axes.coords_to_point(*bottom_left)\n            bottom_right_coord = self.axes.coords_to_point(*bottom_right)\n            top_right_coord = self.axes.coords_to_point(*top_right)\n            top_left_coord = self.axes.coords_to_point(*top_left)\n            points = [\n                bottom_left_coord,\n                bottom_right_coord,\n                top_right_coord,\n                top_left_coord,\n            ]\n            # construct a polygon object from those manim coordinates\n            rectangle = Polygon(\n                *points, color=color, fill_opacity=0.3, stroke_opacity=0.0\n            )\n            return rectangle\n\n        manim_rectangles = []\n        for rect in rectangles:\n            color = self.class_colors[int(rect[4])]\n            rectangle = convert_rectangle_to_polygon(rect)\n            manim_rectangles.append(rectangle)\n\n        manim_rectangles = merge_overlapping_polygons(\n            manim_rectangles, colors=[BLUE, GREEN, ORANGE]\n        )\n\n        return manim_rectangles\n\n    @override_animation(Create)\n    def create_override(self):\n        # play a reveal of all of the surface rectangles\n        animations = []\n        for rectangle in self.surface_rectangles:\n            animations.append(Create(rectangle))\n        animation_group = AnimationGroup(*animations)\n\n        return animation_group\n\n    @override_animation(Uncreate)\n    def uncreate_override(self):\n        # play a reveal of all of the surface rectangles\n        animations = []\n        for rectangle in self.surface_rectangles:\n            animations.append(Uncreate(rectangle))\n        animation_group = AnimationGroup(*animations)\n\n        return animation_group\n\n\nclass DecisionTree:\n    \"\"\"\n    Draw a single tree node\n    \"\"\"\n\n    def _make_node(\n        self,\n        feature,\n        threshold,\n        values,\n        is_leaf=False,\n        depth=0,\n        leaf_colors=[BLUE, ORANGE, GREEN],\n    ):\n        if not is_leaf:\n            node_text = f\"{feature}\\n    <= {threshold:.3f} cm\"\n            # draw decision text\n            decision_text = Text(node_text, color=WHITE)\n            # draw a box\n            bounding_box = SurroundingRectangle(decision_text, buff=0.3, color=WHITE)\n            node = VGroup()\n            node.add(bounding_box)\n            node.add(decision_text)\n            # return the node\n        else:\n            # plot the appropriate image\n            class_index = np.argmax(values)\n            # get image\n            pil_image = images[class_index]\n            leaf_group = Group()\n            node = ImageMobject(pil_image)\n            node.scale(1.5)\n            rectangle = Rectangle(\n                width=node.width + 0.05,\n                height=node.height + 0.05,\n                color=leaf_colors[class_index],\n                stroke_width=6,\n            )\n            rectangle.move_to(node.get_center())\n            rectangle.shift([-0.02, 0.02, 0])\n            leaf_group.add(rectangle)\n            leaf_group.add(node)\n            node = leaf_group\n\n        return node\n\n    def _make_connection(self, top, bottom, is_leaf=False):\n        top_node_bottom_location = top.get_center()\n        top_node_bottom_location[1] -= top.height / 2\n        bottom_node_top_location = bottom.get_center()\n        bottom_node_top_location[1] += bottom.height / 2\n        line = Line(top_node_bottom_location, bottom_node_top_location, color=WHITE)\n        return line\n\n    def _make_tree(self, tree, feature_names=[\"Sepal Length\", \"Sepal Width\"]):\n        tree_group = Group()\n        max_depth = tree.max_depth\n        # make the base node\n        feature_name = feature_names[tree.feature[0]]\n        threshold = tree.threshold[0]\n        values = tree.value[0]\n        nodes_map = {}\n        root_node = self._make_node(feature_name, threshold, values, depth=0)\n        nodes_map[0] = root_node\n        tree_group.add(root_node)\n        # save some information\n        node_height = root_node.height\n        node_width = root_node.width\n        scale_factor = 1.0\n        edge_map = {}\n        # tree height\n        tree_height = scale_factor * node_height * max_depth\n        tree_width = scale_factor * 2**max_depth * node_width\n        # traverse tree\n        def recurse(node, depth, direction, parent_object, parent_node):\n            # make sure it is a valid node\n            # make the node object\n            is_leaf = tree.children_left[node] == tree.children_right[node]\n            feature_name = feature_names[tree.feature[node]]\n            threshold = tree.threshold[node]\n            values = tree.value[node]\n            node_object = self._make_node(\n                feature_name, threshold, values, depth=depth, is_leaf=is_leaf\n            )\n            nodes_map[node] = node_object\n            node_height = node_object.height\n            # set the node position\n            direction_factor = -1 if direction == \"left\" else 1\n            shift_right_amount = (\n                0.8 * direction_factor * scale_factor * tree_width / (2**depth) / 2\n            )\n            if is_leaf:\n                shift_down_amount = -1.0 * scale_factor * node_height\n            else:\n                shift_down_amount = -1.8 * scale_factor * node_height\n            node_object.match_x(parent_object).match_y(parent_object).shift(\n                [shift_right_amount, shift_down_amount, 0]\n            )\n            tree_group.add(node_object)\n            # make a connection\n            connection = self._make_connection(\n                parent_object, node_object, is_leaf=is_leaf\n            )\n            edge_name = str(parent_node) + \",\" + str(node)\n            edge_map[edge_name] = connection\n            tree_group.add(connection)\n            # recurse\n            if not is_leaf:\n                recurse(tree.children_left[node], depth + 1, \"left\", node_object, node)\n                recurse(\n                    tree.children_right[node], depth + 1, \"right\", node_object, node\n                )\n\n        recurse(tree.children_left[0], 1, \"left\", root_node, 0)\n        recurse(tree.children_right[0], 1, \"right\", root_node, 0)\n\n        tree_group.scale(0.35)\n        return tree_group, nodes_map, edge_map\n\n    def color_example_path(\n        self, tree_group, nodes_map, tree, edge_map, example, color=YELLOW, thickness=2\n    ):\n        # get decision path\n        decision_path = tree.decision_path(example)[0]\n        path_indices = decision_path.indices\n        # highlight edges\n        for node_index in range(0, len(path_indices) - 1):\n            current_val = path_indices[node_index]\n            next_val = path_indices[node_index + 1]\n            edge_str = str(current_val) + \",\" + str(next_val)\n            edge = edge_map[edge_str]\n            animation_two = AnimationGroup(\n                nodes_map[current_val].animate.set_color(color)\n            )\n            self.play(animation_two, run_time=0.5)\n            animation_one = AnimationGroup(\n                edge.animate.set_color(color),\n                # edge.animate.set_stroke_width(4),\n            )\n            self.play(animation_one, run_time=0.5)\n        # surround the bottom image\n        last_path_index = path_indices[-1]\n        last_path_rectangle = nodes_map[last_path_index][0]\n        self.play(last_path_rectangle.animate.set_color(color))\n\n    def create_sklearn_tree(self, max_tree_depth=1):\n        # learn the decision tree\n        iris = load_iris()\n        tree = learn_iris_decision_tree(iris, max_depth=max_tree_depth)\n        feature_names = iris.feature_names[0:2]\n        return tree.tree_\n\n    def make_tree(self, max_tree_depth=2):\n        sklearn_tree = self.create_sklearn_tree()\n        # make the tree\n        tree_group, nodes_map, edge_map = self._make_tree(\n            sklearn_tree.tree_, feature_names\n        )\n        tree_group.shift([0, 5.5, 0])\n        return tree_group\n        # self.add(tree_group)\n        # self.play(SpinInFromNothing(tree_group), run_time=3)\n        # self.color_example_path(tree_group, nodes_map, tree, edge_map, iris.data[None, 0, 0:2])\n\n\nclass DecisionTreeSplitScene(Scene):\n    def make_decision_tree_classifier(self, max_depth=4):\n        decision_tree = DecisionTreeClassifier(\n            random_state=1, max_depth=max_depth, max_leaf_nodes=8\n        )\n        decision_tree = decision_tree.fit(iris.data[:, :2], iris.target)\n        # output the decisioin tree in some format\n        return decision_tree\n\n    def make_split_animation(self, data, classes, data_labels, main_axes):\n\n        \"\"\"\n        def make_entropy_animation_and_plot(dim=0, num_entropy_values=50):\n            # calculate the entropy values\n            axes_group = VGroup()\n            # make axes\n            range_vals = [np.amin(data, axis=0)[dim], np.amax(data, axis=0)[dim]]\n            axes = Axes(x_range=range_vals,\n                        y_range=[0, 1.0],\n                        x_length=9,\n                        y_length=4,\n            #            axis_config={\"number_scale_value\":0.75, \"include_numbers\":True},\n                        tips=False,\n                    )\n            axes_group.add(axes)\n            # make axis labels\n            # x_label\n            x_label = Text(data_labels[dim], font=font) \\\n                            .match_y(axes.get_axes()[0]) \\\n                            .shift([0.5, -0.75, 0]) \\\n                            .scale(font_scale*1.2)\n            axes_group.add(x_label)\n            # y_label\n            y_label = Text(\"Information Gain\", font=font) \\\n                .match_x(axes.get_axes()[1]) \\\n                .shift([-0.75, 0, 0]) \\\n                .rotate(np.pi / 2) \\\n                .scale(font_scale * 1.2)\n\n            axes_group.add(y_label)\n            # line animation\n            information_gains = []\n            def entropy_function(split_value):\n                # lower entropy\n                lower_set = np.nonzero(data[:, dim] <= split_value)[0]\n                lower_set = classes[lower_set]\n                lower_entropy = entropy(lower_set)\n                # higher entropy\n                higher_set = np.nonzero(data[:, dim] > split_value)[0]\n                higher_set = classes[higher_set]\n                higher_entropy = entropy(higher_set)\n                # calculate entropies\n                all_entropy = entropy(classes, base=2)\n                lower_entropy = entropy(lower_set, base=2)\n                higher_entropy = entropy(higher_set, base=2)\n                mean_entropy = (lower_entropy + higher_entropy) / 2\n                # calculate information gain\n                lower_prob = len(lower_set) / len(data[:, dim])\n                higher_prob = len(higher_set) / len(data[:, dim])\n                info_gain = all_entropy - (lower_prob * lower_entropy + higher_prob * higher_entropy)\n                information_gains.append((split_value, info_gain))\n                return info_gain\n\n            data_range = np.amin(data[:, dim]), np.amax(data[:, dim])\n\n            entropy_graph = axes.get_graph(\n                entropy_function, \n            #    color=RED, \n            #    x_range=data_range\n            )\n            axes_group.add(entropy_graph)\n            axes_group.shift([4.0, 2, 0])\n            axes_group.scale(0.5)\n            dot_animation = Dot(color=WHITE)\n            axes_group.add(dot_animation)\n            # make animations\n            animation_group = AnimationGroup(\n                Create(axes_group, run_time=2),\n                Wait(3),\n                MoveAlongPath(dot_animation, entropy_graph, run_time=20, rate_func=rate_functions.ease_in_out_quad),\n                Wait(2)\n            )\n\n            return axes_group, animation_group, information_gains\n        \"\"\"\n\n        def make_split_line_animation(dim=0):\n            # make a line along one of the dims and move it up and down\n            origin_coord = [\n                np.amin(data, axis=0)[0] - 0.2,\n                np.amin(data, axis=0)[1] - 0.2,\n            ]\n            origin_point = main_axes.coords_to_point(*origin_coord)\n            top_left_coord = [origin_coord[0], np.amax(data, axis=0)[1]]\n            bottom_right_coord = [np.amax(data, axis=0)[0] - 0.2, origin_coord[1]]\n            if dim == 0:\n                other_coord = top_left_coord\n                moving_line_coord = bottom_right_coord\n            else:\n                other_coord = bottom_right_coord\n                moving_line_coord = top_left_coord\n            other_point = main_axes.coords_to_point(*other_coord)\n            moving_line_point = main_axes.coords_to_point(*moving_line_coord)\n            moving_line = Line(origin_point, other_point, color=RED)\n            movement_line = Line(origin_point, moving_line_point)\n            if dim == 0:\n                movement_line.shift([0, moving_line.height / 2, 0])\n            else:\n                movement_line.shift([moving_line.width / 2, 0, 0])\n            # move the moving line along the movement line\n            animation = MoveAlongPath(\n                moving_line,\n                movement_line,\n                run_time=20,\n                rate_func=rate_functions.ease_in_out_quad,\n            )\n            return animation, moving_line\n\n        # plot the line in white then make it invisible\n        # make an animation along the line\n        # make a\n        # axes_one_group, top_animation_group, info_gains = make_entropy_animation_and_plot(dim=0)\n        line_movement, first_moving_line = make_split_line_animation(dim=0)\n        # axes_two_group, bottom_animation_group, _ = make_entropy_animation_and_plot(dim=1)\n        second_line_movement, second_moving_line = make_split_line_animation(dim=1)\n        # axes_two_group.shift([0, -3, 0])\n        animation_group_one = AnimationGroup(\n            #    top_animation_group,\n            line_movement,\n        )\n\n        animation_group_two = AnimationGroup(\n            #    bottom_animation_group,\n            second_line_movement,\n        )\n        \"\"\"\n        both_axes_group = VGroup(\n            axes_one_group,\n            axes_two_group\n        )\n        \"\"\"\n\n        return (\n            animation_group_one,\n            animation_group_two,\n            first_moving_line,\n            second_moving_line,\n            None,\n            None,\n        )\n        #    both_axes_group, \\\n        #    info_gains\n\n    def construct(self):\n        # make the points\n        iris_dataset_plot = IrisDatasetPlot()\n        iris_dataset_plot.all_group.scale(1.0)\n        iris_dataset_plot.all_group.shift([-3, 0.2, 0])\n        # make the entropy line graph\n        # entropy_line_graph = self.draw_entropy_line_graph()\n        # arrange the plots\n        # do animations\n        self.play(Create(iris_dataset_plot))\n        # make the decision tree classifier\n        decision_tree_classifier = self.make_decision_tree_classifier()\n        decision_tree_surface = DecisionTreeSurface(\n            decision_tree_classifier, iris.data, iris_dataset_plot.axes_group[0]\n        )\n        self.play(Create(decision_tree_surface))\n        self.wait(3)\n        self.play(Uncreate(decision_tree_surface))\n        main_axes = iris_dataset_plot.axes_group[0]\n        (\n            split_animation_one,\n            split_animation_two,\n            first_moving_line,\n            second_moving_line,\n            both_axes_group,\n            info_gains,\n        ) = self.make_split_animation(\n            iris.data[:, 0:2], iris.target, iris.feature_names, main_axes\n        )\n        self.play(split_animation_one)\n        self.wait(0.1)\n        self.play(Uncreate(first_moving_line))\n        self.wait(3)\n        self.play(split_animation_two)\n        self.wait(0.1)\n        self.play(Uncreate(second_moving_line))\n        self.wait(0.1)\n        # highlight the maximum on top\n        # sort by second key\n        \"\"\"\n        highest_info_gain = sorted(info_gains, key=lambda x: x[1])[-1]\n        highest_info_gain_point = both_axes_group[0][0].coords_to_point(*highest_info_gain)\n        highlighted_peak = Dot(highest_info_gain_point, color=YELLOW)\n        # get location of highest info gain point\n        highest_info_gain_point_in_iris_graph = iris_dataset_plot.axes_group[0].coords_to_point(*[highest_info_gain[0], 0])\n        first_moving_line.start[0] = highest_info_gain_point_in_iris_graph[0]\n        first_moving_line.end[0] = highest_info_gain_point_in_iris_graph[0]\n        self.play(Create(highlighted_peak))\n        self.play(Create(first_moving_line))\n        text = Text(\"Highest Information Gain\")\n        text.scale(0.4)\n        text.move_to(highlighted_peak)\n        text.shift([0, 0.5, 0])\n        self.play(Create(text))\n        \"\"\"\n        self.wait(1)\n        # draw the basic tree\n        decision_tree_classifier = self.make_decision_tree_classifier(max_depth=1)\n        decision_tree_surface = DecisionTreeSurface(\n            decision_tree_classifier, iris.data, iris_dataset_plot.axes_group[0]\n        )\n        decision_tree_graph, _, _ = DecisionTree()._make_tree(\n            decision_tree_classifier.tree_\n        )\n        decision_tree_graph.match_y(iris_dataset_plot.axes_group)\n        decision_tree_graph.shift([4, 0, 0])\n        self.play(Create(decision_tree_surface))\n        uncreate_animation = AnimationGroup(\n            #    Uncreate(both_axes_group),\n            #    Uncreate(highlighted_peak),\n            Uncreate(second_moving_line),\n            #    Unwrite(text)\n        )\n        self.play(uncreate_animation)\n        self.wait(0.5)\n        self.play(FadeIn(decision_tree_graph))\n        # self.play(FadeIn(highlighted_peak))\n        self.wait(5)\n", "original_prompt": "Decision Tree Split Scene"}
{"prompt": "This code appears to be an interactive Python script using the `manim` library for creating animations. Here's a breakdown of what it does:\n\n1. It first sets up some variables and imports necessary libraries.\n2. It creates a 2D plane (`axes`) where the distribution and chains will be plotted.\n3. It generates two random chains, one forward and one backward, using the `generate_forward_and_reverse_chains` function.\n4. It creates an image with the prompt \"An oil painting of a flaming dragon.\"\n5. It animates the following steps:\n\t* First, it does the forward noise process by adding noise to the image at each timestep. This is done using the `RandomWalk` animation group.\n\t* Second, it does the reverse noise process by removing noise from the image at each timestep. This is also done using the `RandomWalk` animation group.\n6. Finally, it adds some final touches, such as updating the title and fading out the random walk.\n\nHere's a refactored version of the code with some improvements:\n\n```python\nfrom manim import *\n\nclass DiffusionProcess(Scene):\n    def construct(self):\n        # Create the axes\n        axes, axes_group = make_2d_diffusion_space()\n\n        # Set up the prompt\n        paragraph_prompt = '\"An oil painting of\\n a flaming dragon.\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        self.add(text_prompt)\n\n        # Create the image with the prompt\n        stable_diffusion_images = [\n            ImageMobject(\"image1.png\"),\n            ImageMobject(\"image2.png\"),\n            ImageMobject(\"image3.png\"),  # Add more images as needed\n        ]\n        image = stable_diffusion_images[0]\n        self.play(FadeIn(image))\n\n        # Define the forward and backward chains\n        def generate_chain(start_point, target_point, num_steps):\n            forward_chain = []\n            for _ in range(num_steps):\n                forward_chain.append(random_point_in_ellipse(\n                    axes, start_point, target_point, 0.5\n                ))\n                start_point = target_point\n            return forward_chain\n\n        forward_chain = generate_chain([3.0, -3.0], [-7.0, -4.0], 10)\n\n        # Generate the reverse chain\n        def generate_reverse_chain(start_point, num_steps):\n            reverse_chain = []\n            for _ in range(num_steps):\n                reverse_chain.append(random_point_in_ellipse(\n                    axes, start_point, [3.0, -3.0], 0.5\n                ))\n                start_point = [-7.0, -4.0]\n            return reverse_chain\n\n        backward_chain = generate_reverse_chain([3.0, -3.0], 10)\n\n        # Create the forward and backward random walks\n        def create_random_walk(chain):\n            random_walk = RandomWalk(chain, axes=axes)\n            animation_group = []\n            for timestep in range(len(chain)):\n                image = ImageMobject(\"image\" + str(timestep) + \".png\")\n                image.set_height(image.get_height())\n                self.play(FadeOut(self.last_used_image), FadeIn(image))\n                new_image = image\n                if timestep < len(chain) - 1:\n                    replace_image_animation = AnimationGroup(\n                        FadeOut(new_image),\n                        FadeIn(image),\n                        lag_ratio=0.0\n                    )\n                animation_group.append(replace_image_animation)\n            return Succession(*animation_group)\n\n        forward_random_walk = create_random_walk(forward_chain)\n        backward_random_walk = create_random_walk(backward_chain)\n\n        # Play the animations\n        self.play(\n            forward_random_walk,\n            backward_random_walk\n        )\n\n# Helper function to create a 2D plane with axes\ndef make_2d_diffusion_space():\n    axes = NumberPlane(width=10, height=5)\n    return axes\n\nclass RandomWalk(Scene):\n    def construct(self, chain, axes=None):\n        self.chain = chain\n        if axes is None:\n            axes = axes\n        random_walk = []\n        for point in self.chain:\n            image = ImageMobject(\"image\" + str(len(random_walk)) + \".png\")\n            image.set_height(image.get_height())\n            self.play(FadeIn(image))\n            new_image = image\n            if len(self.chain) > 1:\n                replace_image_animation = AnimationGroup(\n                    FadeOut(new_image),\n                    FadeIn(image),\n                    lag_ratio=0.0\n                )\n            random_walk.append(replace_image_animation)\n        return Succession(*random_walk)\n\nclass Point(Scene):\n    def construct(self, point, axes=None):\n        self.point = point\n        if axes is None:\n            axes = axes\n        self.x = axes.get_x_axis()\n        self.y = axes.get_y_axis()\n\n    def draw_point(self, color):\n        line = Line(\n            self.point,\n            Point(self.point + self.x * 0.1),\n            stroke_width=5,\n            color=color,\n        )\n        point = self.point\n        return line\n\n# Helper function to generate a random point in an ellipse\ndef random_point_in_ellipse(axes, center, target_center, radius):\n    x = axes.get_x_axis()\n    y = axes.get_y_axis()\n    theta = uniform_random_in_range(-pi, pi)\n    r = sqrt(radius**2 - (x - center[0])**2)\n    x_coord = radius * cos(theta) + center[0]\n    y_coord = radius * sin(theta) + center[1]\n    return Point(x_coord, y_coord)\n\nclass Scene(Scene):\n    def construct(self):\n        # ... (rest of the code remains the same)\n```\n\nThis refactored version includes:\n\n*   Improved organization and naming conventions\n*   Removed unnecessary comments\n*   Replaced `random_point_in_ellipse` with a more concise version using trigonometry\n*   Removed unused variables and functions\n*   Simplified the creation of random walks and images\n*   Improved error handling by making some parameters optional\n\nNote that this is just one possible way to refactor the code, and there may be other approaches depending on your specific use case.", "response": "\"\"\"\n    Shows video of diffusion process. \n\"\"\"\nimport manim_ml\nfrom manim import *\nfrom PIL import Image\nimport os\nfrom diffusers import StableDiffusionPipeline\nimport numpy as np\nimport scipy\n\nfrom manim_ml.diffusion.mcmc import metropolis_hastings_sampler, gaussian_proposal\nfrom manim_ml.diffusion.random_walk import RandomWalk\nfrom manim_ml.utils.mobjects.probability import make_dist_image_mobject_from_samples\n\ndef generate_stable_diffusion_images(prompt, num_inference_steps=30):\n    \"\"\"Generates a list of progressively denoised images using Stable Diffusion\n\n    Parameters\n    ----------\n    num_inference_steps : int, optional\n        _description_, by default 30\n    \"\"\"\n    pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n    image_list = [] \n    # Save image callback\n    def save_image_callback(step, timestep, latents):\n        print(\"Saving image callback\")\n        # Decode the latents\n        image = pipeline.vae.decode(latents)\n        image_list.append(image)\n\n    # Generate the image\n    pipeline(prompt, num_inference_steps=num_inference_steps, callback=save_image_callback)\n\n    return image_list\n\ndef make_time_schedule_bar(num_time_steps=30):\n    \"\"\"Makes a bar that gets moved back and forth according to the diffusion model time\"\"\"\n    # Draw time bar with initial value\n    time_bar = NumberLine(\n        [0, num_time_steps], length=25, stroke_width=10, include_ticks=False, include_numbers=False,\n        color=manim_ml.config.color_scheme.secondary_color\n\n    )\n    time_bar.shift(4.5 * DOWN)\n    current_time = ValueTracker(0.3)\n    time_point = time_bar.number_to_point(current_time.get_value())\n    time_dot = Dot(time_point, color=manim_ml.config.color_scheme.secondary_color, radius=0.2)\n    label_location = time_bar.number_to_point(1.0)\n    label_location -= DOWN * 0.1\n    label_text = MathTex(\"t\", color=manim_ml.config.color_scheme.text_color).scale(1.5)\n    # label_text = Text(\"time\")\n    label_text.move_to(time_bar.get_center())\n    label_text.shift(DOWN * 0.5)\n    # Make an updater for the dot\n    def dot_updater(time_dot):\n        # Get location on time_bar\n        point_loc = time_bar.number_to_point(current_time.get_value())\n        time_dot.move_to(point_loc)\n\n    time_dot.add_updater(dot_updater)\n\n    return time_bar, time_dot, label_text, current_time\n\ndef make_2d_diffusion_space():\n    \"\"\"Makes a 2D axis where the diffusion random walk happens in. \n    There is also a distribution of points representing the true distribution\n    of images. \n    \"\"\"\n    axes_group = Group()\n    x_range = [-8, 8]\n    y_range = [-8, 8]\n    y_length = 13\n    x_length = 13\n    # Make an axis\n    axes = Axes(\n        x_range=x_range,\n        y_range=y_range,\n        x_length=x_length,\n        y_length=y_length,\n        # x_axis_config={\"stroke_opacity\": 1.0, \"stroke_color\": WHITE},\n        # y_axis_config={\"stroke_opacity\": 1.0, \"stroke_color\": WHITE},\n        # tips=True,\n    )\n    # Make the distribution for the true images as some gaussian mixture in \n    # the bottom right\n    gaussian_a = np.random.multivariate_normal(\n        mean=[3.0, -3.2],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(100)\n    )\n    gaussian_b = np.random.multivariate_normal(\n        mean=[3.0, 3.0],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(100)\n    )\n    gaussian_c = np.random.multivariate_normal(\n        mean=[0.0, -1.6],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(200)\n    )\n    all_gaussian_samples = np.concatenate([gaussian_a, gaussian_b, gaussian_c], axis=0)\n    print(f\"Shape of all gaussian samples: {all_gaussian_samples.shape}\")\n\n    image_mobject = make_dist_image_mobject_from_samples(\n        all_gaussian_samples,\n        xlim=(x_range[0], x_range[1]),\n        ylim=(y_range[0], y_range[1])\n    )\n    image_mobject.scale_to_fit_height(\n        axes.get_height()     \n    )\n    image_mobject.move_to(axes)\n\n    axes_group.add(axes)\n    axes_group.add(image_mobject)\n\n    return axes_group\n\ndef generate_forward_and_reverse_chains(start_point, target_point, num_inference_steps=30):\n    \"\"\"Here basically we want to generate a forward and reverse chain\n    for the diffusion model where the reverse chain starts at the end of the forward\n    chain, and the end of the reverse chain ends somewhere within a certain radius of the \n    reverse chain.\n\n    This can be done in a sortof hacky way by doing metropolis hastings from a start point\n    to a distribution centered about the start point and vica versa.\n\n    Parameters\n    ----------\n    start_point : _type_\n        _description_\n    \"\"\"\n    def start_dist_log_prob_fn(x):\n        \"\"\"Log probability of the start distribution\"\"\"\n        # Make it a gaussian in top left of the 2D space\n        gaussian_pdf = scipy.stats.multivariate_normal(\n            mean=target_point,\n            cov=[1.0, 1.0]\n        ).pdf(x)\n\n        return np.log(gaussian_pdf)\n    \n    def end_dist_log_prob_fn(x):\n        gaussian_pdf = scipy.stats.multivariate_normal(\n            mean=start_point,\n            cov=[1.0, 1.0]\n        ).pdf(x)\n\n        return np.log(gaussian_pdf)\n\n    forward_chain, _, _ = metropolis_hastings_sampler(\n        log_prob_fn=start_dist_log_prob_fn, \n        prop_fn=gaussian_proposal,\n        iterations=num_inference_steps,\n        initial_location=start_point\n    )\n\n    end_point = forward_chain[-1]\n\n    reverse_chain, _, _ = metropolis_hastings_sampler(\n        log_prob_fn=end_dist_log_prob_fn,\n        prop_fn=gaussian_proposal,\n        iterations=num_inference_steps,\n        initial_location=end_point\n    )\n\n    return forward_chain, reverse_chain\n\n# Make the scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 30.0\nconfig.frame_width = 30.0\n\nclass DiffusionProcess(Scene):\n\n    def construct(self):\n        # Parameters\n        num_inference_steps = 50\n        image_save_dir = \"diffusion_images\"\n        prompt = \"An oil painting of a dragon.\"\n        start_time = 0\n        # Compute the stable diffusion images\n        if len(os.listdir(image_save_dir)) < num_inference_steps:\n            stable_diffusion_images = generate_stable_diffusion_images(\n                prompt,\n                num_inference_steps=num_inference_steps\n            )\n            for index, image in enumerate(stable_diffusion_images):\n                image.save(f\"{image_save_dir}/{index}.png\")\n        else:\n            stable_diffusion_images = [Image.open(f\"{image_save_dir}/{i}.png\") for i in range(num_inference_steps)]\n        # Reverse order of list to be in line with theory timesteps\n        stable_diffusion_images = stable_diffusion_images[::-1]\n        # Add the initial location of the first image\n        start_image = stable_diffusion_images[start_time]\n        image_mobject = ImageMobject(start_image)\n        image_mobject.scale(0.55)\n        image_mobject.shift(LEFT * 7.5)\n        self.add(image_mobject)\n        # Make the time schedule bar\n        time_bar, time_dot, time_label, current_time = make_time_schedule_bar(num_time_steps=num_inference_steps)\n        # Place the bar at the bottom of the screen\n        time_bar.move_to(image_mobject.get_bottom() + DOWN * 2)\n        time_bar.set_x(0)\n        self.add(time_bar)\n        # Place the time label below the time bar\n        self.add(time_label)\n        time_label.next_to(time_bar, DOWN, buff=0.5)\n        # Add 0 and T labels above the bar\n        zero_label = Text(\"0\")\n        zero_label.next_to(time_bar.get_left(), UP, buff=0.5)\n        self.add(zero_label)\n        t_label = Text(\"T\")\n        t_label.next_to(time_bar.get_right(), UP, buff=0.5)\n        self.add(t_label)\n        # Move the time dot to zero\n        time_dot.set_value(0)\n        time_dot.move_to(time_bar.number_to_point(0))\n        self.add(time_dot)\n        # Add the prompt above the image\n        paragraph_prompt = '\"An oil painting of\\n a flaming dragon.\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        text_prompt.next_to(image_mobject, UP, buff=0.6)\n        self.add(text_prompt)\n        # Generate the chain data\n        forward_chain, reverse_chain = generate_forward_and_reverse_chains(\n            start_point=[3.0, -3.0],\n            target_point=[-7.0, -4.0],\n            num_inference_steps=num_inference_steps\n        )\n        # Make the axes that the distribution and chains go on\n        axes, axes_group = make_2d_diffusion_space()\n        # axes_group.match_height(image_mobject)\n        axes_group.shift(RIGHT * 7.5)\n        axes.shift(RIGHT * 7.5)\n        self.add(axes_group)\n        # Add title below distribution\n        title = Text(\"Distribution of Real Images\", font_size=48, color=RED)\n        title.move_to(axes_group)\n        title.shift(DOWN * 6)\n        line = Line(\n            title.get_top(),\n            axes_group.get_bottom() - 4 * DOWN,\n        )\n        self.add(title)\n        self.add(line)\n        # Add a title above the plot\n        title = Text(\"Reverse Diffusion Process\", font_size=64)\n        title.move_to(axes)\n        title.match_y(text_prompt)\n        self.add(title)\n        # First do the forward noise process of adding noise to an image\n        forward_random_walk = RandomWalk(forward_chain, axes=axes)\n        # Sync up forward random walk\n        synced_animations = []\n        forward_random_walk_animation = forward_random_walk.animate_random_walk()\n        print(len(forward_random_walk_animation.animations))\n        for timestep, transition_animation in enumerate(forward_random_walk_animation.animations):\n            # Make an animation moving the time bar\n            # time_bar_animation = current_time.animate.set_value(timestep)\n            time_bar_animation = current_time.animate.set_value(timestep)\n            # Make an animation replacing the current image with the timestep image\n            new_image = ImageMobject(stable_diffusion_images[timestep])\n            new_image = new_image.set_height(image_mobject.get_height())\n            new_image.move_to(image_mobject)\n            image_mobject = new_image\n            replace_image_animation = AnimationGroup(\n                FadeOut(image_mobject),\n                FadeIn(new_image),\n                lag_ratio=1.0\n            )\n            # Sync them together\n            # synced_animations.append(\n            self.play(\n                Succession(\n                    transition_animation, \n                    time_bar_animation,\n                    replace_image_animation,\n                    lag_ratio=0.0\n                )\n            )\n\n        # Fade out the random walk\n        self.play(forward_random_walk.fade_out_random_walk())\n        # self.play(\n        # Succession(\n        # *synced_animations,\n        # )\n        # )\n        new_title = Text(\"Forward Diffusion Process\", font_size=64)\n        new_title.move_to(title)\n        self.play(ReplacementTransform(title, new_title))\n        # Second do the reverse noise process of removing noise from the image\n        backward_random_walk = RandomWalk(reverse_chain, axes=axes)\n        # Sync up forward random walk\n        synced_animations = []\n        backward_random_walk_animation = backward_random_walk.animate_random_walk()\n        for timestep, transition_animation in enumerate(backward_random_walk_animation.animations):\n            timestep = num_inference_steps - timestep - 1\n            if timestep == num_inference_steps - 1:\n                continue\n            # Make an animation moving the time bar\n            # time_bar_animation = time_dot.animate.set_value(timestep)\n            time_bar_animation = current_time.animate.set_value(timestep)\n            # Make an animation replacing the current image with the timestep image\n            new_image = ImageMobject(stable_diffusion_images[timestep])\n            new_image = new_image.set_height(image_mobject.get_height())\n            new_image.move_to(image_mobject)\n            image_mobject = new_image\n            replace_image_animation = AnimationGroup(\n                FadeOut(image_mobject),\n                FadeIn(new_image),\n                lag_ratio=1.0\n            )\n            # Sync them together\n            # synced_animations.append(\n            self.play(\n                Succession(\n                    transition_animation, \n                    time_bar_animation,\n                    replace_image_animation,\n                    lag_ratio=0.0\n                )\n            )\n        # self.play(\n        # Succession(\n        # *synced_animations,\n        # )\n        # )\n", "original_prompt": "Diffusion Process Diffusion Process"}
{"prompt": "Show a 3D Gaussian-like embedding with points moving from the center to the edges, accompanied by the creation of a VAE decoder neural network structure.", "response": "\"\"\"This module is dedicated to visualizing VAE disentanglement\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\n\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nimport pickle\n\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef construct_image_mobject(input_image, height=2.3):\n    \"\"\"Constructs an ImageMobject from a numpy grayscale image\"\"\"\n    # Convert image to rgb\n    if len(input_image.shape) == 2:\n        input_image = np.repeat(input_image, 3, axis=0)\n        input_image = np.rollaxis(input_image, 0, start=3)\n    #  Make the ImageMobject\n    image_mobject = ImageMobject(input_image, image_mode=\"RGB\")\n    image_mobject.set_resampling_algorithm(RESAMPLING_ALGORITHMS[\"nearest\"])\n    image_mobject.height = height\n\n    return image_mobject\n\n\nclass DisentanglementVisualization(VGroup):\n    def __init__(\n        self,\n        model_path=ROOT_DIR\n        / \"examples/variational_autoencoder/autoencoder_models/saved_models/model_dim2.pth\",\n        image_height=0.35,\n    ):\n        self.model_path = model_path\n        self.image_height = image_height\n        # Load disentanglement image objects\n        with open(\n            ROOT_DIR\n            / \"examples/variational_autoencoder/autoencoder_models/disentanglement.pkl\",\n            \"rb\",\n        ) as f:\n            self.image_handler = pickle.load(f)\n\n    def make_disentanglement_generation_animation(self):\n        animation_list = []\n        for image_index, image in enumerate(self.image_handler[\"images\"]):\n            image_mobject = construct_image_mobject(image, height=self.image_height)\n            r, c = self.image_handler[\"bin_indices\"][image_index]\n            # Move the image to the correct location\n            r_offset = -1.2\n            c_offset = 0.25\n            image_location = [\n                c_offset + c * self.image_height,\n                r_offset + r * self.image_height,\n                0,\n            ]\n            image_mobject.move_to(image_location)\n            animation_list.append(FadeIn(image_mobject))\n\n        generation_animation = AnimationGroup(*animation_list[::-1], lag_ratio=1.0)\n        return generation_animation\n\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 5.0\nconfig.frame_width = 5.0\n\n\nclass DisentanglementScene(Scene):\n    \"\"\"Disentanglement Scene Object\"\"\"\n\n    def _construct_embedding(self, point_color=BLUE, dot_radius=0.05):\n        \"\"\"Makes a Gaussian-like embedding\"\"\"\n        embedding = VGroup()\n        # Sample points from a Gaussian\n        num_points = 200\n        standard_deviation = [0.6, 0.8]\n        mean = [0, 0]\n        points = np.random.normal(mean, standard_deviation, size=(num_points, 2))\n        # Make an axes\n        embedding.axes = Axes(\n            x_range=[-3, 3],\n            y_range=[-3, 3],\n            x_length=2.2,\n            y_length=2.2,\n            tips=False,\n        )\n        # Add each point to the axes\n        self.point_dots = VGroup()\n        for point in points:\n            point_location = embedding.axes.coords_to_point(*point)\n            dot = Dot(point_location, color=point_color, radius=dot_radius / 2)\n            self.point_dots.add(dot)\n\n        embedding.add(self.point_dots)\n        return embedding\n\n    def construct(self):\n        # Make the VAE decoder\n        vae_decoder = NeuralNetwork(\n            [\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n            ],\n            layer_spacing=0.55,\n        )\n\n        vae_decoder.shift([-0.55, 0, 0])\n        self.play(Create(vae_decoder), run_time=1)\n        # Make the embedding\n        embedding = self._construct_embedding()\n        embedding.scale(0.9)\n        embedding.move_to(vae_decoder.get_left())\n        embedding.shift([-0.85, 0, 0])\n        self.play(Create(embedding))\n        # Make disentanglment visualization\n        disentanglement = DisentanglementVisualization()\n        disentanglement_animation = (\n            disentanglement.make_disentanglement_generation_animation()\n        )\n        self.play(disentanglement_animation, run_time=3)\n        self.play(Wait(2))\n", "original_prompt": "Disentanglement Disentanglement"}
{"prompt": "Show an animation demonstrating how to create a 2D graph of epsilon nearest neighbors using Manim, with dynamic balls representing data points and lines connecting them based on distance thresholds.", "response": "\"\"\"\n    Example where I draw an epsilon nearest neighbor graph animation\n\"\"\"\nfrom cProfile import label\nfrom manim import *\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import SpectralClustering\nimport numpy as np\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1200\nconfig.frame_height = 12.0\nconfig.frame_width = 12.0\n\n\ndef make_moon_points(num_samples=100, noise=0.1, random_seed=1):\n    \"\"\"Make two half moon point shapes\"\"\"\n    # Make sure the points are normalized\n    X, y = make_moons(n_samples=num_samples, noise=noise, random_state=random_seed)\n    X -= np.mean(X, axis=0)\n    X /= np.std(X, axis=0)\n    X[:, 1] += 0.3\n    # X[:, 0] /= 2 # squeeze width\n\n    return X\n\n\ndef make_epsilon_balls(epsilon_value, points, axes, ball_color=RED, opacity=0.0):\n    \"\"\"Draws epsilon balls\"\"\"\n    balls = []\n    for point in points:\n        ball = Circle(epsilon_value, color=ball_color, fill_opacity=opacity)\n        global_location = axes.coords_to_point(*point)\n        ball.move_to(global_location)\n        balls.append(ball)\n\n    return VGroup(*balls)\n\n\ndef make_epsilon_graph(epsilon_value, dots, points, edge_color=ORANGE):\n    \"\"\"Makes an epsilon nearest neighbor graph for the given dots\"\"\"\n    # First compute the adjacency matrix from the epsilon value and the points\n    num_dots = len(dots)\n    adjacency_matrix = np.zeros((num_dots, num_dots))\n    # Note: just doing lower triangular matrix\n    for i in range(num_dots):\n        for j in range(i):\n            dist = np.linalg.norm(dots[i].get_center() - dots[j].get_center())\n            is_connected = 1 if dist < epsilon_value else 0\n            adjacency_matrix[i, j] = is_connected\n    # Draw a graph based on the adjacency matrix\n    edges = []\n    for i in range(num_dots):\n        for j in range(i):\n            is_connected = adjacency_matrix[i, j]\n            if is_connected:\n                # Draw a connection between the corresponding dots\n                dot_a = dots[i]\n                dot_b = dots[j]\n                edge = Line(\n                    dot_a.get_center(),\n                    dot_b.get_center(),\n                    color=edge_color,\n                    stroke_width=3,\n                )\n                edges.append(edge)\n\n    return VGroup(*edges), adjacency_matrix\n\n\ndef perform_spectral_clustering(adjacency_matrix):\n    \"\"\"Performs spectral clustering given adjacency matrix\"\"\"\n    clustering = SpectralClustering(\n        n_clusters=2, affinity=\"precomputed\", random_state=0\n    ).fit(adjacency_matrix)\n    labels = clustering.labels_\n\n    return labels\n\n\ndef make_color_change_animation(labels, dots, colors=[ORANGE, GREEN]):\n    \"\"\"Makes a color change animation\"\"\"\n    anims = []\n\n    for index in range(len(labels)):\n        color = colors[labels[index]]\n        dot = dots[index]\n        anims.append(dot.animate.set_color(color))\n\n    return AnimationGroup(*anims, lag_ratio=0.0)\n\n\nclass EpsilonNearestNeighborScene(Scene):\n    def construct(\n        self,\n        num_points=200,\n        dot_radius=0.1,\n        dot_color=BLUE,\n        ball_color=WHITE,\n        noise=0.1,\n        ball_opacity=0.0,\n        random_seed=2,\n    ):\n        # Make moon shape points\n        # Note: dot is the drawing object and point is the math concept\n        moon_points = make_moon_points(\n            num_samples=num_points, noise=noise, random_seed=random_seed\n        )\n        # Make an axes\n        axes = Axes(\n            x_range=[-6, 6, 1],\n            y_range=[-6, 6, 1],\n            x_length=12,\n            y_length=12,\n            tips=False,\n            axis_config={\"stroke_color\": \"#000000\"},\n        )\n        axes.scale(2.2)\n        self.add(axes)\n        # Draw points\n        dots = []\n        for point in moon_points:\n            axes_location = axes.coords_to_point(*point)\n            dot = Dot(axes_location, color=dot_color, radius=dot_radius, z_index=1)\n            dots.append(dot)\n\n        dots = VGroup(*dots)\n        self.play(Create(dots))\n        # Draw epsilon bar with initial value\n        epsilon_bar = NumberLine(\n            [0, 2], length=8, stroke_width=2, include_ticks=False, include_numbers=False\n        )\n        epsilon_bar.shift(4.5 * DOWN)\n        self.play(Create(epsilon_bar))\n        current_epsilon = ValueTracker(0.3)\n        epsilon_point = epsilon_bar.number_to_point(current_epsilon.get_value())\n        epsilon_dot = Dot(epsilon_point)\n        self.add(epsilon_dot)\n        label_location = epsilon_bar.number_to_point(1.0)\n        label_location -= DOWN * 0.1\n        label_text = MathTex(\"\\epsilon\").scale(1.5)\n        # label_text = Text(\"Epsilon\")\n        label_text.move_to(epsilon_bar.get_center())\n        label_text.shift(DOWN * 0.5)\n        self.add(label_text)\n        # Make an updater for the dot\n        def dot_updater(epsilon_dot):\n            # Get location on epsilon_bar\n            point_loc = epsilon_bar.number_to_point(current_epsilon.get_value())\n            epsilon_dot.move_to(point_loc)\n\n        epsilon_dot.add_updater(dot_updater)\n        # Make the epsilon balls\n        epsilon_balls = make_epsilon_balls(\n            current_epsilon.get_value(),\n            moon_points,\n            axes,\n            ball_color=ball_color,\n            opacity=ball_opacity,\n        )\n        # Set up updater for radius of balls\n        def epsilon_balls_updater(epsilon_balls):\n            for ball in epsilon_balls:\n                ball.set_width(current_epsilon.get_value())\n\n        # Turn epsilon up and down\n        epsilon_balls.add_updater(epsilon_balls_updater)\n        # Fade in the initial balls\n        self.play(FadeIn(epsilon_balls), lag_ratio=0.0)\n        # Iterate through different values of epsilon\n        for value in [1.5, 0.5, 0.9]:\n            self.play(current_epsilon.animate.set_value(value), run_time=2.5)\n        # Perform clustering\n        epsilon_value = 0.9\n        # Show connecting graph\n        epsilon_graph, adjacency_matrix = make_epsilon_graph(\n            current_epsilon.get_value(), dots, moon_points, edge_color=WHITE\n        )\n        self.play(FadeOut(epsilon_balls))\n        self.play(FadeIn(epsilon_graph))\n        # Fade out balls\n        self.play(Wait(1.5))\n        # Perform clustering\n        labels = perform_spectral_clustering(adjacency_matrix)\n        # Change the colors of the dots\n        color_change_animation = make_color_change_animation(labels, dots)\n        self.play(color_change_animation)\n        # Fade out graph edges\n        self.play(FadeOut(epsilon_graph))\n        self.play(Wait(5.0))\n", "original_prompt": "Epsilon Nn Graph Epsilon Nn Graph"}
{"prompt": "Show a Generative Adversarial Network (GAN) animation, highlighting layers and displaying real and fake images with probabilities.", "response": "import random\nfrom pathlib import Path\n\nfrom PIL import Image\nfrom manim import *\nfrom manim_ml.neural_network.layers.embedding import EmbeddingLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.vector import VectorLayer\n\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\nconfig.pixel_height = 1080\nconfig.pixel_width = 1080\nconfig.frame_height = 8.3\nconfig.frame_width = 8.3\n\n\nclass GAN(Mobject):\n    \"\"\"Generative Adversarial Network\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.make_entities()\n        self.place_entities()\n        self.titles = self.make_titles()\n\n    def make_entities(self, image_height=1.2):\n        \"\"\"Makes all of the network entities\"\"\"\n        # Make the fake image layer\n        default_image = Image.open(ROOT_DIR / \"assets/gan/fake_image.png\")\n        numpy_image = np.asarray(default_image)\n        self.fake_image_layer = ImageLayer(\n            numpy_image, height=image_height, show_image_on_create=False\n        )\n        # Make the Generator Network\n        self.generator = NeuralNetwork(\n            [\n                EmbeddingLayer(covariance=np.array([[3.0, 0], [0, 3.0]])).scale(1.3),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                self.fake_image_layer,\n            ],\n            layer_spacing=0.1,\n        )\n\n        self.add(self.generator)\n        # Make the Discriminator\n        self.discriminator = NeuralNetwork(\n            [\n                FeedForwardLayer(5),\n                FeedForwardLayer(1),\n                VectorLayer(1, value_func=lambda: random.uniform(0, 1)),\n            ],\n            layer_spacing=0.1,\n        )\n        self.add(self.discriminator)\n        # Make Ground Truth Dataset\n        default_image = Image.open(ROOT_DIR / \"assets/gan/real_image.jpg\")\n        numpy_image = np.asarray(default_image)\n        self.ground_truth_layer = ImageLayer(numpy_image, height=image_height)\n        self.add(self.ground_truth_layer)\n\n        self.scale(1)\n\n    def place_entities(self):\n        \"\"\"Positions entities in correct places\"\"\"\n        # Place relative to generator\n        # Place the ground_truth image layer\n        self.ground_truth_layer.next_to(self.fake_image_layer, DOWN, 0.8)\n        # Group the images\n        image_group = Group(self.ground_truth_layer, self.fake_image_layer)\n        # Move the discriminator to the right of thee generator\n        self.discriminator.next_to(self.generator, RIGHT, 0.2)\n        self.discriminator.match_y(image_group)\n        # Move the discriminator to the height of the center of the image_group\n        # self.discriminator.match_y(image_group)\n        # self.ground_truth_layer.next_to(self.fake_image_layer, DOWN, 0.5)\n\n    def make_titles(self):\n        \"\"\"Makes titles for the different entities\"\"\"\n        titles = VGroup()\n\n        self.ground_truth_layer_title = Text(\"Real Image\").scale(0.3)\n        self.ground_truth_layer_title.next_to(self.ground_truth_layer, UP, 0.1)\n        self.add(self.ground_truth_layer_title)\n        titles.add(self.ground_truth_layer_title)\n        self.fake_image_layer_title = Text(\"Fake Image\").scale(0.3)\n        self.fake_image_layer_title.next_to(self.fake_image_layer, UP, 0.1)\n        self.add(self.fake_image_layer_title)\n        titles.add(self.fake_image_layer_title)\n        # Overhead title\n        overhead_title = Text(\"Generative Adversarial Network\").scale(0.75)\n        overhead_title.shift(np.array([0, 3.5, 0]))\n        titles.add(overhead_title)\n        # Probability title\n        self.probability_title = Text(\"Probability\").scale(0.5)\n        self.probability_title.move_to(self.discriminator.input_layers[-2])\n        self.probability_title.shift(UP)\n        self.probability_title.shift(RIGHT * 1.05)\n        titles.add(self.probability_title)\n\n        return titles\n\n    def make_highlight_generator_rectangle(self):\n        \"\"\"Returns animation that highlights the generators contents\"\"\"\n        group = VGroup()\n\n        generator_surrounding_group = Group(self.generator, self.fake_image_layer_title)\n\n        generator_surrounding_rectangle = SurroundingRectangle(\n            generator_surrounding_group, buff=0.1, stroke_width=4.0, color=\"#0FFF50\"\n        )\n        group.add(generator_surrounding_rectangle)\n        title = Text(\"Generator\").scale(0.5)\n        title.next_to(generator_surrounding_rectangle, UP, 0.2)\n        group.add(title)\n\n        return group\n\n    def make_highlight_discriminator_rectangle(self):\n        \"\"\"Makes a rectangle for highlighting the discriminator\"\"\"\n        discriminator_group = Group(\n            self.discriminator,\n            self.fake_image_layer,\n            self.ground_truth_layer,\n            self.fake_image_layer_title,\n            self.probability_title,\n        )\n\n        group = VGroup()\n\n        discriminator_surrounding_rectangle = SurroundingRectangle(\n            discriminator_group, buff=0.05, stroke_width=4.0, color=\"#0FFF50\"\n        )\n        group.add(discriminator_surrounding_rectangle)\n        title = Text(\"Discriminator\").scale(0.5)\n        title.next_to(discriminator_surrounding_rectangle, UP, 0.2)\n        group.add(title)\n\n        return group\n\n    def make_generator_forward_pass(self):\n        \"\"\"Makes forward pass of the generator\"\"\"\n\n        forward_pass = self.generator.make_forward_pass_animation(dist_theme=\"ellipse\")\n\n        return forward_pass\n\n    def make_discriminator_forward_pass(self):\n        \"\"\"Makes forward pass of the discriminator\"\"\"\n\n        disc_forward = self.discriminator.make_forward_pass_animation()\n\n        return disc_forward\n\n    @override_animation(Create)\n    def _create_override(self):\n        \"\"\"Overrides create\"\"\"\n        animation_group = AnimationGroup(\n            Create(self.generator),\n            Create(self.discriminator),\n            Create(self.ground_truth_layer),\n            Create(self.titles),\n        )\n        return animation_group\n\n\nclass GANScene(Scene):\n    \"\"\"GAN Scene\"\"\"\n\n    def construct(self):\n        gan = GAN().scale(1.70)\n        gan.move_to(ORIGIN)\n        gan.shift(DOWN * 0.35)\n        gan.shift(LEFT * 0.1)\n        self.play(Create(gan), run_time=3)\n        # Highlight generator\n        highlight_generator_rectangle = gan.make_highlight_generator_rectangle()\n        self.play(Create(highlight_generator_rectangle), run_time=1)\n        # Generator forward pass\n        gen_forward_pass = gan.make_generator_forward_pass()\n        self.play(gen_forward_pass, run_time=5)\n        # Fade out generator highlight\n        self.play(Uncreate(highlight_generator_rectangle), run_time=1)\n        # Highlight discriminator\n        highlight_discriminator_rectangle = gan.make_highlight_discriminator_rectangle()\n        self.play(Create(highlight_discriminator_rectangle), run_time=1)\n        # Discriminator forward pass\n        discriminator_forward_pass = gan.make_discriminator_forward_pass()\n        self.play(discriminator_forward_pass, run_time=5)\n        # Unhighlight discriminator\n        self.play(Uncreate(highlight_discriminator_rectangle), run_time=1)\n", "original_prompt": "Gan Gan"}
{"prompt": "\"Create a 3D animation showing VAE interpolation with zooming and fading in animations on top.\"", "response": "\"\"\"Visualization of VAE Interpolation\"\"\"\nimport sys\nimport os\n\nsys.path.append(os.environ[\"PROJECT_ROOT\"])\nfrom manim import *\nimport pickle\nimport numpy as np\nimport manim_ml.neural_network as neural_network\nimport examples.variational_autoencoder.variational_autoencoder as variational_autoencoder\n\n\"\"\"\n    The VAE Scene for the twitter video. \n\"\"\"\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n# Set random seed so point distribution is constant\nnp.random.seed(1)\n\n\nclass InterpolationScene(MovingCameraScene):\n    \"\"\"Scene object for a Variational Autoencoder and Autoencoder\"\"\"\n\n    def construct(self):\n        # Set Scene config\n        vae = variational_autoencoder.VariationalAutoencoder(\n            dot_radius=0.035, layer_spacing=0.5\n        )\n        vae.move_to(ORIGIN)\n        vae.encoder.shift(LEFT * 0.5)\n        vae.decoder.shift(RIGHT * 0.5)\n        mnist_image_handler = variational_autoencoder.MNISTImageHandler()\n        image_pair = mnist_image_handler.image_pairs[3]\n        # Make forward pass animation and DO NOT run it\n        forward_pass_animation = vae.make_forward_pass_animation(image_pair)\n        # Make the interpolation animation\n        interpolation_images = mnist_image_handler.interpolation_images\n        interpolation_animation = vae.make_interpolation_animation(interpolation_images)\n        embedding_zoom_animation = self.camera.auto_zoom(\n            [vae.embedding, vae.decoder, vae.output_image], margin=0.5\n        )\n        # Make animations\n        forward_pass_animations = []\n        for i in range(7):\n            anim = vae.decoder.make_forward_propagation_animation(run_time=0.5)\n            forward_pass_animations.append(anim)\n        forward_pass_animation_group = AnimationGroup(\n            *forward_pass_animations, lag_ratio=1.0\n        )\n        # Make forward pass animations\n        self.play(Create(vae), run_time=1.5)\n        self.play(FadeOut(vae.encoder), run_time=1.0)\n        self.play(embedding_zoom_animation, run_time=1.5)\n        interpolation_animation = AnimationGroup(\n            forward_pass_animation_group, interpolation_animation\n        )\n        self.play(interpolation_animation, run_time=9.0)\n", "original_prompt": "Interpolation Interpolation"}
{"prompt": "Demonstrate a 3D neural network visualization, showcasing layers with transformations like image input, convolutional and pooling operations.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 20.0\nconfig.frame_width = 20.0\n\nROOT_DIR = Path(__file__).parents[2]\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=4.5),\n                Convolutional2DLayer(1, 28),\n                Convolutional2DLayer(6, 28, 5),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(16, 10, 5),\n                MaxPooling2DLayer(kernel_size=2),\n                FeedForwardLayer(8),\n                FeedForwardLayer(3),\n                FeedForwardLayer(2),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # Group it all\n        # group = Group(nn, code)\n        # group.move_to(ORIGIN)\n        nn.move_to(ORIGIN)\n        # Play animation\n        # forward_pass = nn.make_forward_pass_animation()\n        # self.wait(1)\n        # self.play(forward_pass)\n", "original_prompt": "Lenet Lenet"}
{"prompt": "Demonstrate a Manim animation showcasing a neural network logo with text and an underline, scaled to fit within a 4x4 frame.", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\n\nimport manim_ml\nmanim_ml.config.color_scheme = \"light_mode\"\n\nfrom manim_ml.neural_network.architectures.feed_forward import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 1000\nconfig.pixel_width = 1000\nconfig.frame_height = 4.0\nconfig.frame_width = 4.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.text = Text(\"ManimML\", color=manim_ml.config.color_scheme.text_color)\n        self.text.scale(1.0)\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 6, 3], layer_spacing=0.3, node_color=BLUE\n        )\n        self.neural_network.scale(1.0)\n        self.neural_network.move_to(self.text.get_bottom())\n        self.neural_network.shift(1.25 * DOWN)\n        self.logo_group = Group(self.text, self.neural_network)\n        self.logo_group.scale(1.0)\n        self.logo_group.move_to(ORIGIN)\n        self.play(Write(self.text), run_time=1.0)\n        self.play(Create(self.neural_network), run_time=3.0)\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            # Create(self.surrounding_rectangle)\n        )\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            #    Create(self.surrounding_rectangle)\n        )\n        self.play(animation_group, runtime=5.0)\n        self.wait(5)\n", "original_prompt": "Logo Logo"}
{"prompt": "Create a visually striking logo featuring a scaled-up neural network diagram with animated forward pass.", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\nfrom manim_ml.neural_network.neural_network import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 400\nconfig.pixel_width = 600\nconfig.frame_height = 8.0\nconfig.frame_width = 10.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 5], layer_spacing=0.6, node_color=BLUE, edge_width=6\n        )\n        self.neural_network.scale(3)\n        self.neural_network.move_to(ORIGIN)\n        self.play(Create(self.neural_network))\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n        )\n        self.play(animation_group)\n        self.wait(5)\n", "original_prompt": "Logo Website Logo"}
{"prompt": "Demonstrate a Manim animation showcasing a neural network logo with text and animations of calculations.", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\n\nimport manim_ml\nmanim_ml.config.color_scheme = \"light_mode\"\n\nfrom manim_ml.neural_network.architectures.feed_forward import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 1000\nconfig.pixel_width = 2000\nconfig.frame_height = 4.0\nconfig.frame_width = 8.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.text = Text(\"ManimML\", color=manim_ml.config.color_scheme.text_color)\n        self.text.scale(1.0)\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 6, 3], layer_spacing=0.3, node_color=BLUE\n        )\n        self.neural_network.scale(0.8)\n        self.neural_network.next_to(self.text, RIGHT, buff=0.5)\n        # self.neural_network.move_to(self.text.get_right())\n        # self.neural_network.shift(1.25 * DOWN)\n        self.logo_group = Group(self.text, self.neural_network)\n        self.logo_group.scale(1.0)\n        self.logo_group.move_to(ORIGIN)\n        self.play(Write(self.text), run_time=1.0)\n        self.play(Create(self.neural_network), run_time=3.0)\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            # Create(self.surrounding_rectangle)\n        )\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            #    Create(self.surrounding_rectangle)\n        )\n        self.play(animation_group, runtime=5.0)\n        self.wait(5)\n", "original_prompt": "Logo Wide Logo"}
{"prompt": "Demonstrate a Gaussian Mixture Model with animated MCMC chain sampling and visualization on a dark background.", "response": "from manim import *\n\nimport scipy.stats\nfrom manim_ml.diffusion.mcmc import MCMCAxes\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use('dark_background')\n\n# Make the specific scene\nconfig.pixel_height = 720\nconfig.pixel_width = 720\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\nclass MCMCWarmupScene(Scene):\n\n    def construct(self):\n        # Define the Gaussian Mixture likelihood\n        def gaussian_mm_logpdf(x):\n            \"\"\"Gaussian Mixture Model Log PDF\"\"\"\n            # Choose two arbitrary Gaussians\n            # Big Gaussian\n            big_gaussian_pdf = scipy.stats.multivariate_normal(\n                mean=[-0.5, -0.5],\n                cov=[1.0, 1.0]\n            ).pdf(x)\n            # Little Gaussian\n            little_gaussian_pdf = scipy.stats.multivariate_normal(\n                mean=[2.3, 1.9],\n                cov=[0.3, 0.3]\n            ).pdf(x)\n            # Sum their likelihoods and take the log\n            logpdf = np.log(big_gaussian_pdf + little_gaussian_pdf)\n\n            return logpdf\n\n        # Generate a bunch of true samples\n        true_samples = []\n        # Generate samples for little gaussian\n        little_gaussian_samples = np.random.multivariate_normal(\n            mean=[2.3, 1.9],\n            cov=[[0.3, 0.0], [0.0, 0.3]],\n            size=(10000)\n        )\n        big_gaussian_samples = np.random.multivariate_normal(\n            mean=[-0.5, -0.5],\n            cov=[[1.0, 0.0], [0.0, 1.0]],\n            size=(10000)\n        )\n        true_samples = np.concatenate((little_gaussian_samples, big_gaussian_samples))\n        # Make the MCMC axes\n        axes = MCMCAxes(\n            x_range=[-5, 5],\n            y_range=[-5, 5],\n            x_length=7.0,\n            y_length=7.0\n        )\n        axes.move_to(ORIGIN)\n        self.play(\n            Create(axes)\n        )\n        # Make the chain sampling animation \n        chain_sampling_animation = axes.visualize_metropolis_hastings_chain_sampling(\n            log_prob_fn=gaussian_mm_logpdf, \n            true_samples=true_samples,\n            sampling_kwargs={\n                \"iterations\": 2000,\n                \"warm_up\": 50,\n                \"initial_location\": np.array([-3.5, 3.5]),\n                \"sampling_seed\": 4\n            },\n        )\n        self.play(chain_sampling_animation)\n        self.wait(3)\n", "original_prompt": "Mcmc Warmup Mcmc"}
{"prompt": "The code snippet you provided appears to be a part of a larger program that creates and animates a 3D scene. The specific functionality being asked is not explicitly stated, but based on the context, it seems like you're looking for suggestions or improvements on how to optimize the animation of the Neocognitron model.\n\nHere are a few observations and potential suggestions:\n\n1. **Performance optimization**: If the animation is slow, you might want to consider using `VGroup` to group the objects in your scene together, which can reduce the number of object updates during each frame.\n2. **Camera orientation**: The `set_camera_orientation` method seems like it could be improved by making it a separate function or by allowing users to input specific angles for each component (e.g., phi, theta, gamma).\n3. **Scene layout**: The use of `shift` and `next_to` methods to position objects in the scene can be flexible but might become unwieldy as the number of layers increases. Consider using a more structured approach, such as creating subplots or sections for each layer.\n4. **Animation timing**: It's not clear what the desired animation speed is. You could consider adding an `animation_time` parameter to your animations to control how long they take.\n\nHere's a sample code snippet that demonstrates some of these suggestions:\n\n```python\nclass Scene(ThreeDScene):\n    def construct(self):\n        neocognitron = Neocognitron(self.camera)\n        neocognitron.shift(DOWN*0.5)\n\n        # Create separate subplots for the layers to reduce clutter\n        layer_spacing = 1.5\n        self.add_layer(neocognitron, layer_spacing)\n\n        # Create an animation of the forward pass\n        self.play(neocognitron.make_forward_pass_animation())\n\n    def add_layer(self, layer, spacing):\n        # Group all objects from this layer together\n        group = VGroup()\n        for obj in layer.layers:\n            if isinstance(obj, NeocognitronLayer):\n                group.add(obj.plane_label)\n                group.add(obj)\n        group.shift(spacing * ORIGIN)\n\n        # Position the layer and add it to the scene\n        layer.move_to(ORIGIN)\n        self.add(group)\n```\n\nThis code uses `VGroup` to simplify the animation of individual layers and improves the structure of the scene. The `add_layer` method allows for easy extension or modification of each layer's position.\n\nThese are just a few potential suggestions, and you may have different requirements or preferences depending on your specific use case.", "response": "from manim import *\n\nfrom manim_ml.neural_network import NeuralNetwork\nfrom manim_ml.neural_network.layers.parent_layers import NeuralNetworkLayer, ConnectiveLayer, ThreeDLayer\nimport manim_ml\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 10.5\nconfig.frame_width = 10.5\n\nclass NeocognitronFilter(VGroup):\n    \"\"\"\n        Filter connecting the S and C Cells of a neocognitron layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_cell, \n        output_cell, \n        receptive_field_radius, \n        outline_color=BLUE,\n        active_color=ORANGE, \n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.outline_color = outline_color\n        self.active_color = active_color\n        self.input_cell = input_cell\n        self.output_cell = output_cell\n        # Draw the receptive field\n        # Make the points of a equilateral triangle in the plane of the \n        # cell about a random center\n        # center_point = input_cell.get_center()\n        shift_amount_x = np.random.uniform(\n            -(input_cell.get_height()/2 - receptive_field_radius - 0.01),\n            input_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        shift_amount_y = np.random.uniform(\n            -(input_cell.get_height()/2 - receptive_field_radius - 0.01),\n            input_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        # center_point += np.array([shift_amount_x, shift_amount_y, 0])\n        # # Make the triangle points\n        # side_length = np.sqrt(3) * receptive_field_radius\n        # normal_vector = np.cross(\n        #     input_cell.get_left() - input_cell.get_center(),\n        #     input_cell.get_top() - input_cell.get_center(),\n        # )\n        # Get vector in the plane of the cell\n        # vector_in_plane = input_cell.get_left() - input_cell.get_center()\n        # point_a = center_point + vector_in_plane * receptive_field_radius\n        # # rotate the vector 120 degrees about the vector normal to the cell\n        # vector_in_plane = rotate_vector(vector_in_plane, PI/3, normal_vector)\n        # point_b = center_point + vector_in_plane * receptive_field_radius\n        # # rotate the vector 120 degrees about the vector normal to the cell\n        # vector_in_plane = rotate_vector(vector_in_plane, PI/3, normal_vector)\n        # point_c = center_point + vector_in_plane * receptive_field_radius\n        # self.receptive_field = Circle.from_three_points(\n        #     point_a, \n        #     point_b,\n        #     point_c,\n        #     color=outline_color,\n        #     stroke_width=2.0,\n        # )\n        self.receptive_field = Circle(\n            radius=receptive_field_radius,\n            color=outline_color,\n            stroke_width=3.0,\n        )\n        self.add(self.receptive_field)\n        # Move receptive field to a random point within the cell\n        # minus the radius of the receptive field\n        self.receptive_field.move_to(\n            input_cell\n        )\n        # Shift a random amount in the x and y direction within \n        self.receptive_field.shift(\n            np.array([shift_amount_x, shift_amount_y, 0])\n        )\n        # Choose a random point on the c_cell\n        shift_amount_x = np.random.uniform(\n            -(output_cell.get_height()/2 - receptive_field_radius - 0.01),\n            output_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        shift_amount_y = np.random.uniform(\n            -(output_cell.get_height()/2 - receptive_field_radius - 0.01),\n            output_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        self.dot = Dot(\n            color=outline_color,\n            radius=0.04\n        )\n        self.dot.move_to(output_cell.get_center() + np.array([shift_amount_x, shift_amount_y, 0]))\n        self.add(self.dot)\n        # Make lines connecting receptive field to the dot\n        self.lines = VGroup()\n        self.lines.add(\n            Line(\n                self.receptive_field.get_center() + np.array([0, receptive_field_radius, 0]),\n                self.dot,\n                color=outline_color,\n                stroke_width=3.0,\n            )\n        )\n        self.lines.add(\n            Line(\n                self.receptive_field.get_center() - np.array([0, receptive_field_radius, 0]),\n                self.dot,\n                color=outline_color,\n                stroke_width=3.0,\n            )\n        )\n        self.add(self.lines)\n\n    def make_filter_pulse_animation(self, **kwargs):\n        succession = Succession(\n            ApplyMethod(\n                self.receptive_field.set_color, \n                self.active_color, \n                run_time=0.25\n            ),\n            ApplyMethod(\n                self.receptive_field.set_color, \n                self.outline_color, \n                run_time=0.25\n            ),\n            ShowPassingFlash(\n                self.lines.copy().set_color(self.active_color),\n                time_width=0.5,\n            ),\n            ApplyMethod(\n                self.dot.set_color, \n                self.active_color, \n                run_time=0.25\n            ),\n            ApplyMethod(\n                self.dot.set_color, \n                self.outline_color, \n                run_time=0.25\n            ),\n        )\n\n        return succession\n\nclass NeocognitronLayer(NeuralNetworkLayer, ThreeDLayer):\n    \n    def __init__(\n        self, \n        num_cells, \n        cell_height,\n        receptive_field_radius,\n        layer_name,\n        active_color=manim_ml.config.color_scheme.active_color,\n        cell_color=manim_ml.config.color_scheme.secondary_color,\n        outline_color=manim_ml.config.color_scheme.primary_color,\n        show_outline=True,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.num_cells = num_cells\n        self.cell_height = cell_height\n        self.receptive_field_radius = receptive_field_radius\n        self.layer_name = layer_name\n        self.active_color = active_color\n        self.cell_color = cell_color\n        self.outline_color = outline_color\n        self.show_outline = show_outline\n\n        self.plane_label = Text(layer_name).scale(0.4)\n\n    def make_cell_plane(self, layer_name, cell_buffer=0.1, stroke_width=2.0):\n        \"\"\"Makes a plane of cells. \n        \"\"\"\n        cell_plane = VGroup()\n        for cell_index in range(self.num_cells):\n            # Draw the cell box\n            cell_box = Rectangle(\n                width=self.cell_height,\n                height=self.cell_height,\n                color=self.cell_color,\n                fill_color=self.cell_color,\n                fill_opacity=0.3,\n                stroke_width=stroke_width,\n            )\n            if cell_index > 0:\n                cell_box.next_to(cell_plane[-1], DOWN, buff=cell_buffer)\n            cell_plane.add(cell_box)\n        # Draw the outline\n        if self.show_outline:\n            self.plane_outline = SurroundingRectangle(\n                cell_plane,\n                color=self.cell_color,\n                buff=cell_buffer,\n                stroke_width=stroke_width,\n            )\n            cell_plane.add(\n                self.plane_outline\n            )\n        # Draw a label above the container box\n        self.plane_label.next_to(cell_plane, UP, buff=0.2)\n        cell_plane.add(self.plane_label)\n\n        return cell_plane\n\n    def construct_layer(self, input_layer, output_layer, **kwargs):\n        # Make the Cell layer\n        self.cell_plane = self.make_cell_plane(self.layer_name)\n        self.add(self.cell_plane)\n        \n    def make_forward_pass_animation(self, layer_args={}, **kwargs):\n        \"\"\"Forward pass for query\"\"\"\n        # # Pulse and un pulse the cell plane rectangle\n        flash_outline_animations = []\n        for cell in self.cell_plane:\n            flash_outline_animations.append(\n                Succession(\n                    ApplyMethod(\n                        cell.set_stroke_color,\n                        self.active_color,\n                        run_time=0.25\n                    ),\n                    ApplyMethod(\n                        cell.set_stroke_color,\n                        self.outline_color,\n                        run_time=0.25\n                    )\n                )\n            )\n        \n        return AnimationGroup(\n            *flash_outline_animations,\n            lag_ratio=0.0\n        )\n\nclass NeocognitronToNeocognitronLayer(ConnectiveLayer):\n    input_class = NeocognitronLayer\n    output_class = NeocognitronLayer\n\n    def __init__(self, input_layer, output_layer, **kwargs):\n        super().__init__(input_layer, output_layer, **kwargs)\n\n    def construct_layer(self, input_layer, output_layer, **kwargs):\n        self.filters = []\n        for cell_index in range(input_layer.num_cells):\n            input_cell = input_layer.cell_plane[cell_index]\n            # Randomly choose a cell from the output layer\n            output_cell = output_layer.cell_plane[\n                np.random.randint(0, output_layer.num_cells)\n            ]\n            # Make the filter\n            filter = NeocognitronFilter(\n                input_cell=input_cell,\n                output_cell=output_cell,\n                receptive_field_radius=input_layer.receptive_field_radius,\n                outline_color=self.input_layer.outline_color\n            )\n            # filter = NeocognitronFilter(\n            #     outline_color=input_layer.outline_color\n            # )\n            self.filters.append(filter)\n\n        self.add(VGroup(*self.filters))\n\n    def make_forward_pass_animation(self, layer_args={}, **kwargs):\n        \"\"\"Forward pass for query\"\"\"\n        filter_pulses = []\n        for filter in self.filters:\n            filter_pulses.append(\n                filter.make_filter_pulse_animation()\n            )\n        return AnimationGroup(\n            *filter_pulses\n        )\n\nmanim_ml.neural_network.layers.util.register_custom_connective_layer(\n    NeocognitronToNeocognitronLayer,\n)\n\nclass Neocognitron(NeuralNetwork):\n\n    def __init__(\n        self, \n        camera,\n        cells_per_layer=[4, 5, 4, 4, 3, 3, 5, 5],\n        cell_heights=[0.8, 0.8, 0.8*0.75, 0.8*0.75, 0.8*0.75**2, 0.8*0.75**2, 0.8*0.75**3, 0.02],\n        layer_names=[\"S1\", \"C1\", \"S2\", \"C2\", \"S3\", \"C3\", \"S4\", \"C4\"],\n        receptive_field_sizes=[0.2, 0.2, 0.2*0.75, 0.2*0.75, 0.2*0.75**2, 0.2*0.75**2, 0.2*0.75**3, 0.0],\n    ):\n        self.cells_per_layer = cells_per_layer\n        self.cell_heights = cell_heights\n        self.layer_names = layer_names\n        self.receptive_field_sizes = receptive_field_sizes\n        # Make the neo-cognitron input layer\n        input_layers = []\n        input_layers.append(\n            NeocognitronLayer(\n                1,\n                1.5,\n                0.2,\n                \"U0\",\n                show_outline=False,\n            )\n        )\n        # Make the neo-cognitron layers\n        for i in range(len(cells_per_layer)):\n            layer = NeocognitronLayer(\n                cells_per_layer[i],\n                cell_heights[i],\n                receptive_field_sizes[i],\n                layer_names[i]\n            )\n            input_layers.append(layer)\n        \n        # Make all of the layer labels fixed in frame\n        for layer in input_layers:\n            if isinstance(layer, NeocognitronLayer):\n                # camera.add_fixed_orientation_mobjects(layer.plane_label)\n                pass\n\n        all_layers = []\n        for layer_index in range(len(input_layers) - 1):\n            input_layer = input_layers[layer_index]\n            all_layers.append(input_layer)\n            output_layer = input_layers[layer_index + 1]\n            connective_layer = NeocognitronToNeocognitronLayer(\n                input_layer,\n                output_layer\n            )\n            all_layers.append(connective_layer)\n        all_layers.append(input_layers[-1])\n\n        def custom_layout_func(neural_network):\n            # Insert the connective layers\n            # Pass the layers to neural network super class\n            # Rotate pairs of layers\n            layer_index = 1\n            while layer_index < len(input_layers):\n                prev_layer = input_layers[layer_index - 1]\n                s_layer = input_layers[layer_index]\n                s_layer.next_to(prev_layer, RIGHT, buff=0.0)\n                s_layer.shift(RIGHT * 0.4)\n                c_layer = input_layers[layer_index + 1]\n                c_layer.next_to(s_layer, RIGHT, buff=0.0)\n                c_layer.shift(RIGHT * 0.2)\n                # Rotate the pair of layers\n                group = Group(s_layer, c_layer)\n                group.move_to(np.array([group.get_center()[0], 0, 0]))\n\n                # group.shift(layer_index * 3 * np.array([0, 0, 1.0]))\n                # group.rotate(40 * DEGREES, axis=UP, about_point=group.get_center())\n                # c_layer.rotate(40*DEGREES, axis=UP, about_point=group.get_center())\n                # s_layer.shift(\n                #     layer_index // 2 * np.array([0, 0, 0.3])\n                # )\n                # c_layer.shift(\n                #     layer_index // 2 * np.array([0, 0, 0.3])\n                # )\n                layer_index += 2\n            neural_network.move_to(ORIGIN)\n\n        super().__init__(\n            all_layers,\n            layer_spacing=0.5,\n            custom_layout_func=custom_layout_func\n        )\n\nclass Scene(ThreeDScene):\n\n    def construct(self):\n        neocognitron = Neocognitron(self.camera)\n        neocognitron.shift(DOWN*0.5)\n        self.add(neocognitron)\n        title = Text(\"Neocognitron\").scale(1)\n        self.add_fixed_in_frame_mobjects(title)\n        title.next_to(neocognitron, UP, buff=0.4)\n        self.add(title)\n        \"\"\"\n        self.play(\n            neocognitron.make_forward_pass_animation()\n        )\n        \"\"\"\n        print(self.camera.gamma)\n        print(self.camera.theta)\n        print(self.camera.phi)\n        neocognitron.rotate(90*DEGREES, axis=RIGHT)\n        neocognitron.shift(np.array([0, 0, -0.2]))\n        # title.rotate(90*DEGREES, axis=RIGHT)\n        # self.set_camera_orientation(phi=-15*DEGREES)\n        # vec = np.array([0.0, 0.2, 0.0])\n        # vec /= np.linalg.norm(vec)\n        # x, y, z = vec[0], vec[1], vec[2]\n        # theta = np.arccos(z)\n        # phi = np.arctan(y / x)\n        self.set_camera_orientation(phi=90 * DEGREES, theta=-70*DEGREES, gamma=0*DEGREES)\n        # self.set_camera_orientation(theta=theta, phi=phi)\n\n        forward_pass = neocognitron.make_forward_pass_animation()\n        self.play(forward_pass)\n\n", "original_prompt": "Neocognitron Neocognitron"}
{"prompt": "It appears that the code snippet you provided is a Python script for creating a visualization of an oracle-guided image synthesis process using VAE (Variational Autoencoder). The script uses a combination of classes and functions to create the architecture, perform localization procedure, training procedure, and generate estimate images.\n\nHere's a high-level overview of how the script works:\n\n1. **Architecture Creation**: The `construct` method creates the neural network architecture, which includes the encoder, decoder, and triplet layer.\n2. **Localization Procedure**: The `make_localization_time_step` method performs a single iteration of the localization procedure, which involves calculating the posterior distribution and updating it based on the prior distribution.\n3. **Training Procedure**: The `make_triplet_forward_animation` method performs an unsupervised image reconstruction forward pass using the encoder.\n4. **Estimate Generation**: The `make_generate_estimate_animation` method generates an estimate image by sampling from the posterior distribution.\n\nSome key classes and functions in the script include:\n\n*   `Create`: A class that creates a visualization of a VAE architecture.\n*   `TripletLayer`: A class that represents a triplet layer, which is used for unsupervised image reconstruction.\n*   `Encoder` and `Decoder`: Classes that represent the encoder and decoder components of the neural network architecture.\n*   `Localizer`: A class that performs the localization procedure.\n\nThe script uses various visualization tools and libraries, such as Matplotlib, to create interactive visualizations of the VAE architecture, localization procedure, and estimate generation process.", "response": "\"\"\"\n    Here is a animated explanatory figure for the \"Oracle Guided Image Synthesis with Relative Queries\" paper. \n\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\nfrom manim_ml.neural_network.layers import triplet\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.paired_query import PairedQueryLayer\nfrom manim_ml.neural_network.layers.triplet import TripletLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom manim_ml.neural_network.layers import FeedForwardLayer, EmbeddingLayer\nfrom manim_ml.neural_network.layers.util import get_connective_layer\nimport os\n\nfrom manim_ml.utils.mobjects.probability import GaussianDistribution\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\nROOT_DIR = Path(__file__).parents[3]\n\n\nclass Localizer:\n    \"\"\"\n    Holds the localizer object, which contains the queries, images, etc.\n    needed to represent a localization run.\n    \"\"\"\n\n    def __init__(self, axes):\n        # Set dummy values for these\n        self.index = -1\n        self.axes = axes\n        self.num_queries = 3\n        self.assets_path = ROOT_DIR / \"assets/oracle_guidance\"\n        self.ground_truth_image_path = self.assets_path / \"ground_truth.jpg\"\n        self.ground_truth_location = np.array([2, 3])\n        # Prior distribution\n        print(\"initial gaussian\")\n        self.prior_distribution = GaussianDistribution(\n            self.axes,\n            mean=np.array([0.0, 0.0]),\n            cov=np.array([[3, 0], [0, 3]]),\n            dist_theme=\"ellipse\",\n            color=GREEN,\n        )\n        # Define the query images and embedded locations\n        # Contains image paths [(positive_path, negative_path), ...]\n        self.query_image_paths = [\n            (\n                os.path.join(self.assets_path, \"positive_1.jpg\"),\n                os.path.join(self.assets_path, \"negative_1.jpg\"),\n            ),\n            (\n                os.path.join(self.assets_path, \"positive_2.jpg\"),\n                os.path.join(self.assets_path, \"negative_2.jpg\"),\n            ),\n            (\n                os.path.join(self.assets_path, \"positive_3.jpg\"),\n                os.path.join(self.assets_path, \"negative_3.jpg\"),\n            ),\n        ]\n        # Contains 2D locations for each image [([2, 3], [2, 4]), ...]\n        self.query_locations = [\n            (np.array([-1, -1]), np.array([1, 1])),\n            (np.array([1, -1]), np.array([-1, 1])),\n            (np.array([0.3, -0.6]), np.array([-0.5, 0.7])),\n        ]\n        # Make the covariances for each query\n        self.query_covariances = [\n            (np.array([[0.3, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n            (np.array([[0.2, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n            (np.array([[0.2, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n        ]\n        # Posterior distributions over time GaussianDistribution objects\n        self.posterior_distributions = [\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([-0.3, -0.3]),\n                cov=np.array([[5, -4], [-4, 6]]),\n            ).scale(0.6),\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([0.25, -0.25]),\n                cov=np.array([[3, -2], [-2, 4]]),\n            ).scale(0.35),\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([0.4, -0.35]),\n                cov=np.array([[1, 0], [0, 1]]),\n            ).scale(0.3),\n        ]\n        # Some assumptions\n        assert len(self.query_locations) == len(self.query_image_paths)\n        assert len(self.query_locations) == len(self.posterior_distributions)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"Steps through each localization time instance\"\"\"\n        if self.index < len(self.query_image_paths):\n            self.index += 1\n        else:\n            raise StopIteration\n\n        # Return query_paths, query_locations, posterior\n        out_tuple = (\n            self.query_image_paths[self.index],\n            self.query_locations[self.index],\n            self.posterior_distributions[self.index],\n            self.query_covariances[self.index],\n        )\n\n        return out_tuple\n\n\nclass OracleGuidanceVisualization(Scene):\n    def __init__(self):\n        super().__init__()\n        self.neural_network, self.embedding_layer = self.make_vae()\n        self.localizer = iter(Localizer(self.embedding_layer.axes))\n        self.subtitle = None\n        self.title = None\n        # Set image paths\n        # VAE embedding animation image paths\n        self.assets_path = ROOT_DIR / \"assets/oracle_guidance\"\n        self.input_embed_image_path = os.path.join(self.assets_path, \"input_image.jpg\")\n        self.output_embed_image_path = os.path.join(\n            self.assets_path, \"output_image.jpg\"\n        )\n\n    def make_vae(self):\n        \"\"\"Makes a simple VAE architecture\"\"\"\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\")\n        self.encoder = NeuralNetwork(\n            [\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n            ]\n        )\n\n        self.decoder = NeuralNetwork(\n            [\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n            ]\n        )\n\n        neural_network = NeuralNetwork([self.encoder, self.decoder])\n\n        neural_network.shift(DOWN * 0.4)\n        return neural_network, embedding_layer\n\n    @override_animation(Create)\n    def _create_animation(self):\n        animation_group = AnimationGroup(Create(self.neural_network))\n\n        return animation_group\n\n    def insert_at_start(self, layer, create=True):\n        \"\"\"Inserts a layer at the beggining of the network\"\"\"\n        # Note: Does not move the rest of the network\n        current_first_layer = self.encoder.all_layers[0]\n        # Get connective layer\n        connective_layer = get_connective_layer(layer, current_first_layer)\n        # Insert both layers\n        self.encoder.all_layers.insert(0, layer)\n        self.encoder.all_layers.insert(1, connective_layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        layer.shift(DOWN * 0.4)\n        layer.shift(LEFT * 2.35)\n        # Make insert animation\n        if not create:\n            animation_group = AnimationGroup(Create(connective_layer))\n        else:\n            animation_group = AnimationGroup(Create(layer), Create(connective_layer))\n        self.play(animation_group)\n\n    def remove_start_layer(self):\n        \"\"\"Removes the first layer of the network\"\"\"\n        first_layer = self.encoder.all_layers.remove_at_index(0)\n        first_connective = self.encoder.all_layers.remove_at_index(0)\n        # Make remove animations\n        animation_group = AnimationGroup(\n            FadeOut(first_layer), FadeOut(first_connective)\n        )\n\n        self.play(animation_group)\n\n    def insert_at_end(self, layer):\n        \"\"\"Inserts the given layer at the end of the network\"\"\"\n        current_last_layer = self.decoder.all_layers[-1]\n        # Get connective layer\n        connective_layer = get_connective_layer(current_last_layer, layer)\n        # Insert both layers\n        self.decoder.all_layers.add(connective_layer)\n        self.decoder.all_layers.add(layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        layer.shift(DOWN * 0.4)\n        layer.shift(RIGHT * 2.35)\n        # Make insert animation\n        animation_group = AnimationGroup(Create(layer), Create(connective_layer))\n        self.play(animation_group)\n\n    def remove_end_layer(self):\n        \"\"\"Removes the lsat layer of the network\"\"\"\n        first_layer = self.decoder.all_layers.remove_at_index(-1)\n        first_connective = self.decoder.all_layers.remove_at_index(-1)\n        # Make remove animations\n        animation_group = AnimationGroup(\n            FadeOut(first_layer), FadeOut(first_connective)\n        )\n\n        self.play(animation_group)\n\n    def change_title(self, text, title_location=np.array([0, 1.25, 0]), font_size=24):\n        \"\"\"Changes title to the given text\"\"\"\n        if self.title is None:\n            self.title = Text(text, font_size=font_size)\n            self.title.move_to(title_location)\n            self.add(self.title)\n            self.play(Write(self.title), run_time=1)\n            self.wait(1)\n            return\n\n        self.play(Unwrite(self.title))\n        new_title = Text(text, font_size=font_size)\n        new_title.move_to(self.title)\n        self.title = new_title\n        self.wait(0.1)\n        self.play(Write(self.title))\n\n    def change_subtitle(self, text, title_location=np.array([0, 0.9, 0]), font_size=20):\n        \"\"\"Changes subtitle to the next algorithm step\"\"\"\n        if self.subtitle is None:\n            self.subtitle = Text(text, font_size=font_size)\n            self.subtitle.move_to(title_location)\n            self.play(Write(self.subtitle))\n            return\n\n        self.play(Unwrite(self.subtitle))\n        new_title = Text(text, font_size=font_size)\n        new_title.move_to(title_location)\n        self.subtitle = new_title\n        self.wait(0.1)\n        self.play(Write(self.subtitle))\n\n    def make_embed_input_image_animation(self, input_image_path, output_image_path):\n        \"\"\"Makes embed input image animation\"\"\"\n        # insert the input image at the begginging\n        input_image_layer = ImageLayer.from_path(input_image_path)\n        input_image_layer.scale(0.6)\n        current_first_layer = self.encoder.all_layers[0]\n        # Get connective layer\n        connective_layer = get_connective_layer(input_image_layer, current_first_layer)\n        # Insert both layers\n        self.encoder.all_layers.insert(0, input_image_layer)\n        self.encoder.all_layers.insert(1, connective_layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        input_image_layer.shift(DOWN * 0.4)\n        input_image_layer.shift(LEFT * 2.35)\n        # Play full forward pass\n        forward_pass = self.neural_network.make_forward_pass_animation(\n            layer_args={\n                self.encoder: {\n                    self.embedding_layer: {\n                        \"dist_args\": {\n                            \"cov\": np.array([[1.5, 0], [0, 1.5]]),\n                            \"mean\": np.array([0.5, 0.5]),\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": ORANGE,\n                        }\n                    }\n                }\n            }\n        )\n        self.play(forward_pass)\n        # insert the output image at the end\n        output_image_layer = ImageLayer.from_path(output_image_path)\n        output_image_layer.scale(0.6)\n        self.insert_at_end(output_image_layer)\n        # Remove the input and output layers\n        self.remove_start_layer()\n        self.remove_end_layer()\n        # Remove the latent distribution\n        self.play(FadeOut(self.embedding_layer.latent_distribution))\n\n    def make_localization_time_step(self, old_posterior):\n        \"\"\"\n        Performs one query update for the localization procedure\n\n        Procedure:\n        a. Embed query input images\n        b. Oracle is asked a query\n        c. Query is embedded\n        d. Show posterior update\n        e. Show current recomendation\n        \"\"\"\n        # Helper functions\n        def embed_query_to_latent_space(query_locations, query_covariance):\n            \"\"\"Makes animation for a paired query\"\"\"\n            # Assumes first layer of neural network is a PairedQueryLayer\n            # Make the embedding animation\n            # Wait\n            self.play(Wait(1))\n            # Embed the query to latent space\n            self.change_subtitle(\"3. Embed the Query in Latent Space\")\n            # Make forward pass animation\n            self.embedding_layer.paired_query_mode = True\n            # Make embedding embed query animation\n            embed_query_animation = self.encoder.make_forward_pass_animation(\n                run_time=5,\n                layer_args={\n                    self.embedding_layer: {\n                        \"positive_dist_args\": {\n                            \"cov\": query_covariance[0],\n                            \"mean\": query_locations[0],\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": BLUE,\n                        },\n                        \"negative_dist_args\": {\n                            \"cov\": query_covariance[1],\n                            \"mean\": query_locations[1],\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": RED,\n                        },\n                    }\n                },\n            )\n            self.play(embed_query_animation)\n\n        # Access localizer information\n        query_paths, query_locations, posterior_distribution, query_covariances = next(\n            self.localizer\n        )\n        positive_path, negative_path = query_paths\n        # Make subtitle for present user with query\n        self.change_subtitle(\"2. Present User with Query\")\n        # Insert the layer into the encoder\n        query_layer = PairedQueryLayer.from_paths(\n            positive_path, negative_path, grayscale=False\n        )\n        query_layer.scale(0.5)\n        self.insert_at_start(query_layer)\n        # Embed query to latent space\n        query_to_latent_space_animation = embed_query_to_latent_space(\n            query_locations, query_covariances\n        )\n        # Wait\n        self.play(Wait(1))\n        # Update the posterior\n        self.change_subtitle(\"4. Update the Posterior\")\n        # Remove the old posterior\n        self.play(ReplacementTransform(old_posterior, posterior_distribution))\n        \"\"\"\n        self.play(\n            self.embedding_layer.remove_gaussian_distribution(self.localizer.posterior_distribution)\n        )\n        \"\"\"\n        # self.embedding_layer.add_gaussian_distribution(posterior_distribution)\n        # self.localizer.posterior_distribution = posterior_distribution\n        # Remove query layer\n        self.remove_start_layer()\n        # Remove query ellipses\n\n        fade_outs = []\n        for dist in self.embedding_layer.gaussian_distributions:\n            self.embedding_layer.gaussian_distributions.remove(dist)\n            fade_outs.append(FadeOut(dist))\n\n        if not len(fade_outs) == 0:\n            fade_outs = AnimationGroup(*fade_outs)\n            self.play(fade_outs)\n\n        return posterior_distribution\n\n    def make_generate_estimate_animation(self, estimate_image_path):\n        \"\"\"Makes the generate estimate animation\"\"\"\n        # Change embedding layer mode\n        self.embedding_layer.paired_query_mode = False\n        # Sample from posterior distribution\n        # self.embedding_layer.latent_distribution = self.localizer.posterior_distribution\n        emb_to_ff_ind = self.neural_network.all_layers.index_of(self.encoder)\n        embedding_to_ff = self.neural_network.all_layers[emb_to_ff_ind + 1]\n        self.play(embedding_to_ff.make_forward_pass_animation())\n        # Pass through decoder\n        self.play(self.decoder.make_forward_pass_animation(), run_time=1)\n        # Create Image layer after the decoder\n        output_image_layer = ImageLayer.from_path(estimate_image_path)\n        output_image_layer.scale(0.5)\n        self.insert_at_end(output_image_layer)\n        # Wait\n        self.wait(1)\n        # Remove the image at the end\n        print(self.neural_network)\n        self.remove_end_layer()\n\n    def make_triplet_forward_animation(self):\n        \"\"\"Make triplet forward animation\"\"\"\n        # Make triplet layer\n        anchor_path = os.path.join(self.assets_path, \"anchor.jpg\")\n        positive_path = os.path.join(self.assets_path, \"positive.jpg\")\n        negative_path = os.path.join(self.assets_path, \"negative.jpg\")\n        triplet_layer = TripletLayer.from_paths(\n            anchor_path,\n            positive_path,\n            negative_path,\n            grayscale=False,\n            font_size=100,\n            buff=1.05,\n        )\n        triplet_layer.scale(0.10)\n        self.insert_at_start(triplet_layer)\n        # Make latent triplet animation\n        self.play(\n            self.encoder.make_forward_pass_animation(\n                layer_args={\n                    self.embedding_layer: {\n                        \"triplet_args\": {\n                            \"anchor_dist\": {\n                                \"cov\": np.array([[0.3, 0], [0, 0.3]]),\n                                \"mean\": np.array([0.7, 1.4]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": BLUE,\n                            },\n                            \"positive_dist\": {\n                                \"cov\": np.array([[0.2, 0], [0, 0.2]]),\n                                \"mean\": np.array([0.8, -0.4]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": GREEN,\n                            },\n                            \"negative_dist\": {\n                                \"cov\": np.array([[0.4, 0], [0, 0.25]]),\n                                \"mean\": np.array([-1, -1.2]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": RED,\n                            },\n                        }\n                    }\n                },\n                run_time=3,\n            )\n        )\n\n    def construct(self):\n        \"\"\"\n        Makes the whole visualization.\n\n        1. Create the Architecture\n            a. Create the traditional VAE architecture with images\n        2. The Localization Procedure\n        3. The Training Procedure\n        \"\"\"\n        # 1. Create the Architecture\n        self.neural_network.scale(1.2)\n        create_vae = Create(self.neural_network)\n        self.play(create_vae, run_time=3)\n        # Make changing title\n        self.change_title(\"Oracle Guided Image Synthesis\\n      with Relative Queries\")\n        # 2. The Localization Procedure\n        self.change_title(\"The Localization Procedure\")\n        # Make algorithm subtitle\n        self.change_subtitle(\"Algorithm Steps\")\n        # Wait\n        self.play(Wait(1))\n        # Make prior distribution subtitle\n        self.change_subtitle(\"1. Calculate Prior Distribution\")\n        # Draw the prior distribution\n        self.play(Create(self.localizer.prior_distribution))\n        old_posterior = self.localizer.prior_distribution\n        # For N queries update the posterior\n        for query_index in range(self.localizer.num_queries):\n            # Make localization iteration\n            old_posterior = self.make_localization_time_step(old_posterior)\n            self.play(Wait(1))\n            if not query_index == self.localizer.num_queries - 1:\n                # Repeat\n                self.change_subtitle(\"5. Repeat\")\n                # Wait a second\n                self.play(Wait(1))\n        # Generate final estimate\n        self.change_subtitle(\"5. Generate Estimate Image\")\n        # Generate an estimate image\n        estimate_image_path = os.path.join(self.assets_path, \"estimate_image.jpg\")\n        self.make_generate_estimate_animation(estimate_image_path)\n        self.wait(1)\n        # Remove old posterior\n        self.play(FadeOut(old_posterior))\n        # 3. The Training Procedure\n        self.change_title(\"The Training Procedure\")\n        # Make training animation\n        # Do an Image forward pass\n        self.change_subtitle(\"1. Unsupervised Image Reconstruction\")\n        self.make_embed_input_image_animation(\n            self.input_embed_image_path, self.output_embed_image_path\n        )\n        self.wait(1)\n        # Do triplet forward pass\n        self.change_subtitle(\"2. Triplet Loss in Latent Space\")\n        self.make_triplet_forward_animation()\n        self.wait(1)\n", "original_prompt": "Paper Visualizations Oracle Guidance Oracle Guidance"}
{"prompt": "Show a 3D neural network diagram with layers and translations.", "response": "from manim import *\n\nfrom manim_ml.neural_network import FeedForwardLayer, NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1200\nconfig.frame_height = 4.0\nconfig.frame_width = 4.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                FeedForwardLayer(num_nodes=3),\n                FeedForwardLayer(num_nodes=5),\n                FeedForwardLayer(num_nodes=3),\n            ]\n        )\n        self.add(nn)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n", "original_prompt": "Readme Example A Simple Feed Forward Network"}
{"prompt": "Demonstrate a 3D neural network model composed of convolutional and feed-forward layers, with animated forward pass.", "response": "from manim import *\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, filter_spacing=0.32),\n                Convolutional2DLayer(\n                    3, 5, 3, filter_spacing=0.32, activation_function=\"ReLU\"\n                ),\n                FeedForwardLayer(3, activation_function=\"Sigmoid\"),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "original_prompt": "Readme Example Activation Functions"}
{"prompt": "Show a 3D neural network with convolutional and feedforward layers, animated in a forward pass.", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "original_prompt": "Readme Example Animating The Forward Pass"}
{"prompt": "Create a 3D neural network animation with images, convolutional layers and two feedforward layers, moving to the center of the screen.", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n    ImageLayer,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "original_prompt": "Readme Example Convolutional Neural Network With Images"}
{"prompt": "Show a 3D neural network with layered convolutional and feedforward layers transforming over time.", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n", "original_prompt": "Readme Example Convolutional Neural Networks"}
{"prompt": "Demonstrate a 3D neural network with multiple convolutional and feed-forward layers, animating its movement in space.", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "original_prompt": "Readme Example Example"}
{"prompt": "Demonstrate a 3D neural network model with convolutional and feedforward layers, then animate a forward pass.", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "original_prompt": "Readme Example First Neural Network"}
{"prompt": "Show a neural network with 4 layers (convolutions and max pooling) animating a forward pass over 10 seconds.", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass MaxPoolingScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 8),\n                Convolutional2DLayer(3, 6, 3),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(5, 2, 2),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "original_prompt": "Readme Example Max Pooling"}
{"prompt": "Demonstrate a 5-layer neural network with alternating input and hidden layer sizes, showcasing random weights animation.", "response": "from manim import *\nfrom manim_ml.neural_network.animations.dropout import (\n    make_neural_network_dropout_animation,\n)\nfrom manim_ml.neural_network import FeedForwardLayer, NeuralNetwork\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 5.0\nconfig.frame_width = 5.0\n\n\nclass DropoutNeuralNetworkScene(Scene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                FeedForwardLayer(3, rectangle_color=BLUE),\n                FeedForwardLayer(5, rectangle_color=BLUE),\n                FeedForwardLayer(3, rectangle_color=BLUE),\n                FeedForwardLayer(5, rectangle_color=BLUE),\n                FeedForwardLayer(4, rectangle_color=BLUE),\n            ],\n            layer_spacing=0.4,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        self.play(\n            make_neural_network_dropout_animation(\n                nn, dropout_rate=0.25, do_forward_pass=True\n            )\n        )\n        self.wait(1)\n", "original_prompt": "Readme Example Neural Network Dropout"}
{"prompt": "Demonstrate a 2D convolutional neural network's architecture, showcasing layered transformations with images and filters.", "response": "from manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass ConvolutionalNetworkScene(Scene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        self.play(nn.make_forward_pass_animation())\n", "original_prompt": "Readme Example Old Example"}
{"prompt": "\"Create a 3D animation that displays a single, animated text message in the scene.\"", "response": "from manim import *\n\n# Import modules here\n\n\nclass BasicScene(ThreeDScene):\n    def construct(self):\n        # Your code goes here\n        text = Text(\"Your first scene!\")\n        self.add(text)\n", "original_prompt": "Readme Example Setting Up A Scene"}
{"prompt": "Demonstrate a 3D neural network visualization with image input, two convolutional layers, and rotation.", "response": "from manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.parent_layers import ThreeDLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 800\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/doggo.jpeg\")\n        numpy_image = np.asarray(image)\n        # Rotate the Three D layer position\n        ThreeDLayer.rotation_angle = 15 * DEGREES\n        ThreeDLayer.rotation_axis = [1, -1.0, 0]\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, 7, 3, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 5, 1, 1, filter_spacing=0.18),\n            ],\n            layer_spacing=0.25,\n            layout_direction=\"top_to_bottom\",\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.scale(1.5)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation(\n            highlight_active_feature_map=True,\n        )\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Translation Equivariance Translation Equivariance"}
{"prompt": "Demonstrate visualization of digit 2 dataset's latent space, creating a grid of images with corresponding points in a lower-dimensional space.", "response": "import pickle\nimport sys\nimport os\n\nsys.path.append(os.environ[\"PROJECT_ROOT\"])\nfrom autoencoder_models.variational_autoencoder import (\n    VAE,\n    load_dataset,\n    load_vae_from_path,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport scipy\nimport scipy.stats\nimport cv2\n\n\ndef binned_images(model_path, num_x_bins=6, plot=False):\n    latent_dim = 2\n    model = load_vae_from_path(model_path, latent_dim)\n    image_dataset = load_dataset(digit=2)\n    # Compute embedding\n    num_images = 500\n    embedding = []\n    images = []\n    for i in range(num_images):\n        image, _ = image_dataset[i]\n        mean, _, recon, _ = model.forward(image)\n        mean = mean.detach().numpy()\n        recon = recon.detach().numpy()\n        recon = recon.reshape(32, 32)\n        images.append(recon.squeeze())\n        if latent_dim > 2:\n            mean = mean[:2]\n        embedding.append(mean)\n    images = np.stack(images)\n    tsne_points = np.array(embedding)\n    tsne_points = (tsne_points - tsne_points.mean(axis=0)) / (tsne_points.std(axis=0))\n    # make vis\n    num_points = np.shape(tsne_points)[0]\n    x_min = np.amin(tsne_points.T[0])\n    y_min = np.amin(tsne_points.T[1])\n    y_max = np.amax(tsne_points.T[1])\n    x_max = np.amax(tsne_points.T[0])\n    # make the bins from the ranges\n    # to keep it square the same width is used for x and y dim\n    x_bins, step = np.linspace(x_min, x_max, num_x_bins, retstep=True)\n    x_bins = x_bins.astype(float)\n    num_y_bins = np.absolute(np.ceil((y_max - y_min) / step)).astype(int)\n    y_bins = np.linspace(y_min, y_max, num_y_bins)\n    # sort the tsne_points into a 2d histogram\n    tsne_points = tsne_points.squeeze()\n    hist_obj = scipy.stats.binned_statistic_dd(\n        tsne_points,\n        np.arange(num_points),\n        statistic=\"count\",\n        bins=[x_bins, y_bins],\n        expand_binnumbers=True,\n    )\n    # sample one point from each bucket\n    binnumbers = hist_obj.binnumber\n    num_x_bins = np.amax(binnumbers[0]) + 1\n    num_y_bins = np.amax(binnumbers[1]) + 1\n    binnumbers = binnumbers.T\n    # some places have no value in a region\n    used_mask = np.zeros((num_y_bins, num_x_bins))\n    image_bins = np.zeros(\n        (num_y_bins, num_x_bins, 3, np.shape(images)[2], np.shape(images)[2])\n    )\n    for i, bin_num in enumerate(list(binnumbers)):\n        used_mask[bin_num[1], bin_num[0]] = 1\n        image_bins[bin_num[1], bin_num[0]] = images[i]\n    # plot a grid of the images\n    fig, axs = plt.subplots(\n        nrows=np.shape(y_bins)[0],\n        ncols=np.shape(x_bins)[0],\n        constrained_layout=False,\n        dpi=50,\n    )\n    images = []\n    bin_indices = []\n    for y in range(num_y_bins):\n        for x in range(num_x_bins):\n            if used_mask[y, x] > 0.0:\n                image = np.uint8(image_bins[y][x].squeeze() * 255)\n                image = np.rollaxis(image, 0, 3)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                axs[num_y_bins - 1 - y][x].imshow(image)\n                images.append(image)\n                bin_indices.append((y, x))\n            axs[y, x].axis(\"off\")\n    if plot:\n        plt.axis(\"off\")\n        plt.show()\n    else:\n        return images, bin_indices\n\n\ndef generate_disentanglement(model_path=\"saved_models/model_dim2.pth\"):\n    \"\"\"Generates disentanglement visualization and serializes it\"\"\"\n    # Disentanglement object\n    disentanglement_object = {}\n    # Make Disentanglement\n    images, bin_indices = binned_images(model_path)\n    disentanglement_object[\"images\"] = images\n    disentanglement_object[\"bin_indices\"] = bin_indices\n    # Serialize Images\n    with open(\"disentanglement.pkl\", \"wb\") as f:\n        pickle.dump(disentanglement_object, f)\n\n\nif __name__ == \"__main__\":\n    plot = False\n    if plot:\n        model_path = \"saved_models/model_dim2.pth\"\n        # uniform_image_sample(model_path)\n        binned_images(model_path)\n    else:\n        generate_disentanglement()\n", "original_prompt": "Variational Autoencoder Autoencoder Models Generate Disentanglement"}
{"prompt": "Create a side-by-side visualization of original and reconstructed MNIST images.", "response": "import torch\nfrom variational_autoencoder import VAE\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport numpy as np\nimport pickle\n\n# Load model\nvae = VAE(latent_dim=16)\nvae.load_state_dict(torch.load(\"saved_models/model.pth\"))\n# Transforms images to a PyTorch Tensor\ntensor_transform = transforms.ToTensor()\n# Download the MNIST Dataset\ndataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=tensor_transform\n)\n# Generate reconstructions\nnum_recons = 10\nfig, axs = plt.subplots(num_recons, 2, figsize=(2, num_recons))\nimage_pairs = []\nfor i in range(num_recons):\n    base_image, _ = dataset[i]\n    base_image = base_image.reshape(-1, 28 * 28)\n    _, _, recon_image, _ = vae.forward(base_image)\n    base_image = base_image.detach().numpy()\n    base_image = np.reshape(base_image, (28, 28)) * 255\n    recon_image = recon_image.detach().numpy()\n    recon_image = np.reshape(recon_image, (28, 28)) * 255\n    # Add to plot\n    axs[i][0].imshow(base_image)\n    axs[i][1].imshow(recon_image)\n    # image pairs\n    image_pairs.append((base_image, recon_image))\n\nwith open(\"image_pairs.pkl\", \"wb\") as f:\n    pickle.dump(image_pairs, f)\n\nplt.show()\n", "original_prompt": "Variational Autoencoder Autoencoder Models Generate Images"}
{"prompt": "Demonstrate a sequence of images generated by interpolating between two input images using a VAE model.", "response": "import torch\nfrom variational_autoencoder import VAE, load_dataset\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport numpy as np\nimport pickle\n\n# Load model\nvae = VAE(latent_dim=16)\nvae.load_state_dict(torch.load(\"saved_models/model.pth\"))\ndataset = load_dataset()\n# Generate reconstructions\nnum_images = 50\nimage_pairs = []\nsave_object = {\"interpolation_path\": [], \"interpolation_images\": []}\n\n# Make interpolation path\nimage_a, image_b = dataset[0][0], dataset[1][0]\nimage_a = image_a.view(32 * 32)\nimage_b = image_b.view(32 * 32)\nz_a, _, _, _ = vae.forward(image_a)\nz_a = z_a.detach().cpu().numpy()\nz_b, _, _, _ = vae.forward(image_b)\nz_b = z_b.detach().cpu().numpy()\ninterpolation_path = np.linspace(z_a, z_b, num=num_images)\n# interpolation_path[:, 4] = np.linspace(-3, 3, num=num_images)\nsave_object[\"interpolation_path\"] = interpolation_path\n\nfor i in range(num_images):\n    # Generate\n    z = torch.Tensor(interpolation_path[i]).unsqueeze(0)\n    gen_image = vae.decode(z).detach().numpy()\n    gen_image = np.reshape(gen_image, (32, 32)) * 255\n    save_object[\"interpolation_images\"].append(gen_image)\n\nfig, axs = plt.subplots(num_images, 1, figsize=(1, num_images))\nimage_pairs = []\nfor i in range(num_images):\n    recon_image = save_object[\"interpolation_images\"][i]\n    # Add to plot\n    axs[i].imshow(recon_image)\n\n# Perform intrpolations\nwith open(\"interpolations.pkl\", \"wb\") as f:\n    pickle.dump(save_object, f)\n\nplt.show()\n", "original_prompt": "Variational Autoencoder Autoencoder Models Generate Interpolation"}
{"prompt": "Based on the provided code, here is a high-quality, readable, and well-documented version of the code with explanations:\n\n**Variational Autoencoder (VAE) Implementation**\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport os\n\n# Define the VAE model\nclass VAE(nn.Module):\n    def __init__(self, latent_dim, input_dim, hidden_dim):\n        super(VAE, self).__init__()\n        self.latent_dim = latent_dim\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        # Encoder (Bottleneck layer)\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Decoder (Bottleneck layer)\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.ConvTranspose2d(hidden_dim, 1, kernel_size=4, stride=2, padding=1),\n            nn.Sigmoid()\n        )\n\n    def encode(self, x):\n        # Get the mean and log variance of the latent space\n        z = self.encoder(x.view(-1, self.input_dim))\n        return z[:, :self.latent_dim], z[:, self.latent_dim:]\n\n    def decode(self, z):\n        # Generate an image from the latent space\n        reconstructed = self.decoder(z)\n        return reconstructed\n\n    def forward(self, x):\n        mean, log_var = self.encode(x)\n        eps = torch.randn_like(mean)\n        z = mean + torch.exp(log_var / 2) * eps\n        reconstructed = self.decode(z)\n        return mean, log_var, reconstructed, x\n\n# Define the loss function\ndef loss_function(mean, log_var, reconstructed, original, kl_beta=0.0001):\n    # Calculate the Kullback-Leibler (KL) divergence term\n    kl = torch.mean(-0.5 * torch.sum(1 + log_var - mean**2 - log_var.exp(), dim=1), dim=0)\n    # Calculate the Reconstruction loss term\n    recon = torch.nn.functional.mse_loss(reconstructed, original)\n    return kl_beta * kl + recon\n\n# Train the VAE model\ndef train_model(latent_dim, input_dim, hidden_dim, epochs, plot=True):\n    # Load the MNIST dataset\n    transform = transforms.Compose([transforms.ToTensor()])\n    dataset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n\n    # Initialize the VAE model and optimizer\n    model = VAE(latent_dim=latent_dim, input_dim=input_dim, hidden_dim=hidden_dim)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)\n\n    outputs = []\n    losses = []\n    for epoch in range(epochs):\n        for (image, _) in loader:\n            # Forward pass\n            mean, log_var, reconstructed, _ = model(image)\n            loss = loss_function(mean, log_var, reconstructed, image)\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # Store the losses and outputs\n            if torch.isnan(loss):\n                raise Exception()\n            losses.append(loss.detach().cpu())\n            outputs.append((epoch, image, reconstructed))\n\n        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n\n    # Save the trained model\n    torch.save(model.state_dict(), os.path.join(os.environ[\"PROJECT_ROOT\"], f\"examples/variational_autoencoder/autoencoder_model/saved_models/model_dim{latent_dim}.pth\"))\n\n    if plot:\n        plt.style.use(\"fivethirtyeight\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        # Plot the last 100 values\n        plt.plot(losses)\n        plt.show()\n\n# Run the training script\nif __name__ == \"__main__\":\n    latent_dim = 2\n    input_dim = 784\n    hidden_dim = 128\n    epochs = 40\n    train_model(latent_dim=latent_dim, input_dim=input_dim, hidden_dim=hidden_dim, epochs=epochs)\n```\n\n**Explanation**\n\nThis code implements a Variational Autoencoder (VAE) in PyTorch. The VAE is a type of deep learning model that can be used for unsupervised learning and dimensionality reduction.\n\nThe `VAE` class defines the VAE architecture, including the encoder and decoder networks. The `encode` method computes the mean and log variance of the latent space, while the `decode` method generates an image from the latent space. The `forward` method defines the forward pass through the network.\n\nThe `loss_function` function defines the loss function used to train the VAE. It calculates the Kullback-Leibler (KL) divergence term and the Reconstruction loss term.\n\nThe `train_model` function trains the VAE model using stochastic gradient descent (SGD). It loads the MNIST dataset, initializes the VAE model and optimizer, and trains the network for a specified number of epochs.\n\nFinally, the code saves the trained model to disk using PyTorch's `torch.save` method. If the `plot` argument is set to `True`, it plots the loss values over time using Matplotlib.\n\n**Note**\n\nThis code assumes that you have PyTorch and NumPy installed on your system. You can install them using pip: `pip install torch numpy matplotlib`.", "response": "import torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport math\n\n\"\"\"\n    These are utility functions that help to calculate the input and output\n    sizes of convolutional neural networks\n\"\"\"\n\n\ndef num2tuple(num):\n    return num if isinstance(num, tuple) else (num, num)\n\n\ndef conv2d_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    h_w, kernel_size, stride, pad, dilation = (\n        num2tuple(h_w),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(pad),\n        num2tuple(dilation),\n    )\n    pad = num2tuple(pad[0]), num2tuple(pad[1])\n\n    h = math.floor(\n        (h_w[0] + sum(pad[0]) - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1\n    )\n    w = math.floor(\n        (h_w[1] + sum(pad[1]) - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1\n    )\n\n    return h, w\n\n\ndef convtransp2d_output_shape(\n    h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0\n):\n    h_w, kernel_size, stride, pad, dilation, out_pad = (\n        num2tuple(h_w),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(pad),\n        num2tuple(dilation),\n        num2tuple(out_pad),\n    )\n    pad = num2tuple(pad[0]), num2tuple(pad[1])\n\n    h = (\n        (h_w[0] - 1) * stride[0]\n        - sum(pad[0])\n        + dialation[0] * (kernel_size[0] - 1)\n        + out_pad[0]\n        + 1\n    )\n    w = (\n        (h_w[1] - 1) * stride[1]\n        - sum(pad[1])\n        + dialation[1] * (kernel_size[1] - 1)\n        + out_pad[1]\n        + 1\n    )\n\n    return h, w\n\n\ndef conv2d_get_padding(h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1):\n    h_w_in, h_w_out, kernel_size, stride, dilation = (\n        num2tuple(h_w_in),\n        num2tuple(h_w_out),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(dilation),\n    )\n\n    p_h = (\n        (h_w_out[0] - 1) * stride[0]\n        - h_w_in[0]\n        + dilation[0] * (kernel_size[0] - 1)\n        + 1\n    )\n    p_w = (\n        (h_w_out[1] - 1) * stride[1]\n        - h_w_in[1]\n        + dilation[1] * (kernel_size[1] - 1)\n        + 1\n    )\n\n    return (math.floor(p_h / 2), math.ceil(p_h / 2)), (\n        math.floor(p_w / 2),\n        math.ceil(p_w / 2),\n    )\n\n\ndef convtransp2d_get_padding(\n    h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1, out_pad=0\n):\n    h_w_in, h_w_out, kernel_size, stride, dilation, out_pad = (\n        num2tuple(h_w_in),\n        num2tuple(h_w_out),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(dilation),\n        num2tuple(out_pad),\n    )\n\n    p_h = (\n        -(\n            h_w_out[0]\n            - 1\n            - out_pad[0]\n            - dilation[0] * (kernel_size[0] - 1)\n            - (h_w_in[0] - 1) * stride[0]\n        )\n        / 2\n    )\n    p_w = (\n        -(\n            h_w_out[1]\n            - 1\n            - out_pad[1]\n            - dilation[1] * (kernel_size[1] - 1)\n            - (h_w_in[1] - 1) * stride[1]\n        )\n        / 2\n    )\n\n    return (math.floor(p_h / 2), math.ceil(p_h / 2)), (\n        math.floor(p_w / 2),\n        math.ceil(p_w / 2),\n    )\n\n\ndef load_dataset(train=True, digit=None):\n    # Transforms images to a PyTorch Tensor\n    tensor_transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()])\n\n    # Download the MNIST Dataset\n    dataset = datasets.MNIST(\n        root=\"./data\", train=train, download=True, transform=tensor_transform\n    )\n    # Load specific image\n    if not digit is None:\n        idx = dataset.train_labels == digit\n        dataset.targets = dataset.targets[idx]\n        dataset.data = dataset.data[idx]\n\n    return dataset\n\n\ndef load_vae_from_path(path, latent_dim):\n    model = VAE(latent_dim)\n    model.load_state_dict(torch.load(path))\n\n    return model\n\n\n# Creating a PyTorch class\n# 28*28 ==> 9 ==> 28*28\nclass VAE(torch.nn.Module):\n    def __init__(self, latent_dim=5, layer_count=4, channels=1):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.in_shape = 32\n        self.layer_count = layer_count\n        self.channels = channels\n        self.d = 128\n        mul = 1\n        inputs = self.channels\n        out_sizes = [(self.in_shape, self.in_shape)]\n        for i in range(self.layer_count):\n            setattr(self, \"conv%d\" % (i + 1), nn.Conv2d(inputs, self.d * mul, 4, 2, 1))\n            setattr(self, \"conv%d_bn\" % (i + 1), nn.BatchNorm2d(self.d * mul))\n            h_w = (out_sizes[-1][-1], out_sizes[-1][-1])\n            out_sizes.append(\n                conv2d_output_shape(h_w, kernel_size=4, stride=2, pad=1, dilation=1)\n            )\n            inputs = self.d * mul\n            mul *= 2\n\n        self.d_max = inputs\n        self.last_size = out_sizes[-1][-1]\n        self.num_linear = self.last_size**2 * self.d_max\n        # Encoder linear layers\n        self.encoder_mean_linear = nn.Linear(self.num_linear, self.latent_dim)\n        self.encoder_logvar_linear = nn.Linear(self.num_linear, self.latent_dim)\n        # Decoder linear layer\n        self.decoder_linear = nn.Linear(self.latent_dim, self.num_linear)\n\n        mul = inputs // self.d // 2\n\n        for i in range(1, self.layer_count):\n            setattr(\n                self,\n                \"deconv%d\" % (i + 1),\n                nn.ConvTranspose2d(inputs, self.d * mul, 4, 2, 1),\n            )\n            setattr(self, \"deconv%d_bn\" % (i + 1), nn.BatchNorm2d(self.d * mul))\n            inputs = self.d * mul\n            mul //= 2\n\n        setattr(\n            self,\n            \"deconv%d\" % (self.layer_count + 1),\n            nn.ConvTranspose2d(inputs, self.channels, 4, 2, 1),\n        )\n\n    def encode(self, x):\n        if len(x.shape) < 3:\n            x = x.unsqueeze(0)\n        if len(x.shape) < 4:\n            x = x.unsqueeze(1)\n        batch_size = x.shape[0]\n\n        for i in range(self.layer_count):\n            x = F.relu(\n                getattr(self, \"conv%d_bn\" % (i + 1))(\n                    getattr(self, \"conv%d\" % (i + 1))(x)\n                )\n            )\n\n        x = x.view(batch_size, -1)\n\n        mean = self.encoder_mean_linear(x)\n        logvar = self.encoder_logvar_linear(x)\n\n        return mean, logvar\n\n    def decode(self, x):\n        x = x.view(x.shape[0], self.latent_dim)\n        x = self.decoder_linear(x)\n        x = x.view(x.shape[0], self.d_max, self.last_size, self.last_size)\n        # x = self.deconv1_bn(x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(1, self.layer_count):\n            x = F.leaky_relu(\n                getattr(self, \"deconv%d_bn\" % (i + 1))(\n                    getattr(self, \"deconv%d\" % (i + 1))(x)\n                ),\n                0.2,\n            )\n        x = getattr(self, \"deconv%d\" % (self.layer_count + 1))(x)\n        x = torch.sigmoid(x)\n        return x\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        mean, logvar = self.encode(x)\n        eps = torch.randn(batch_size, self.latent_dim)\n        z = mean + torch.exp(logvar / 2) * eps\n        reconstructed = self.decode(z)\n        return mean, logvar, reconstructed, x\n\n\ndef train_model(latent_dim=16, plot=True, digit=1, epochs=200):\n    dataset = load_dataset(train=True, digit=digit)\n    # DataLoader is used to load the dataset\n    # for training\n    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n    # Model Initialization\n    model = VAE(latent_dim=latent_dim)\n    # Validation using MSE Loss function\n    def loss_function(mean, log_var, reconstructed, original, kl_beta=0.0001):\n        kl = torch.mean(\n            -0.5 * torch.sum(1 + log_var - mean**2 - log_var.exp(), dim=1), dim=0\n        )\n        recon = torch.nn.functional.mse_loss(reconstructed, original)\n        # print(f\"KL Error {kl}, Recon Error {recon}\")\n        return kl_beta * kl + recon\n\n    # Using an Adam Optimizer with lr = 0.1\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0e-8)\n\n    outputs = []\n    losses = []\n    for epoch in tqdm(range(epochs)):\n        for (image, _) in loader:\n            # Output of Autoencoder\n            mean, log_var, reconstructed, image = model(image)\n            # Calculating the loss function\n            loss = loss_function(mean, log_var, reconstructed, image)\n            # The gradients are set to zero,\n            # the the gradient is computed and stored.\n            # .step() performs parameter update\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # Storing the losses in a list for plotting\n            if torch.isnan(loss):\n                raise Exception()\n            losses.append(loss.detach().cpu())\n            outputs.append((epochs, image, reconstructed))\n\n    torch.save(\n        model.state_dict(),\n        os.path.join(\n            os.environ[\"PROJECT_ROOT\"],\n            f\"examples/variational_autoencoder/autoencoder_model/saved_models/model_dim{latent_dim}.pth\",\n        ),\n    )\n\n    if plot:\n        # Defining the Plot Style\n        plt.style.use(\"fivethirtyeight\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        # Plotting the last 100 values\n        plt.plot(losses)\n        plt.show()\n\n\nif __name__ == \"__main__\":\n    train_model(latent_dim=2, digit=2, epochs=40)\n", "original_prompt": "Variational Autoencoder Autoencoder Models Variational Autoencoder"}
{"prompt": "Demonstrate a Variational Autoencoder (VAE) neural network animation with images and layers transformations.", "response": "\"\"\"Autoencoder Manim Visualizations\n\nIn this module I define Manim visualizations for Variational Autoencoders\nand Traditional Autoencoders.\n\n\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\nimport numpy as np\nfrom PIL import Image\nfrom manim_ml.neural_network.layers import EmbeddingLayer\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.layers import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass VAEScene(Scene):\n    \"\"\"Scene object for a Variational Autoencoder and Autoencoder\"\"\"\n\n    def construct(self):\n        numpy_image = np.asarray(Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\"))\n        vae = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.4),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(dist_theme=\"ellipse\"),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.4),\n            ]\n        )\n\n        self.play(Create(vae))\n        self.play(vae.make_forward_pass_animation(run_time=15))\n", "original_prompt": "Variational Autoencoder Variational Autoencoder"}


