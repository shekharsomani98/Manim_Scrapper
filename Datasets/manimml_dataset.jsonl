{"prompt": "Basic Neural Network Basic Neural Network", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass NeuralNetworkScene(Scene):\n    \"\"\"Test Scene for the Neural Network\"\"\"\n\n    def construct(self):\n        # Make the Layer object\n        layers = [FeedForwardLayer(3), FeedForwardLayer(5), FeedForwardLayer(3)]\n        nn = NeuralNetwork(layers)\n        nn.scale(2)\n        nn.move_to(ORIGIN)\n        # Make Animation\n        self.add(nn)\n        # self.play(Create(nn))\n        forward_propagation_animation = nn.make_forward_pass_animation(\n            run_time=5, passing_flash=True\n        )\n\n        self.play(forward_propagation_animation)\n", "file_path": "examples/basic_neural_network/basic_neural_network.py"}
{"prompt": "Basic Neural Network Basic Neural Network", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass NeuralNetworkScene(Scene):\n    \"\"\"Test Scene for the Neural Network\"\"\"\n\n    def construct(self):\n        # Make the Layer object\n        layers = [FeedForwardLayer(3), FeedForwardLayer(5), FeedForwardLayer(3)]\n        nn = NeuralNetwork(layers)\n        nn.scale(2)\n        nn.move_to(ORIGIN)\n        # Make Animation\n        self.add(nn)\n        # self.play(Create(nn))\n        forward_propagation_animation = nn.make_forward_pass_animation(\n            run_time=5, passing_flash=True\n        )\n\n        self.play(forward_propagation_animation)\n", "file_path": "examples/basic_neural_network/basic_neural_network.py"}
{"prompt": "Basic Neural Network Residual Block", "response": "from manim import *\n\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.math_operation_layer import MathOperationLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        nn = NeuralNetwork({\n            \"feed_forward_1\": FeedForwardLayer(3),\n            \"feed_forward_2\": FeedForwardLayer(3, activation_function=\"ReLU\"),\n            \"feed_forward_3\": FeedForwardLayer(3),\n            \"sum_operation\": MathOperationLayer(\"+\", activation_function=\"ReLU\"),\n        })\n        nn.add_connection(\"feed_forward_1\", \"sum_operation\")\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Add the network\n        nn = NeuralNetwork({\n                \"feed_forward_1\": FeedForwardLayer(3),\n                \"feed_forward_2\": FeedForwardLayer(3, activation_function=\"ReLU\"),\n                \"feed_forward_3\": FeedForwardLayer(3),\n                \"sum_operation\": MathOperationLayer(\"+\", activation_function=\"ReLU\"),\n            },\n            layer_spacing=0.38\n        )\n        # Make connections\n        input_blank_dot = Dot(\n            nn.input_layers_dict[\"feed_forward_1\"].get_left() - np.array([0.65, 0.0, 0.0])\n        )\n        nn.add_connection(input_blank_dot, \"feed_forward_1\", arc_direction=\"straight\")\n        nn.add_connection(\"feed_forward_1\", \"sum_operation\")\n        output_blank_dot = Dot(\n            nn.input_layers_dict[\"sum_operation\"].get_right() + np.array([0.65, 0.0, 0.0])\n        )\n        nn.add_connection(\"sum_operation\", output_blank_dot, arc_direction=\"straight\")\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        # Group it all\n        group = Group(nn, code)\n        group.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)", "file_path": "examples/basic_neural_network/residual_block.py"}
{"prompt": "Cnn Activation Functions", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork([\n            # ... Layers at start\n            Convolutional2DLayer(3, 5, 3, activation_function=\"ReLU\"),\n            # ... Layers at end\n        ])\n        # Play the animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        font=\"Monospace\",\n        background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.45)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7),\n                Convolutional2DLayer(3, 5, 3, activation_function=\"ReLU\"),\n                Convolutional2DLayer(5, 3, 3, activation_function=\"ReLU\"),\n                FeedForwardLayer(3),\n                FeedForwardLayer(1),\n            ],\n            layer_spacing=0.25,\n        )\n        # nn.scale(0.7)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # nn.move_to(ORIGIN)\n        # Move everything up\n        # Group(nn, code).move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "file_path": "examples/cnn/activation_functions.py"}
{"prompt": "Cnn Cnn", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image, height=1.5),\n            Convolutional2DLayer(1, 7, 3),\n            Convolutional2DLayer(3, 5, 3),\n            Convolutional2DLayer(5, 3, 1),\n            FeedForwardLayer(3),\n            FeedForwardLayer(3),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.50)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # Group it all\n        # group = Group(nn, code)\n        # group.move_to(ORIGIN)\n        nn.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "file_path": "examples/cnn/cnn.py"}
{"prompt": "Cnn Cnn Max Pool", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork([\n            ImageLayer(image),\n            Convolutional2DLayer(1, 8),\n            MaxPooling2DLayer(kernel_size=2),\n            Convolutional2DLayer(3, 2, 3),\n        ])\n        # Play the animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        font=\"Monospace\",\n        background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.4)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 8, filter_spacing=0.32),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(3, 2, 3, filter_spacing=0.32),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        Group(code, nn).move_to(ORIGIN)\n        self.add(code)\n        self.wait(5)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "file_path": "examples/cnn/cnn_max_pool.py"}
{"prompt": "Cnn One By One Convolution", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image, height=1.5),\n            Convolutional2DLayer(1, 5, 5, 1, 1),\n            Convolutional2DLayer(4, 5, 5, 1, 1),\n            Convolutional2DLayer(2, 5, 5),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.50)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 5, 1, filter_spacing=0.32),\n                Convolutional2DLayer(4, 5, 1, filter_spacing=0.32),\n                Convolutional2DLayer(2, 5, 5, filter_spacing=0.32),\n            ],\n            layer_spacing=0.4,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        # Group it all\n        group = Group(nn, code)\n        group.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "file_path": "examples/cnn/one_by_one_convolution.py"}
{"prompt": "Cnn Padding Example", "response": "from manim import *\nfrom manim_ml.neural_network.layers.image import ImageLayer\nimport numpy as np\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image),\n            Convolutional2DLayer(1, 6, 1, padding=1),\n            Convolutional2DLayer(3, 6, 3),\n            FeedForwardLayer(3),\n            FeedForwardLayer(1),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(\n                    num_feature_maps=1,\n                    feature_map_size=6,\n                    padding=1,\n                    padding_dashed=True,\n                ),\n                Convolutional2DLayer(\n                    num_feature_maps=3,\n                    feature_map_size=6,\n                    filter_size=3,\n                    padding=0,\n                    padding_dashed=False,\n                ),\n                FeedForwardLayer(3),\n                FeedForwardLayer(1),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        self.add(nn)\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        Group(code, nn).move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass, run_time=20)\n", "file_path": "examples/cnn/padding_example.py"}
{"prompt": "Cnn Resnet Block", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\nfrom manim_ml.neural_network import Convolutional2DLayer, NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork({\n            \"layer1\": Convolutional2DLayer(1, 5, padding=1),\n            \"layer2\": Convolutional2DLayer(1, 5, 3, padding=1),\n            \"layer3\": Convolutional2DLayer(1, 5, 3, padding=1)\n        })\n        # Add the residual connection\n        nn.add_connection(\"layer1\", \"layer3\")\n        # Make the animation\n        self.play(nn.make_forward_pass_animation())\n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass ConvScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n\n        nn = NeuralNetwork(\n            {\n                \"layer1\": Convolutional2DLayer(1, 5, padding=1),\n                \"layer2\": Convolutional2DLayer(1, 5, 3, padding=1),\n                \"layer3\": Convolutional2DLayer(1, 5, 3, padding=1),\n            },\n            layer_spacing=0.25,\n        )\n\n        nn.add_connection(\"layer1\", \"layer3\")\n\n        self.add(nn)\n\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        Group(code, nn).move_to(ORIGIN)\n\n        self.play(nn.make_forward_pass_animation(), run_time=8)\n", "file_path": "examples/cnn/resnet_block.py"}
{"prompt": "Code Snippet Image Nn Code Snippet", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass ImageNeuralNetworkScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make image object\n            image = Image.open('images/image.jpeg')\n            numpy_image = np.asarray(image)\n            # Make Neural Network\n            layers = [\n                ImageLayer(numpy_image, height=1.4),\n                FeedForwardLayer(3), \n                FeedForwardLayer(5),\n                FeedForwardLayer(3)\n            ]\n            nn = NeuralNetwork(layers)\n            self.add(nn)\n            # Play animation\n            self.play(\n                nn.make_forward_pass_animation()\n            )\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            background_stroke_color=WHITE,\n            insert_line_no=False,\n            style=\"monokai\",\n            # background=\"window\",\n            language=\"py\",\n        )\n        code.scale(0.2)\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        layers = [\n            ImageLayer(numpy_image, height=1.4),\n            FeedForwardLayer(3),\n            FeedForwardLayer(5),\n            FeedForwardLayer(3),\n            FeedForwardLayer(6),\n        ]\n        nn = NeuralNetwork(layers)\n        nn.scale(0.9)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.rotate(-PI / 2)\n        nn.layers[0].image_mobject.rotate(PI / 2)\n        nn.layers[0].image_mobject.shift([0, -0.4, 0])\n        nn.shift([1.5, 0.3, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(1.9)\n        code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(run_time=10))\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Feed Forward Neural Network\n    ffnn_scene = FeedForwardNeuralNetworkScene()\n    ffnn_scene.render()\n    # Neural Network\n    nn_scene = NeuralNetworkScene()\n    nn_scene.render()\n", "file_path": "examples/code_snippet/image_nn_code_snippet.py"}
{"prompt": "Code Snippet Vae Code Landscape", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer, EmbeddingLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 720\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass VAECodeSnippetScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make Neural Network\n            nn = NeuralNetwork([\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ], layer_spacing=0.1)\n            # Play animation\n            self.play(nn.make_forward_pass_animation())\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            # background_stroke_color=WHITE,\n            insert_line_no=False,\n            background=\"window\",\n            # font=\"Monospace\",\n            style=\"one-dark\",\n            language=\"py\",\n        )\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\", point_radius=0.04).scale(\n            1.0\n        )\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ],\n            layer_spacing=0.1,\n        )\n\n        nn.scale(1.1)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        # nn.rotate(-PI/2)\n        # nn.all_layers[0].image_mobject.rotate(PI/2)\n        # nn.all_layers[0].image_mobject.shift([0, -0.4, 0])\n        # nn.all_layers[-1].image_mobject.rotate(PI/2)\n        # nn.all_layers[-1].image_mobject.shift([0, -0.4, 0])\n        nn.shift([0, -1.4, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(0.52)\n        code_snippet.shift([0, 1.25, 0])\n        # code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(), run_time=10)\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Neural Network\n    nn_scene = VAECodeSnippetScene()\n    nn_scene.render()\n", "file_path": "examples/code_snippet/vae_code_landscape.py"}
{"prompt": "Code Snippet Vae Nn Code Snippet", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer, EmbeddingLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass VAECodeSnippetScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make image object\n            image = Image.open('images/image.jpeg')\n            numpy_image = np.asarray(image)\n            # Make Neural Network\n            nn = NeuralNetwork([\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ], layer_spacing=0.1)\n            self.add(nn)\n            # Play animation\n            self.play(\n                nn.make_forward_pass_animation()\n            )\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            # background_stroke_color=WHITE,\n            insert_line_no=False,\n            background=\"window\",\n            # font=\"Monospace\",\n            style=\"one-dark\",\n            language=\"py\",\n        )\n        code.scale(0.2)\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\", point_radius=0.04).scale(\n            1.0\n        )\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.0),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.0),\n            ],\n            layer_spacing=0.1,\n        )\n\n        nn.scale(0.65)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.rotate(-PI / 2)\n        nn.all_layers[0].image_mobject.rotate(PI / 2)\n        # nn.all_layers[0].image_mobject.shift([0, -0.4, 0])\n        nn.all_layers[-1].image_mobject.rotate(PI / 2)\n        # nn.all_layers[-1].image_mobject.shift([0, -0.4, 0])\n        nn.shift([1.5, 0.0, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(1.9)\n        code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(), run_time=10)\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Neural Network\n    nn_scene = VAECodeSnippetScene()\n    nn_scene.render()\n", "file_path": "examples/code_snippet/vae_nn_code_snippet.py"}
{"prompt": "Cross Attention Vis Cross Attention Vis", "response": "\"\"\"\n    Here I thought it would be interesting to visualize the cross attention\n    maps produced by the UNet of a text-to-image diffusion model. \n\n    The key thing I want to show is how images and text are broken up into tokens (patches for images),\n    and those tokens are used to compute a cross attention score, which can be interpreted\n    as a 2D heatmap over the image patches for each text token. \n    \n\n    Necessary operations:\n        1. [X] Split an image into patches and \"expand\" the patches outward to highlight that they are \n            separate. \n        2. Split text into tokens using the tokenizer and display them. \n        3. Compute the cross attention scores (using DAAM) for each word/token and display them as a heatmap.\n        4. Overlay the heatmap over the image patches. Show the overlaying as a transition animation. \n\"\"\"\nimport torch\nimport cv2\nfrom manim import *\nimport numpy as np\nfrom typing import List\nfrom daam import trace, set_seed\nfrom diffusers import StableDiffusionPipeline\nimport torchvision\nimport matplotlib.pyplot as plt\n\nclass ImagePatches(Group):\n    \"\"\"Container object for a grid of ImageMobjects.\"\"\"\n\n    def __init__(self, image: ImageMobject, grid_width=4):\n        \"\"\"\n        Parameters\n        ----------\n        image : ImageMobject\n            image to split into patches\n        grid_width : int, optional\n            number of patches per row, by default 4\n        \"\"\"\n        self.image = image\n        self.grid_width = grid_width\n        super(Group, self).__init__()\n        self.patch_dict = self._split_image_into_patches(image, grid_width)\n\n    def _split_image_into_patches(self, image, grid_width):\n        \"\"\"Splits the images into a set of patches\n\n        Parameters\n        ----------\n        image : ImageMobject\n            image to split into patches\n        grid_width : int\n            number of patches per row\n        \"\"\"\n        patch_dict = {}\n        # Get a pixel array of the image mobject\n        pixel_array = image.pixel_array\n        # Get the height and width of the image\n        height, width = pixel_array.shape[:2]\n        # Split the image into an equal width grid of patches\n        assert height == width, \"Image must be square\"\n        assert height % grid_width == 0, \"Image width must be divisible by grid_width\"\n        patch_width, patch_height = width // grid_width, height // grid_width\n\n        for patch_i in range(self.grid_width):\n            for patch_j in range(self.grid_width):\n                # Get patch pixels\n                i_start, i_end = patch_i * patch_width, (patch_i + 1) * patch_width\n                j_start, j_end = patch_j * patch_height, (patch_j + 1) * patch_height\n                patch_pixels = pixel_array[i_start:i_end, j_start:j_end, :]\n                # Make the patch\n                image_patch = ImageMobject(patch_pixels)\n                # Move the patch to the correct location on the old image\n                image_width = image.get_width()\n                image_center = image.get_center()\n                image_patch.scale(image_width / grid_width / image_patch.get_width())\n                patch_manim_space_width = image_patch.get_width()\n\n                patch_center = image_center\n                patch_center += (patch_j - self.grid_width / 2 + 0.5) * patch_manim_space_width * RIGHT\n                # patch_center = image_center - (patch_i - self.grid_width / 2 + 0.5) * patch_manim_space_width * RIGHT\n                patch_center -= (patch_i - self.grid_width / 2 + 0.5) * patch_manim_space_width * UP\n                # + (patch_j - self.grid_width / 2) * patch_height / 2 * UP\n\n                image_patch.move_to(patch_center)\n\n                self.add(image_patch)\n                patch_dict[(patch_i, patch_j)] = image_patch\n\n        return patch_dict\n\nclass ExpandPatches(Animation):\n\n    def __init__(self, image_patches: ImagePatches, expansion_scale=2.0):\n        \"\"\"\n        Parameters\n        ----------\n        image_patches : ImagePatches\n            set of image patches\n        expansion_scale : float, optional\n            scale factor for expansion, by default 2.0\n        \"\"\"\n        self.image_patches = image_patches\n        self.expansion_scale = expansion_scale\n        super().__init__(image_patches)\n\n    def interpolate_submobject(self, submobject, starting_submobject, alpha):\n        \"\"\"\n        Parameters\n        ----------\n        submobject : ImageMobject\n            current patch\n        starting_submobject : ImageMobject\n            starting patch\n        alpha : float\n            interpolation alpha\n        \"\"\"\n        patch = submobject\n        starting_patch_center = starting_submobject.get_center()\n        image_center = self.image_patches.image.get_center()\n        # Start image vector\n        starting_patch_vector = starting_patch_center - image_center\n        final_image_vector = image_center + starting_patch_vector * self.expansion_scale\n        # Interpolate vectors\n        current_location = alpha * final_image_vector + (1 - alpha) * starting_patch_center\n        # # Need to compute the direction of expansion as the unit vector from the original image center\n        # # and patch center. \n        patch.move_to(current_location)\n\nclass TokenizedText(Group):\n    \"\"\"Tokenizes the given text and displays the tokens as a list of Text Mobjects.\"\"\"\n\n    def __init__(self, text, tokenizer=None):\n        self.text = text\n        if not tokenizer is None:\n            self.tokenizer = tokenizer\n        else:\n            # TODO load default stable diffusion tokenizer here\n            raise NotImplementedError(\"Tokenizer must be provided\")\n\n        self.token_strings = self._tokenize_text(text)\n        self.token_mobjects = self.make_text_mobjects(self.token_strings)\n\n        super(Group, self).__init__()\n        self.add(*self.token_mobjects)\n        self.arrange(RIGHT, buff=-0.05, aligned_edge=UP)\n\n        token_dict = {} \n        for token_index, token_string in enumerate(self.token_strings):\n            token_dict[token_string] = self.token_mobjects[token_index]\n\n        self.token_dict = token_dict\n\n    def _tokenize_text(self, text):\n        \"\"\"Tokenize the text using the tokenizer.\n\n        Parameters\n        ----------\n        text : str\n            text to tokenize\n        \"\"\"\n        tokens = self.tokenizer.tokenize(text)\n        tokens = [token.replace(\"</w>\", \"\") for token in tokens]\n        return tokens\n\n    def make_text_mobjects(self, tokens_list: List[str]):\n        \"\"\"Converts the tokens into a list of TextMobjects.\"\"\"\n        # Make the tokens\n        # Insert phantom\n        tokens_list = [\"l\" + token + \"g\" for token in tokens_list]\n        token_objects = [\n            Text(token, t2c={'[-1:]': '#000000', '[0:1]': \"#000000\"}, font_size=64) \n            for token in tokens_list\n        ]\n\n        return token_objects\n\ndef compute_stable_diffusion_cross_attention_heatmaps(\n    pipe,\n    prompt: str,\n    seed: int = 2,\n    map_resolution=(32, 32)\n):\n    \"\"\"Computes the cross attention heatmaps for the given prompt.\n\n    Parameters\n    ----------\n    prompt : str\n        the prompt\n    seed : int, optional\n        random seed, by default 0\n    map_resolution : tuple, optional\n        resolution to downscale maps to, by default (16, 16)\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    # Get tokens\n    tokens = pipe.tokenizer.tokenize(prompt)\n    tokens = [token.replace(\"</w>\", \"\") for token in tokens]\n    # Set torch seeds\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    gen = set_seed(seed)  # for reproducibility\n\n    heatmap_dict = {}\n    image = None\n\n    with torch.cuda.amp.autocast(dtype=torch.float16), torch.no_grad():\n        with trace(pipe) as tc:\n            out = pipe(prompt, num_inference_steps=30, generator=gen)\n            image = out[0][0]\n            global_heat_map = tc.compute_global_heat_map()\n            for token in tokens: \n                word_heat_map = global_heat_map.compute_word_heat_map(token)\n                heatmap = word_heat_map.heatmap\n\n                # Downscale the heatmap\n                heatmap = heatmap.unsqueeze(0).unsqueeze(0)\n                # Save the heatmap\n                heatmap = torchvision.transforms.Resize(\n                    map_resolution, \n                    interpolation=torchvision.transforms.InterpolationMode.NEAREST\n                )(heatmap)\n                heatmap = heatmap.squeeze(0).squeeze(0)\n                # heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n                # plt.imshow(heatmap)\n                # plt.savefig(f\"{token}.png\")\n                # Convert heatmap to rgb color\n                # normalize\n                heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n                cmap = plt.get_cmap('inferno')\n                heatmap = cmap(heatmap) * 255\n                print(heatmap)\n\n                # print(heatmap)\n                # Make an image mobject for each heatmap\n                print(heatmap.shape)\n                heatmap = ImageMobject(heatmap)\n                heatmap.set_resampling_algorithm(RESAMPLING_ALGORITHMS[\"nearest\"])\n                heatmap_dict[token] = heatmap\n\n    return image, heatmap_dict\n\n# Make the scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 30.0\nconfig.frame_width = 30.0\n\nclass StableDiffusionCrossAttentionScene(Scene):\n\n    def construct(self):\n        # Load the pipeline\n        model_id = 'stabilityai/stable-diffusion-2-base'\n        pipe = StableDiffusionPipeline.from_pretrained(model_id)\n        prompt = \"Astronaut riding a horse on the moon\"\n        # Compute the images and heatmaps\n        image, heatmap_dict = compute_stable_diffusion_cross_attention_heatmaps(pipe, prompt)\n        # 1. Show an image appearing\n        image_mobject = ImageMobject(image)\n        image_mobject.shift(DOWN)\n        image_mobject.scale(0.7)\n        image_mobject.shift(LEFT * 7)\n        self.add(image_mobject)\n        # 1. Show a text prompt and the corresponding generated image\n        paragraph_prompt = '\"Astronaut riding a\\nhorse on the moon\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        text_prompt.next_to(image_mobject, UP, buff=1.5)\n        prompt_title = Text(\"Prompt\", font_size=72)\n        prompt_title.next_to(text_prompt, UP, buff=0.5)\n        self.play(Create(prompt_title))\n        self.play(Create(text_prompt))        \n        # Make an arrow from the text to the image\n        arrow = Arrow(text_prompt.get_bottom(), image_mobject.get_top(), buff=0.5)\n        self.play(GrowArrow(arrow))\n        self.wait(1)\n        self.remove(arrow)\n        # 2. Show the image being split into patches\n        # Make the patches\n        patches = ImagePatches(image_mobject, grid_width=32)\n        # Expand and contract \n        self.remove(image_mobject)\n        self.play(ExpandPatches(patches, expansion_scale=1.2))\n        # Play arrows for visual tokens\n        visual_token_label = Text(\"Visual Tokens\", font_size=72)\n        visual_token_label.next_to(image_mobject, DOWN, buff=1.5)\n        # Draw arrows \n        arrow_animations = []\n        arrows = []\n        for i in range(patches.grid_width):\n            patch = patches.patch_dict[(patches.grid_width - 1, i)]\n            arrow = Line(visual_token_label.get_top(), patch.get_bottom(), buff=0.3)\n            arrow_animations.append(\n                Create(\n                    arrow\n                )\n            )\n            arrows.append(arrow)\n        self.play(AnimationGroup(*arrow_animations, FadeIn(visual_token_label), lag_ratio=0))\n        self.wait(1)\n        self.play(FadeOut(*arrows, visual_token_label))\n        self.play(ExpandPatches(patches, expansion_scale=1/1.2))\n        # 3. Show the text prompt and image being tokenized.\n        tokenized_text = TokenizedText(prompt, pipe.tokenizer)\n        tokenized_text.shift(RIGHT * 7)\n        tokenized_text.shift(DOWN * 7.5)\n        self.play(FadeIn(tokenized_text))\n        # Plot token label\n        token_label = Text(\"Textual Tokens\", font_size=72)\n        token_label.next_to(tokenized_text, DOWN, buff=0.5)\n        arrow_animations = []\n        self.play(Create(token_label)) \n        arrows = []\n        for token_name, token_mobject in tokenized_text.token_dict.items():\n            arrow = Line(token_label.get_top(), token_mobject.get_bottom(), buff=0.3)\n            arrow_animations.append(\n                Create(\n                    arrow\n                )\n            )\n            arrows.append(arrow)\n        self.play(AnimationGroup(*arrow_animations, lag_ratio=0))\n        self.wait(1)\n        self.play(FadeOut(*arrows, token_label))\n        # 4. Show the heatmap corresponding to the cross attention map for each image. \n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"astronaut\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"astronaut\"\n        astronaut_heatmap = heatmap_dict[\"astronaut\"]\n        astronaut_heatmap.height = image_mobject.get_height()\n        astronaut_heatmap.shift(RIGHT * 7)\n        astronaut_heatmap.shift(DOWN)\n        self.play(FadeIn(astronaut_heatmap))\n        self.wait(3)\n        self.remove(surrounding_rectangle)\n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"horse\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"horse\"\n        horse_heatmap = heatmap_dict[\"horse\"]\n        horse_heatmap.height = image_mobject.get_height()\n        horse_heatmap.move_to(astronaut_heatmap)\n        self.play(FadeOut(astronaut_heatmap))\n        self.play(FadeIn(horse_heatmap))\n        self.wait(3)\n        self.remove(surrounding_rectangle)\n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"riding\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"riding\"\n        riding_heatmap = heatmap_dict[\"riding\"]\n        riding_heatmap.height = image_mobject.get_height()\n        riding_heatmap.move_to(horse_heatmap)\n        self.play(FadeOut(horse_heatmap))\n        self.play(FadeIn(riding_heatmap))\n        self.wait(3)\n", "file_path": "examples/cross_attention_vis/cross_attention_vis.py"}
{"prompt": "Decision Tree Decision Tree Surface", "response": "import numpy as np\nfrom collections import deque\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import _tree as ctree\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n\nclass AABB:\n    \"\"\"Axis-aligned bounding box\"\"\"\n\n    def __init__(self, n_features):\n        self.limits = np.array([[-np.inf, np.inf]] * n_features)\n\n    def split(self, f, v):\n        left = AABB(self.limits.shape[0])\n        right = AABB(self.limits.shape[0])\n        left.limits = self.limits.copy()\n        right.limits = self.limits.copy()\n        left.limits[f, 1] = v\n        right.limits[f, 0] = v\n\n        return left, right\n\n\ndef tree_bounds(tree, n_features=None):\n    \"\"\"Compute final decision rule for each node in tree\"\"\"\n    if n_features is None:\n        n_features = np.max(tree.feature) + 1\n    aabbs = [AABB(n_features) for _ in range(tree.node_count)]\n    queue = deque([0])\n    while queue:\n        i = queue.pop()\n        l = tree.children_left[i]\n        r = tree.children_right[i]\n        if l != ctree.TREE_LEAF:\n            aabbs[l], aabbs[r] = aabbs[i].split(tree.feature[i], tree.threshold[i])\n            queue.extend([l, r])\n    return aabbs\n\n\ndef decision_areas(tree_classifier, maxrange, x=0, y=1, n_features=None):\n    \"\"\"Extract decision areas.\n\n    tree_classifier: Instance of a sklearn.tree.DecisionTreeClassifier\n    maxrange: values to insert for [left, right, top, bottom] if the interval is open (+/-inf)\n    x: index of the feature that goes on the x axis\n    y: index of the feature that goes on the y axis\n    n_features: override autodetection of number of features\n    \"\"\"\n    tree = tree_classifier.tree_\n    aabbs = tree_bounds(tree, n_features)\n    maxrange = np.array(maxrange)\n    rectangles = []\n    for i in range(len(aabbs)):\n        if tree.children_left[i] != ctree.TREE_LEAF:\n            continue\n        l = aabbs[i].limits\n        r = [l[x, 0], l[x, 1], l[y, 0], l[y, 1], np.argmax(tree.value[i])]\n        # clip out of bounds indices\n        \"\"\"\n        if r[0] < maxrange[0]:\n            r[0] = maxrange[0]\n        if r[1] > maxrange[1]:\n            r[1] = maxrange[1]\n        if r[2] < maxrange[2]:\n            r[2] = maxrange[2]\n        if r[3] > maxrange[3]:\n            r[3] = maxrange[3]\n        print(r)\n        \"\"\"\n        rectangles.append(r)\n    rectangles = np.array(rectangles)\n    rectangles[:, [0, 2]] = np.maximum(rectangles[:, [0, 2]], maxrange[0::2])\n    rectangles[:, [1, 3]] = np.minimum(rectangles[:, [1, 3]], maxrange[1::2])\n    return rectangles\n\n\ndef plot_areas(rectangles):\n    for rect in rectangles:\n        color = [\"b\", \"r\"][int(rect[4])]\n        print(rect[0], rect[1], rect[2] - rect[0], rect[3] - rect[1])\n        rp = Rectangle(\n            [rect[0], rect[2]],\n            rect[1] - rect[0],\n            rect[3] - rect[2],\n            color=color,\n            alpha=0.3,\n        )\n        plt.gca().add_artist(rp)\n", "file_path": "examples/decision_tree/decision_tree_surface.py"}
{"prompt": "Decision Tree Split Scene", "response": "from sklearn import datasets\nfrom decision_tree_surface import *\nfrom manim import *\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import entropy\nimport math\nfrom PIL import Image\n\niris = datasets.load_iris()\nfont = \"Source Han Sans\"\nfont_scale = 0.75\n\nimages = [\n    Image.open(\"iris_dataset/SetosaFlower.jpeg\"),\n    Image.open(\"iris_dataset/VeriscolorFlower.jpeg\"),\n    Image.open(\"iris_dataset/VirginicaFlower.jpeg\"),\n]\n\n\ndef entropy(class_labels, base=2):\n    # compute the class counts\n    unique, counts = np.unique(class_labels, return_counts=True)\n    dictionary = dict(zip(unique, counts))\n    total = 0.0\n    num_samples = len(class_labels)\n    for class_index in range(0, 3):\n        if not class_index in dictionary:\n            continue\n        prob = dictionary[class_index] / num_samples\n        total += prob * math.log(prob, base)\n    # higher set\n    return -total\n\n\ndef merge_overlapping_polygons(all_polygons, colors=[BLUE, GREEN, ORANGE]):\n    # get all polygons of each color\n    polygon_dict = {\n        str(BLUE).lower(): [],\n        str(GREEN).lower(): [],\n        str(ORANGE).lower(): [],\n    }\n    for polygon in all_polygons:\n        print(polygon_dict)\n        polygon_dict[str(polygon.color).lower()].append(polygon)\n\n    return_polygons = []\n    for color in colors:\n        color = str(color).lower()\n        polygons = polygon_dict[color]\n        points = set()\n        for polygon in polygons:\n            vertices = polygon.get_vertices().tolist()\n            vertices = [tuple(vert) for vert in vertices]\n            for pt in vertices:\n                if pt in points:  # Shared vertice, remove it.\n                    points.remove(pt)\n                else:\n                    points.add(pt)\n        points = list(points)\n        sort_x = sorted(points)\n        sort_y = sorted(points, key=lambda x: x[1])\n\n        edges_h = {}\n        edges_v = {}\n\n        i = 0\n        while i < len(points):\n            curr_y = sort_y[i][1]\n            while i < len(points) and sort_y[i][1] == curr_y:\n                edges_h[sort_y[i]] = sort_y[i + 1]\n                edges_h[sort_y[i + 1]] = sort_y[i]\n                i += 2\n        i = 0\n        while i < len(points):\n            curr_x = sort_x[i][0]\n            while i < len(points) and sort_x[i][0] == curr_x:\n                edges_v[sort_x[i]] = sort_x[i + 1]\n                edges_v[sort_x[i + 1]] = sort_x[i]\n                i += 2\n\n        # Get all the polygons.\n        while edges_h:\n            # We can start with any point.\n            polygon = [(edges_h.popitem()[0], 0)]\n            while True:\n                curr, e = polygon[-1]\n                if e == 0:\n                    next_vertex = edges_v.pop(curr)\n                    polygon.append((next_vertex, 1))\n                else:\n                    next_vertex = edges_h.pop(curr)\n                    polygon.append((next_vertex, 0))\n                if polygon[-1] == polygon[0]:\n                    # Closed polygon\n                    polygon.pop()\n                    break\n            # Remove implementation-markers from the polygon.\n            poly = [point for point, _ in polygon]\n            for vertex in poly:\n                if vertex in edges_h:\n                    edges_h.pop(vertex)\n                if vertex in edges_v:\n                    edges_v.pop(vertex)\n            polygon = Polygon(*poly, color=color, fill_opacity=0.3, stroke_opacity=1.0)\n            return_polygons.append(polygon)\n    return return_polygons\n\n\nclass IrisDatasetPlot(VGroup):\n    def __init__(self):\n        points = iris.data[:, 0:2]\n        labels = iris.feature_names\n        targets = iris.target\n        # Make points\n        self.point_group = self._make_point_group(points, targets)\n        # Make axes\n        self.axes_group = self._make_axes_group(points, labels)\n        # Make legend\n        self.legend_group = self._make_legend(\n            [BLUE, ORANGE, GREEN], iris.target_names, self.axes_group\n        )\n        # Make title\n        # title_text = \"Iris Dataset Plot\"\n        # self.title = Text(title_text).match_y(self.axes_group).shift([0.5, self.axes_group.height / 2 + 0.5, 0])\n        # Make all group\n        self.all_group = Group(self.point_group, self.axes_group, self.legend_group)\n        # scale the groups\n        self.point_group.scale(1.6)\n        self.point_group.match_x(self.axes_group)\n        self.point_group.match_y(self.axes_group)\n        self.point_group.shift([0.2, 0, 0])\n        self.axes_group.scale(0.7)\n        self.all_group.shift([0, 0.2, 0])\n\n    @override_animation(Create)\n    def create_animation(self):\n        animation_group = AnimationGroup(\n            # Perform the animations\n            Create(self.point_group, run_time=2),\n            Wait(0.5),\n            Create(self.axes_group, run_time=2),\n            # add title\n            # Create(self.title),\n            Create(self.legend_group),\n        )\n        return animation_group\n\n    def _make_point_group(self, points, targets, class_colors=[BLUE, ORANGE, GREEN]):\n        point_group = VGroup()\n        for point_index, point in enumerate(points):\n            # draw the dot\n            current_target = targets[point_index]\n            color = class_colors[current_target]\n            dot = Dot(point=np.array([point[0], point[1], 0])).set_color(color)\n            dot.scale(0.5)\n            point_group.add(dot)\n        return point_group\n\n    def _make_legend(self, class_colors, feature_labels, axes):\n        legend_group = VGroup()\n        # Make Text\n        setosa = Text(\"Setosa\", color=BLUE)\n        verisicolor = Text(\"Verisicolor\", color=ORANGE)\n        virginica = Text(\"Virginica\", color=GREEN)\n        labels = VGroup(setosa, verisicolor, virginica).arrange(\n            direction=RIGHT, aligned_edge=LEFT, buff=2.0\n        )\n        labels.scale(0.5)\n        legend_group.add(labels)\n        # surrounding rectangle\n        surrounding_rectangle = SurroundingRectangle(labels, color=WHITE)\n        surrounding_rectangle.move_to(labels)\n        legend_group.add(surrounding_rectangle)\n        # shift the legend group\n        legend_group.move_to(axes)\n        legend_group.shift([0, -3.0, 0])\n        legend_group.match_x(axes[0][0])\n\n        return legend_group\n\n    def _make_axes_group(self, points, labels):\n        axes_group = VGroup()\n        # make the axes\n        x_range = [\n            np.amin(points, axis=0)[0] - 0.2,\n            np.amax(points, axis=0)[0] - 0.2,\n            0.5,\n        ]\n        y_range = [np.amin(points, axis=0)[1] - 0.2, np.amax(points, axis=0)[1], 0.5]\n        axes = Axes(\n            x_range=x_range,\n            y_range=y_range,\n            x_length=9,\n            y_length=6.5,\n            # axis_config={\"number_scale_value\":0.75, \"include_numbers\":True},\n            tips=False,\n        ).shift([0.5, 0.25, 0])\n        axes_group.add(axes)\n        # make axis labels\n        # x_label\n        x_label = (\n            Text(labels[0], font=font)\n            .match_y(axes.get_axes()[0])\n            .shift([0.5, -0.75, 0])\n            .scale(font_scale)\n        )\n        axes_group.add(x_label)\n        # y_label\n        y_label = (\n            Text(labels[1], font=font)\n            .match_x(axes.get_axes()[1])\n            .shift([-0.75, 0, 0])\n            .rotate(np.pi / 2)\n            .scale(font_scale)\n        )\n        axes_group.add(y_label)\n\n        return axes_group\n\n\nclass DecisionTreeSurface(VGroup):\n    def __init__(self, tree_clf, data, axes, class_colors=[BLUE, ORANGE, GREEN]):\n        # take the tree and construct the surface from it\n        self.tree_clf = tree_clf\n        self.data = data\n        self.axes = axes\n        self.class_colors = class_colors\n        self.surface_rectangles = self.generate_surface_rectangles()\n\n    def generate_surface_rectangles(self):\n        # compute data bounds\n        left = np.amin(self.data[:, 0]) - 0.2\n        right = np.amax(self.data[:, 0]) - 0.2\n        top = np.amax(self.data[:, 1])\n        bottom = np.amin(self.data[:, 1]) - 0.2\n        maxrange = [left, right, bottom, top]\n        rectangles = decision_areas(self.tree_clf, maxrange, x=0, y=1, n_features=2)\n        # turn the rectangle objects into manim rectangles\n        def convert_rectangle_to_polygon(rect):\n            # get the points for the rectangle in the plot coordinate frame\n            bottom_left = [rect[0], rect[3]]\n            bottom_right = [rect[1], rect[3]]\n            top_right = [rect[1], rect[2]]\n            top_left = [rect[0], rect[2]]\n            # convert those points into the entire manim coordinates\n            bottom_left_coord = self.axes.coords_to_point(*bottom_left)\n            bottom_right_coord = self.axes.coords_to_point(*bottom_right)\n            top_right_coord = self.axes.coords_to_point(*top_right)\n            top_left_coord = self.axes.coords_to_point(*top_left)\n            points = [\n                bottom_left_coord,\n                bottom_right_coord,\n                top_right_coord,\n                top_left_coord,\n            ]\n            # construct a polygon object from those manim coordinates\n            rectangle = Polygon(\n                *points, color=color, fill_opacity=0.3, stroke_opacity=0.0\n            )\n            return rectangle\n\n        manim_rectangles = []\n        for rect in rectangles:\n            color = self.class_colors[int(rect[4])]\n            rectangle = convert_rectangle_to_polygon(rect)\n            manim_rectangles.append(rectangle)\n\n        manim_rectangles = merge_overlapping_polygons(\n            manim_rectangles, colors=[BLUE, GREEN, ORANGE]\n        )\n\n        return manim_rectangles\n\n    @override_animation(Create)\n    def create_override(self):\n        # play a reveal of all of the surface rectangles\n        animations = []\n        for rectangle in self.surface_rectangles:\n            animations.append(Create(rectangle))\n        animation_group = AnimationGroup(*animations)\n\n        return animation_group\n\n    @override_animation(Uncreate)\n    def uncreate_override(self):\n        # play a reveal of all of the surface rectangles\n        animations = []\n        for rectangle in self.surface_rectangles:\n            animations.append(Uncreate(rectangle))\n        animation_group = AnimationGroup(*animations)\n\n        return animation_group\n\n\nclass DecisionTree:\n    \"\"\"\n    Draw a single tree node\n    \"\"\"\n\n    def _make_node(\n        self,\n        feature,\n        threshold,\n        values,\n        is_leaf=False,\n        depth=0,\n        leaf_colors=[BLUE, ORANGE, GREEN],\n    ):\n        if not is_leaf:\n            node_text = f\"{feature}\\n    <= {threshold:.3f} cm\"\n            # draw decision text\n            decision_text = Text(node_text, color=WHITE)\n            # draw a box\n            bounding_box = SurroundingRectangle(decision_text, buff=0.3, color=WHITE)\n            node = VGroup()\n            node.add(bounding_box)\n            node.add(decision_text)\n            # return the node\n        else:\n            # plot the appropriate image\n            class_index = np.argmax(values)\n            # get image\n            pil_image = images[class_index]\n            leaf_group = Group()\n            node = ImageMobject(pil_image)\n            node.scale(1.5)\n            rectangle = Rectangle(\n                width=node.width + 0.05,\n                height=node.height + 0.05,\n                color=leaf_colors[class_index],\n                stroke_width=6,\n            )\n            rectangle.move_to(node.get_center())\n            rectangle.shift([-0.02, 0.02, 0])\n            leaf_group.add(rectangle)\n            leaf_group.add(node)\n            node = leaf_group\n\n        return node\n\n    def _make_connection(self, top, bottom, is_leaf=False):\n        top_node_bottom_location = top.get_center()\n        top_node_bottom_location[1] -= top.height / 2\n        bottom_node_top_location = bottom.get_center()\n        bottom_node_top_location[1] += bottom.height / 2\n        line = Line(top_node_bottom_location, bottom_node_top_location, color=WHITE)\n        return line\n\n    def _make_tree(self, tree, feature_names=[\"Sepal Length\", \"Sepal Width\"]):\n        tree_group = Group()\n        max_depth = tree.max_depth\n        # make the base node\n        feature_name = feature_names[tree.feature[0]]\n        threshold = tree.threshold[0]\n        values = tree.value[0]\n        nodes_map = {}\n        root_node = self._make_node(feature_name, threshold, values, depth=0)\n        nodes_map[0] = root_node\n        tree_group.add(root_node)\n        # save some information\n        node_height = root_node.height\n        node_width = root_node.width\n        scale_factor = 1.0\n        edge_map = {}\n        # tree height\n        tree_height = scale_factor * node_height * max_depth\n        tree_width = scale_factor * 2**max_depth * node_width\n        # traverse tree\n        def recurse(node, depth, direction, parent_object, parent_node):\n            # make sure it is a valid node\n            # make the node object\n            is_leaf = tree.children_left[node] == tree.children_right[node]\n            feature_name = feature_names[tree.feature[node]]\n            threshold = tree.threshold[node]\n            values = tree.value[node]\n            node_object = self._make_node(\n                feature_name, threshold, values, depth=depth, is_leaf=is_leaf\n            )\n            nodes_map[node] = node_object\n            node_height = node_object.height\n            # set the node position\n            direction_factor = -1 if direction == \"left\" else 1\n            shift_right_amount = (\n                0.8 * direction_factor * scale_factor * tree_width / (2**depth) / 2\n            )\n            if is_leaf:\n                shift_down_amount = -1.0 * scale_factor * node_height\n            else:\n                shift_down_amount = -1.8 * scale_factor * node_height\n            node_object.match_x(parent_object).match_y(parent_object).shift(\n                [shift_right_amount, shift_down_amount, 0]\n            )\n            tree_group.add(node_object)\n            # make a connection\n            connection = self._make_connection(\n                parent_object, node_object, is_leaf=is_leaf\n            )\n            edge_name = str(parent_node) + \",\" + str(node)\n            edge_map[edge_name] = connection\n            tree_group.add(connection)\n            # recurse\n            if not is_leaf:\n                recurse(tree.children_left[node], depth + 1, \"left\", node_object, node)\n                recurse(\n                    tree.children_right[node], depth + 1, \"right\", node_object, node\n                )\n\n        recurse(tree.children_left[0], 1, \"left\", root_node, 0)\n        recurse(tree.children_right[0], 1, \"right\", root_node, 0)\n\n        tree_group.scale(0.35)\n        return tree_group, nodes_map, edge_map\n\n    def color_example_path(\n        self, tree_group, nodes_map, tree, edge_map, example, color=YELLOW, thickness=2\n    ):\n        # get decision path\n        decision_path = tree.decision_path(example)[0]\n        path_indices = decision_path.indices\n        # highlight edges\n        for node_index in range(0, len(path_indices) - 1):\n            current_val = path_indices[node_index]\n            next_val = path_indices[node_index + 1]\n            edge_str = str(current_val) + \",\" + str(next_val)\n            edge = edge_map[edge_str]\n            animation_two = AnimationGroup(\n                nodes_map[current_val].animate.set_color(color)\n            )\n            self.play(animation_two, run_time=0.5)\n            animation_one = AnimationGroup(\n                edge.animate.set_color(color),\n                # edge.animate.set_stroke_width(4),\n            )\n            self.play(animation_one, run_time=0.5)\n        # surround the bottom image\n        last_path_index = path_indices[-1]\n        last_path_rectangle = nodes_map[last_path_index][0]\n        self.play(last_path_rectangle.animate.set_color(color))\n\n    def create_sklearn_tree(self, max_tree_depth=1):\n        # learn the decision tree\n        iris = load_iris()\n        tree = learn_iris_decision_tree(iris, max_depth=max_tree_depth)\n        feature_names = iris.feature_names[0:2]\n        return tree.tree_\n\n    def make_tree(self, max_tree_depth=2):\n        sklearn_tree = self.create_sklearn_tree()\n        # make the tree\n        tree_group, nodes_map, edge_map = self._make_tree(\n            sklearn_tree.tree_, feature_names\n        )\n        tree_group.shift([0, 5.5, 0])\n        return tree_group\n        # self.add(tree_group)\n        # self.play(SpinInFromNothing(tree_group), run_time=3)\n        # self.color_example_path(tree_group, nodes_map, tree, edge_map, iris.data[None, 0, 0:2])\n\n\nclass DecisionTreeSplitScene(Scene):\n    def make_decision_tree_classifier(self, max_depth=4):\n        decision_tree = DecisionTreeClassifier(\n            random_state=1, max_depth=max_depth, max_leaf_nodes=8\n        )\n        decision_tree = decision_tree.fit(iris.data[:, :2], iris.target)\n        # output the decisioin tree in some format\n        return decision_tree\n\n    def make_split_animation(self, data, classes, data_labels, main_axes):\n\n        \"\"\"\n        def make_entropy_animation_and_plot(dim=0, num_entropy_values=50):\n            # calculate the entropy values\n            axes_group = VGroup()\n            # make axes\n            range_vals = [np.amin(data, axis=0)[dim], np.amax(data, axis=0)[dim]]\n            axes = Axes(x_range=range_vals,\n                        y_range=[0, 1.0],\n                        x_length=9,\n                        y_length=4,\n            #            axis_config={\"number_scale_value\":0.75, \"include_numbers\":True},\n                        tips=False,\n                    )\n            axes_group.add(axes)\n            # make axis labels\n            # x_label\n            x_label = Text(data_labels[dim], font=font) \\\n                            .match_y(axes.get_axes()[0]) \\\n                            .shift([0.5, -0.75, 0]) \\\n                            .scale(font_scale*1.2)\n            axes_group.add(x_label)\n            # y_label\n            y_label = Text(\"Information Gain\", font=font) \\\n                .match_x(axes.get_axes()[1]) \\\n                .shift([-0.75, 0, 0]) \\\n                .rotate(np.pi / 2) \\\n                .scale(font_scale * 1.2)\n\n            axes_group.add(y_label)\n            # line animation\n            information_gains = []\n            def entropy_function(split_value):\n                # lower entropy\n                lower_set = np.nonzero(data[:, dim] <= split_value)[0]\n                lower_set = classes[lower_set]\n                lower_entropy = entropy(lower_set)\n                # higher entropy\n                higher_set = np.nonzero(data[:, dim] > split_value)[0]\n                higher_set = classes[higher_set]\n                higher_entropy = entropy(higher_set)\n                # calculate entropies\n                all_entropy = entropy(classes, base=2)\n                lower_entropy = entropy(lower_set, base=2)\n                higher_entropy = entropy(higher_set, base=2)\n                mean_entropy = (lower_entropy + higher_entropy) / 2\n                # calculate information gain\n                lower_prob = len(lower_set) / len(data[:, dim])\n                higher_prob = len(higher_set) / len(data[:, dim])\n                info_gain = all_entropy - (lower_prob * lower_entropy + higher_prob * higher_entropy)\n                information_gains.append((split_value, info_gain))\n                return info_gain\n\n            data_range = np.amin(data[:, dim]), np.amax(data[:, dim])\n\n            entropy_graph = axes.get_graph(\n                entropy_function, \n            #    color=RED, \n            #    x_range=data_range\n            )\n            axes_group.add(entropy_graph)\n            axes_group.shift([4.0, 2, 0])\n            axes_group.scale(0.5)\n            dot_animation = Dot(color=WHITE)\n            axes_group.add(dot_animation)\n            # make animations\n            animation_group = AnimationGroup(\n                Create(axes_group, run_time=2),\n                Wait(3),\n                MoveAlongPath(dot_animation, entropy_graph, run_time=20, rate_func=rate_functions.ease_in_out_quad),\n                Wait(2)\n            )\n\n            return axes_group, animation_group, information_gains\n        \"\"\"\n\n        def make_split_line_animation(dim=0):\n            # make a line along one of the dims and move it up and down\n            origin_coord = [\n                np.amin(data, axis=0)[0] - 0.2,\n                np.amin(data, axis=0)[1] - 0.2,\n            ]\n            origin_point = main_axes.coords_to_point(*origin_coord)\n            top_left_coord = [origin_coord[0], np.amax(data, axis=0)[1]]\n            bottom_right_coord = [np.amax(data, axis=0)[0] - 0.2, origin_coord[1]]\n            if dim == 0:\n                other_coord = top_left_coord\n                moving_line_coord = bottom_right_coord\n            else:\n                other_coord = bottom_right_coord\n                moving_line_coord = top_left_coord\n            other_point = main_axes.coords_to_point(*other_coord)\n            moving_line_point = main_axes.coords_to_point(*moving_line_coord)\n            moving_line = Line(origin_point, other_point, color=RED)\n            movement_line = Line(origin_point, moving_line_point)\n            if dim == 0:\n                movement_line.shift([0, moving_line.height / 2, 0])\n            else:\n                movement_line.shift([moving_line.width / 2, 0, 0])\n            # move the moving line along the movement line\n            animation = MoveAlongPath(\n                moving_line,\n                movement_line,\n                run_time=20,\n                rate_func=rate_functions.ease_in_out_quad,\n            )\n            return animation, moving_line\n\n        # plot the line in white then make it invisible\n        # make an animation along the line\n        # make a\n        # axes_one_group, top_animation_group, info_gains = make_entropy_animation_and_plot(dim=0)\n        line_movement, first_moving_line = make_split_line_animation(dim=0)\n        # axes_two_group, bottom_animation_group, _ = make_entropy_animation_and_plot(dim=1)\n        second_line_movement, second_moving_line = make_split_line_animation(dim=1)\n        # axes_two_group.shift([0, -3, 0])\n        animation_group_one = AnimationGroup(\n            #    top_animation_group,\n            line_movement,\n        )\n\n        animation_group_two = AnimationGroup(\n            #    bottom_animation_group,\n            second_line_movement,\n        )\n        \"\"\"\n        both_axes_group = VGroup(\n            axes_one_group,\n            axes_two_group\n        )\n        \"\"\"\n\n        return (\n            animation_group_one,\n            animation_group_two,\n            first_moving_line,\n            second_moving_line,\n            None,\n            None,\n        )\n        #    both_axes_group, \\\n        #    info_gains\n\n    def construct(self):\n        # make the points\n        iris_dataset_plot = IrisDatasetPlot()\n        iris_dataset_plot.all_group.scale(1.0)\n        iris_dataset_plot.all_group.shift([-3, 0.2, 0])\n        # make the entropy line graph\n        # entropy_line_graph = self.draw_entropy_line_graph()\n        # arrange the plots\n        # do animations\n        self.play(Create(iris_dataset_plot))\n        # make the decision tree classifier\n        decision_tree_classifier = self.make_decision_tree_classifier()\n        decision_tree_surface = DecisionTreeSurface(\n            decision_tree_classifier, iris.data, iris_dataset_plot.axes_group[0]\n        )\n        self.play(Create(decision_tree_surface))\n        self.wait(3)\n        self.play(Uncreate(decision_tree_surface))\n        main_axes = iris_dataset_plot.axes_group[0]\n        (\n            split_animation_one,\n            split_animation_two,\n            first_moving_line,\n            second_moving_line,\n            both_axes_group,\n            info_gains,\n        ) = self.make_split_animation(\n            iris.data[:, 0:2], iris.target, iris.feature_names, main_axes\n        )\n        self.play(split_animation_one)\n        self.wait(0.1)\n        self.play(Uncreate(first_moving_line))\n        self.wait(3)\n        self.play(split_animation_two)\n        self.wait(0.1)\n        self.play(Uncreate(second_moving_line))\n        self.wait(0.1)\n        # highlight the maximum on top\n        # sort by second key\n        \"\"\"\n        highest_info_gain = sorted(info_gains, key=lambda x: x[1])[-1]\n        highest_info_gain_point = both_axes_group[0][0].coords_to_point(*highest_info_gain)\n        highlighted_peak = Dot(highest_info_gain_point, color=YELLOW)\n        # get location of highest info gain point\n        highest_info_gain_point_in_iris_graph = iris_dataset_plot.axes_group[0].coords_to_point(*[highest_info_gain[0], 0])\n        first_moving_line.start[0] = highest_info_gain_point_in_iris_graph[0]\n        first_moving_line.end[0] = highest_info_gain_point_in_iris_graph[0]\n        self.play(Create(highlighted_peak))\n        self.play(Create(first_moving_line))\n        text = Text(\"Highest Information Gain\")\n        text.scale(0.4)\n        text.move_to(highlighted_peak)\n        text.shift([0, 0.5, 0])\n        self.play(Create(text))\n        \"\"\"\n        self.wait(1)\n        # draw the basic tree\n        decision_tree_classifier = self.make_decision_tree_classifier(max_depth=1)\n        decision_tree_surface = DecisionTreeSurface(\n            decision_tree_classifier, iris.data, iris_dataset_plot.axes_group[0]\n        )\n        decision_tree_graph, _, _ = DecisionTree()._make_tree(\n            decision_tree_classifier.tree_\n        )\n        decision_tree_graph.match_y(iris_dataset_plot.axes_group)\n        decision_tree_graph.shift([4, 0, 0])\n        self.play(Create(decision_tree_surface))\n        uncreate_animation = AnimationGroup(\n            #    Uncreate(both_axes_group),\n            #    Uncreate(highlighted_peak),\n            Uncreate(second_moving_line),\n            #    Unwrite(text)\n        )\n        self.play(uncreate_animation)\n        self.wait(0.5)\n        self.play(FadeIn(decision_tree_graph))\n        # self.play(FadeIn(highlighted_peak))\n        self.wait(5)\n", "file_path": "examples/decision_tree/split_scene.py"}
{"prompt": "Diffusion Process Diffusion Process", "response": "\"\"\"\n    Shows video of diffusion process. \n\"\"\"\nimport manim_ml\nfrom manim import *\nfrom PIL import Image\nimport os\nfrom diffusers import StableDiffusionPipeline\nimport numpy as np\nimport scipy\n\nfrom manim_ml.diffusion.mcmc import metropolis_hastings_sampler, gaussian_proposal\nfrom manim_ml.diffusion.random_walk import RandomWalk\nfrom manim_ml.utils.mobjects.probability import make_dist_image_mobject_from_samples\n\ndef generate_stable_diffusion_images(prompt, num_inference_steps=30):\n    \"\"\"Generates a list of progressively denoised images using Stable Diffusion\n\n    Parameters\n    ----------\n    num_inference_steps : int, optional\n        _description_, by default 30\n    \"\"\"\n    pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n    image_list = [] \n    # Save image callback\n    def save_image_callback(step, timestep, latents):\n        print(\"Saving image callback\")\n        # Decode the latents\n        image = pipeline.vae.decode(latents)\n        image_list.append(image)\n\n    # Generate the image\n    pipeline(prompt, num_inference_steps=num_inference_steps, callback=save_image_callback)\n\n    return image_list\n\ndef make_time_schedule_bar(num_time_steps=30):\n    \"\"\"Makes a bar that gets moved back and forth according to the diffusion model time\"\"\"\n    # Draw time bar with initial value\n    time_bar = NumberLine(\n        [0, num_time_steps], length=25, stroke_width=10, include_ticks=False, include_numbers=False,\n        color=manim_ml.config.color_scheme.secondary_color\n\n    )\n    time_bar.shift(4.5 * DOWN)\n    current_time = ValueTracker(0.3)\n    time_point = time_bar.number_to_point(current_time.get_value())\n    time_dot = Dot(time_point, color=manim_ml.config.color_scheme.secondary_color, radius=0.2)\n    label_location = time_bar.number_to_point(1.0)\n    label_location -= DOWN * 0.1\n    label_text = MathTex(\"t\", color=manim_ml.config.color_scheme.text_color).scale(1.5)\n    # label_text = Text(\"time\")\n    label_text.move_to(time_bar.get_center())\n    label_text.shift(DOWN * 0.5)\n    # Make an updater for the dot\n    def dot_updater(time_dot):\n        # Get location on time_bar\n        point_loc = time_bar.number_to_point(current_time.get_value())\n        time_dot.move_to(point_loc)\n\n    time_dot.add_updater(dot_updater)\n\n    return time_bar, time_dot, label_text, current_time\n\ndef make_2d_diffusion_space():\n    \"\"\"Makes a 2D axis where the diffusion random walk happens in. \n    There is also a distribution of points representing the true distribution\n    of images. \n    \"\"\"\n    axes_group = Group()\n    x_range = [-8, 8]\n    y_range = [-8, 8]\n    y_length = 13\n    x_length = 13\n    # Make an axis\n    axes = Axes(\n        x_range=x_range,\n        y_range=y_range,\n        x_length=x_length,\n        y_length=y_length,\n        # x_axis_config={\"stroke_opacity\": 1.0, \"stroke_color\": WHITE},\n        # y_axis_config={\"stroke_opacity\": 1.0, \"stroke_color\": WHITE},\n        # tips=True,\n    )\n    # Make the distribution for the true images as some gaussian mixture in \n    # the bottom right\n    gaussian_a = np.random.multivariate_normal(\n        mean=[3.0, -3.2],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(100)\n    )\n    gaussian_b = np.random.multivariate_normal(\n        mean=[3.0, 3.0],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(100)\n    )\n    gaussian_c = np.random.multivariate_normal(\n        mean=[0.0, -1.6],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(200)\n    )\n    all_gaussian_samples = np.concatenate([gaussian_a, gaussian_b, gaussian_c], axis=0)\n    print(f\"Shape of all gaussian samples: {all_gaussian_samples.shape}\")\n\n    image_mobject = make_dist_image_mobject_from_samples(\n        all_gaussian_samples,\n        xlim=(x_range[0], x_range[1]),\n        ylim=(y_range[0], y_range[1])\n    )\n    image_mobject.scale_to_fit_height(\n        axes.get_height()     \n    )\n    image_mobject.move_to(axes)\n\n    axes_group.add(axes)\n    axes_group.add(image_mobject)\n\n    return axes_group\n\ndef generate_forward_and_reverse_chains(start_point, target_point, num_inference_steps=30):\n    \"\"\"Here basically we want to generate a forward and reverse chain\n    for the diffusion model where the reverse chain starts at the end of the forward\n    chain, and the end of the reverse chain ends somewhere within a certain radius of the \n    reverse chain.\n\n    This can be done in a sortof hacky way by doing metropolis hastings from a start point\n    to a distribution centered about the start point and vica versa.\n\n    Parameters\n    ----------\n    start_point : _type_\n        _description_\n    \"\"\"\n    def start_dist_log_prob_fn(x):\n        \"\"\"Log probability of the start distribution\"\"\"\n        # Make it a gaussian in top left of the 2D space\n        gaussian_pdf = scipy.stats.multivariate_normal(\n            mean=target_point,\n            cov=[1.0, 1.0]\n        ).pdf(x)\n\n        return np.log(gaussian_pdf)\n    \n    def end_dist_log_prob_fn(x):\n        gaussian_pdf = scipy.stats.multivariate_normal(\n            mean=start_point,\n            cov=[1.0, 1.0]\n        ).pdf(x)\n\n        return np.log(gaussian_pdf)\n\n    forward_chain, _, _ = metropolis_hastings_sampler(\n        log_prob_fn=start_dist_log_prob_fn, \n        prop_fn=gaussian_proposal,\n        iterations=num_inference_steps,\n        initial_location=start_point\n    )\n\n    end_point = forward_chain[-1]\n\n    reverse_chain, _, _ = metropolis_hastings_sampler(\n        log_prob_fn=end_dist_log_prob_fn,\n        prop_fn=gaussian_proposal,\n        iterations=num_inference_steps,\n        initial_location=end_point\n    )\n\n    return forward_chain, reverse_chain\n\n# Make the scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 30.0\nconfig.frame_width = 30.0\n\nclass DiffusionProcess(Scene):\n\n    def construct(self):\n        # Parameters\n        num_inference_steps = 50\n        image_save_dir = \"diffusion_images\"\n        prompt = \"An oil painting of a dragon.\"\n        start_time = 0\n        # Compute the stable diffusion images\n        if len(os.listdir(image_save_dir)) < num_inference_steps:\n            stable_diffusion_images = generate_stable_diffusion_images(\n                prompt,\n                num_inference_steps=num_inference_steps\n            )\n            for index, image in enumerate(stable_diffusion_images):\n                image.save(f\"{image_save_dir}/{index}.png\")\n        else:\n            stable_diffusion_images = [Image.open(f\"{image_save_dir}/{i}.png\") for i in range(num_inference_steps)]\n        # Reverse order of list to be in line with theory timesteps\n        stable_diffusion_images = stable_diffusion_images[::-1]\n        # Add the initial location of the first image\n        start_image = stable_diffusion_images[start_time]\n        image_mobject = ImageMobject(start_image)\n        image_mobject.scale(0.55)\n        image_mobject.shift(LEFT * 7.5)\n        self.add(image_mobject)\n        # Make the time schedule bar\n        time_bar, time_dot, time_label, current_time = make_time_schedule_bar(num_time_steps=num_inference_steps)\n        # Place the bar at the bottom of the screen\n        time_bar.move_to(image_mobject.get_bottom() + DOWN * 2)\n        time_bar.set_x(0)\n        self.add(time_bar)\n        # Place the time label below the time bar\n        self.add(time_label)\n        time_label.next_to(time_bar, DOWN, buff=0.5)\n        # Add 0 and T labels above the bar\n        zero_label = Text(\"0\")\n        zero_label.next_to(time_bar.get_left(), UP, buff=0.5)\n        self.add(zero_label)\n        t_label = Text(\"T\")\n        t_label.next_to(time_bar.get_right(), UP, buff=0.5)\n        self.add(t_label)\n        # Move the time dot to zero\n        time_dot.set_value(0)\n        time_dot.move_to(time_bar.number_to_point(0))\n        self.add(time_dot)\n        # Add the prompt above the image\n        paragraph_prompt = '\"An oil painting of\\n a flaming dragon.\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        text_prompt.next_to(image_mobject, UP, buff=0.6)\n        self.add(text_prompt)\n        # Generate the chain data\n        forward_chain, reverse_chain = generate_forward_and_reverse_chains(\n            start_point=[3.0, -3.0],\n            target_point=[-7.0, -4.0],\n            num_inference_steps=num_inference_steps\n        )\n        # Make the axes that the distribution and chains go on\n        axes, axes_group = make_2d_diffusion_space()\n        # axes_group.match_height(image_mobject)\n        axes_group.shift(RIGHT * 7.5)\n        axes.shift(RIGHT * 7.5)\n        self.add(axes_group)\n        # Add title below distribution\n        title = Text(\"Distribution of Real Images\", font_size=48, color=RED)\n        title.move_to(axes_group)\n        title.shift(DOWN * 6)\n        line = Line(\n            title.get_top(),\n            axes_group.get_bottom() - 4 * DOWN,\n        )\n        self.add(title)\n        self.add(line)\n        # Add a title above the plot\n        title = Text(\"Reverse Diffusion Process\", font_size=64)\n        title.move_to(axes)\n        title.match_y(text_prompt)\n        self.add(title)\n        # First do the forward noise process of adding noise to an image\n        forward_random_walk = RandomWalk(forward_chain, axes=axes)\n        # Sync up forward random walk\n        synced_animations = []\n        forward_random_walk_animation = forward_random_walk.animate_random_walk()\n        print(len(forward_random_walk_animation.animations))\n        for timestep, transition_animation in enumerate(forward_random_walk_animation.animations):\n            # Make an animation moving the time bar\n            # time_bar_animation = current_time.animate.set_value(timestep)\n            time_bar_animation = current_time.animate.set_value(timestep)\n            # Make an animation replacing the current image with the timestep image\n            new_image = ImageMobject(stable_diffusion_images[timestep])\n            new_image = new_image.set_height(image_mobject.get_height())\n            new_image.move_to(image_mobject)\n            image_mobject = new_image\n            replace_image_animation = AnimationGroup(\n                FadeOut(image_mobject),\n                FadeIn(new_image),\n                lag_ratio=1.0\n            )\n            # Sync them together\n            # synced_animations.append(\n            self.play(\n                Succession(\n                    transition_animation, \n                    time_bar_animation,\n                    replace_image_animation,\n                    lag_ratio=0.0\n                )\n            )\n\n        # Fade out the random walk\n        self.play(forward_random_walk.fade_out_random_walk())\n        # self.play(\n        # Succession(\n        # *synced_animations,\n        # )\n        # )\n        new_title = Text(\"Forward Diffusion Process\", font_size=64)\n        new_title.move_to(title)\n        self.play(ReplacementTransform(title, new_title))\n        # Second do the reverse noise process of removing noise from the image\n        backward_random_walk = RandomWalk(reverse_chain, axes=axes)\n        # Sync up forward random walk\n        synced_animations = []\n        backward_random_walk_animation = backward_random_walk.animate_random_walk()\n        for timestep, transition_animation in enumerate(backward_random_walk_animation.animations):\n            timestep = num_inference_steps - timestep - 1\n            if timestep == num_inference_steps - 1:\n                continue\n            # Make an animation moving the time bar\n            # time_bar_animation = time_dot.animate.set_value(timestep)\n            time_bar_animation = current_time.animate.set_value(timestep)\n            # Make an animation replacing the current image with the timestep image\n            new_image = ImageMobject(stable_diffusion_images[timestep])\n            new_image = new_image.set_height(image_mobject.get_height())\n            new_image.move_to(image_mobject)\n            image_mobject = new_image\n            replace_image_animation = AnimationGroup(\n                FadeOut(image_mobject),\n                FadeIn(new_image),\n                lag_ratio=1.0\n            )\n            # Sync them together\n            # synced_animations.append(\n            self.play(\n                Succession(\n                    transition_animation, \n                    time_bar_animation,\n                    replace_image_animation,\n                    lag_ratio=0.0\n                )\n            )\n        # self.play(\n        # Succession(\n        # *synced_animations,\n        # )\n        # )\n", "file_path": "examples/diffusion_process/diffusion_process.py"}
{"prompt": "Disentanglement Disentanglement", "response": "\"\"\"This module is dedicated to visualizing VAE disentanglement\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\n\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nimport pickle\n\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef construct_image_mobject(input_image, height=2.3):\n    \"\"\"Constructs an ImageMobject from a numpy grayscale image\"\"\"\n    # Convert image to rgb\n    if len(input_image.shape) == 2:\n        input_image = np.repeat(input_image, 3, axis=0)\n        input_image = np.rollaxis(input_image, 0, start=3)\n    #  Make the ImageMobject\n    image_mobject = ImageMobject(input_image, image_mode=\"RGB\")\n    image_mobject.set_resampling_algorithm(RESAMPLING_ALGORITHMS[\"nearest\"])\n    image_mobject.height = height\n\n    return image_mobject\n\n\nclass DisentanglementVisualization(VGroup):\n    def __init__(\n        self,\n        model_path=ROOT_DIR\n        / \"examples/variational_autoencoder/autoencoder_models/saved_models/model_dim2.pth\",\n        image_height=0.35,\n    ):\n        self.model_path = model_path\n        self.image_height = image_height\n        # Load disentanglement image objects\n        with open(\n            ROOT_DIR\n            / \"examples/variational_autoencoder/autoencoder_models/disentanglement.pkl\",\n            \"rb\",\n        ) as f:\n            self.image_handler = pickle.load(f)\n\n    def make_disentanglement_generation_animation(self):\n        animation_list = []\n        for image_index, image in enumerate(self.image_handler[\"images\"]):\n            image_mobject = construct_image_mobject(image, height=self.image_height)\n            r, c = self.image_handler[\"bin_indices\"][image_index]\n            # Move the image to the correct location\n            r_offset = -1.2\n            c_offset = 0.25\n            image_location = [\n                c_offset + c * self.image_height,\n                r_offset + r * self.image_height,\n                0,\n            ]\n            image_mobject.move_to(image_location)\n            animation_list.append(FadeIn(image_mobject))\n\n        generation_animation = AnimationGroup(*animation_list[::-1], lag_ratio=1.0)\n        return generation_animation\n\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 5.0\nconfig.frame_width = 5.0\n\n\nclass DisentanglementScene(Scene):\n    \"\"\"Disentanglement Scene Object\"\"\"\n\n    def _construct_embedding(self, point_color=BLUE, dot_radius=0.05):\n        \"\"\"Makes a Gaussian-like embedding\"\"\"\n        embedding = VGroup()\n        # Sample points from a Gaussian\n        num_points = 200\n        standard_deviation = [0.6, 0.8]\n        mean = [0, 0]\n        points = np.random.normal(mean, standard_deviation, size=(num_points, 2))\n        # Make an axes\n        embedding.axes = Axes(\n            x_range=[-3, 3],\n            y_range=[-3, 3],\n            x_length=2.2,\n            y_length=2.2,\n            tips=False,\n        )\n        # Add each point to the axes\n        self.point_dots = VGroup()\n        for point in points:\n            point_location = embedding.axes.coords_to_point(*point)\n            dot = Dot(point_location, color=point_color, radius=dot_radius / 2)\n            self.point_dots.add(dot)\n\n        embedding.add(self.point_dots)\n        return embedding\n\n    def construct(self):\n        # Make the VAE decoder\n        vae_decoder = NeuralNetwork(\n            [\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n            ],\n            layer_spacing=0.55,\n        )\n\n        vae_decoder.shift([-0.55, 0, 0])\n        self.play(Create(vae_decoder), run_time=1)\n        # Make the embedding\n        embedding = self._construct_embedding()\n        embedding.scale(0.9)\n        embedding.move_to(vae_decoder.get_left())\n        embedding.shift([-0.85, 0, 0])\n        self.play(Create(embedding))\n        # Make disentanglment visualization\n        disentanglement = DisentanglementVisualization()\n        disentanglement_animation = (\n            disentanglement.make_disentanglement_generation_animation()\n        )\n        self.play(disentanglement_animation, run_time=3)\n        self.play(Wait(2))\n", "file_path": "examples/disentanglement/disentanglement.py"}
{"prompt": "Epsilon Nn Graph Epsilon Nn Graph", "response": "\"\"\"\n    Example where I draw an epsilon nearest neighbor graph animation\n\"\"\"\nfrom cProfile import label\nfrom manim import *\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import SpectralClustering\nimport numpy as np\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1200\nconfig.frame_height = 12.0\nconfig.frame_width = 12.0\n\n\ndef make_moon_points(num_samples=100, noise=0.1, random_seed=1):\n    \"\"\"Make two half moon point shapes\"\"\"\n    # Make sure the points are normalized\n    X, y = make_moons(n_samples=num_samples, noise=noise, random_state=random_seed)\n    X -= np.mean(X, axis=0)\n    X /= np.std(X, axis=0)\n    X[:, 1] += 0.3\n    # X[:, 0] /= 2 # squeeze width\n\n    return X\n\n\ndef make_epsilon_balls(epsilon_value, points, axes, ball_color=RED, opacity=0.0):\n    \"\"\"Draws epsilon balls\"\"\"\n    balls = []\n    for point in points:\n        ball = Circle(epsilon_value, color=ball_color, fill_opacity=opacity)\n        global_location = axes.coords_to_point(*point)\n        ball.move_to(global_location)\n        balls.append(ball)\n\n    return VGroup(*balls)\n\n\ndef make_epsilon_graph(epsilon_value, dots, points, edge_color=ORANGE):\n    \"\"\"Makes an epsilon nearest neighbor graph for the given dots\"\"\"\n    # First compute the adjacency matrix from the epsilon value and the points\n    num_dots = len(dots)\n    adjacency_matrix = np.zeros((num_dots, num_dots))\n    # Note: just doing lower triangular matrix\n    for i in range(num_dots):\n        for j in range(i):\n            dist = np.linalg.norm(dots[i].get_center() - dots[j].get_center())\n            is_connected = 1 if dist < epsilon_value else 0\n            adjacency_matrix[i, j] = is_connected\n    # Draw a graph based on the adjacency matrix\n    edges = []\n    for i in range(num_dots):\n        for j in range(i):\n            is_connected = adjacency_matrix[i, j]\n            if is_connected:\n                # Draw a connection between the corresponding dots\n                dot_a = dots[i]\n                dot_b = dots[j]\n                edge = Line(\n                    dot_a.get_center(),\n                    dot_b.get_center(),\n                    color=edge_color,\n                    stroke_width=3,\n                )\n                edges.append(edge)\n\n    return VGroup(*edges), adjacency_matrix\n\n\ndef perform_spectral_clustering(adjacency_matrix):\n    \"\"\"Performs spectral clustering given adjacency matrix\"\"\"\n    clustering = SpectralClustering(\n        n_clusters=2, affinity=\"precomputed\", random_state=0\n    ).fit(adjacency_matrix)\n    labels = clustering.labels_\n\n    return labels\n\n\ndef make_color_change_animation(labels, dots, colors=[ORANGE, GREEN]):\n    \"\"\"Makes a color change animation\"\"\"\n    anims = []\n\n    for index in range(len(labels)):\n        color = colors[labels[index]]\n        dot = dots[index]\n        anims.append(dot.animate.set_color(color))\n\n    return AnimationGroup(*anims, lag_ratio=0.0)\n\n\nclass EpsilonNearestNeighborScene(Scene):\n    def construct(\n        self,\n        num_points=200,\n        dot_radius=0.1,\n        dot_color=BLUE,\n        ball_color=WHITE,\n        noise=0.1,\n        ball_opacity=0.0,\n        random_seed=2,\n    ):\n        # Make moon shape points\n        # Note: dot is the drawing object and point is the math concept\n        moon_points = make_moon_points(\n            num_samples=num_points, noise=noise, random_seed=random_seed\n        )\n        # Make an axes\n        axes = Axes(\n            x_range=[-6, 6, 1],\n            y_range=[-6, 6, 1],\n            x_length=12,\n            y_length=12,\n            tips=False,\n            axis_config={\"stroke_color\": \"#000000\"},\n        )\n        axes.scale(2.2)\n        self.add(axes)\n        # Draw points\n        dots = []\n        for point in moon_points:\n            axes_location = axes.coords_to_point(*point)\n            dot = Dot(axes_location, color=dot_color, radius=dot_radius, z_index=1)\n            dots.append(dot)\n\n        dots = VGroup(*dots)\n        self.play(Create(dots))\n        # Draw epsilon bar with initial value\n        epsilon_bar = NumberLine(\n            [0, 2], length=8, stroke_width=2, include_ticks=False, include_numbers=False\n        )\n        epsilon_bar.shift(4.5 * DOWN)\n        self.play(Create(epsilon_bar))\n        current_epsilon = ValueTracker(0.3)\n        epsilon_point = epsilon_bar.number_to_point(current_epsilon.get_value())\n        epsilon_dot = Dot(epsilon_point)\n        self.add(epsilon_dot)\n        label_location = epsilon_bar.number_to_point(1.0)\n        label_location -= DOWN * 0.1\n        label_text = MathTex(\"\\epsilon\").scale(1.5)\n        # label_text = Text(\"Epsilon\")\n        label_text.move_to(epsilon_bar.get_center())\n        label_text.shift(DOWN * 0.5)\n        self.add(label_text)\n        # Make an updater for the dot\n        def dot_updater(epsilon_dot):\n            # Get location on epsilon_bar\n            point_loc = epsilon_bar.number_to_point(current_epsilon.get_value())\n            epsilon_dot.move_to(point_loc)\n\n        epsilon_dot.add_updater(dot_updater)\n        # Make the epsilon balls\n        epsilon_balls = make_epsilon_balls(\n            current_epsilon.get_value(),\n            moon_points,\n            axes,\n            ball_color=ball_color,\n            opacity=ball_opacity,\n        )\n        # Set up updater for radius of balls\n        def epsilon_balls_updater(epsilon_balls):\n            for ball in epsilon_balls:\n                ball.set_width(current_epsilon.get_value())\n\n        # Turn epsilon up and down\n        epsilon_balls.add_updater(epsilon_balls_updater)\n        # Fade in the initial balls\n        self.play(FadeIn(epsilon_balls), lag_ratio=0.0)\n        # Iterate through different values of epsilon\n        for value in [1.5, 0.5, 0.9]:\n            self.play(current_epsilon.animate.set_value(value), run_time=2.5)\n        # Perform clustering\n        epsilon_value = 0.9\n        # Show connecting graph\n        epsilon_graph, adjacency_matrix = make_epsilon_graph(\n            current_epsilon.get_value(), dots, moon_points, edge_color=WHITE\n        )\n        self.play(FadeOut(epsilon_balls))\n        self.play(FadeIn(epsilon_graph))\n        # Fade out balls\n        self.play(Wait(1.5))\n        # Perform clustering\n        labels = perform_spectral_clustering(adjacency_matrix)\n        # Change the colors of the dots\n        color_change_animation = make_color_change_animation(labels, dots)\n        self.play(color_change_animation)\n        # Fade out graph edges\n        self.play(FadeOut(epsilon_graph))\n        self.play(Wait(5.0))\n", "file_path": "examples/epsilon_nn_graph/epsilon_nn_graph.py"}
{"prompt": "Gan Gan", "response": "import random\nfrom pathlib import Path\n\nfrom PIL import Image\nfrom manim import *\nfrom manim_ml.neural_network.layers.embedding import EmbeddingLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.vector import VectorLayer\n\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\nconfig.pixel_height = 1080\nconfig.pixel_width = 1080\nconfig.frame_height = 8.3\nconfig.frame_width = 8.3\n\n\nclass GAN(Mobject):\n    \"\"\"Generative Adversarial Network\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.make_entities()\n        self.place_entities()\n        self.titles = self.make_titles()\n\n    def make_entities(self, image_height=1.2):\n        \"\"\"Makes all of the network entities\"\"\"\n        # Make the fake image layer\n        default_image = Image.open(ROOT_DIR / \"assets/gan/fake_image.png\")\n        numpy_image = np.asarray(default_image)\n        self.fake_image_layer = ImageLayer(\n            numpy_image, height=image_height, show_image_on_create=False\n        )\n        # Make the Generator Network\n        self.generator = NeuralNetwork(\n            [\n                EmbeddingLayer(covariance=np.array([[3.0, 0], [0, 3.0]])).scale(1.3),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                self.fake_image_layer,\n            ],\n            layer_spacing=0.1,\n        )\n\n        self.add(self.generator)\n        # Make the Discriminator\n        self.discriminator = NeuralNetwork(\n            [\n                FeedForwardLayer(5),\n                FeedForwardLayer(1),\n                VectorLayer(1, value_func=lambda: random.uniform(0, 1)),\n            ],\n            layer_spacing=0.1,\n        )\n        self.add(self.discriminator)\n        # Make Ground Truth Dataset\n        default_image = Image.open(ROOT_DIR / \"assets/gan/real_image.jpg\")\n        numpy_image = np.asarray(default_image)\n        self.ground_truth_layer = ImageLayer(numpy_image, height=image_height)\n        self.add(self.ground_truth_layer)\n\n        self.scale(1)\n\n    def place_entities(self):\n        \"\"\"Positions entities in correct places\"\"\"\n        # Place relative to generator\n        # Place the ground_truth image layer\n        self.ground_truth_layer.next_to(self.fake_image_layer, DOWN, 0.8)\n        # Group the images\n        image_group = Group(self.ground_truth_layer, self.fake_image_layer)\n        # Move the discriminator to the right of thee generator\n        self.discriminator.next_to(self.generator, RIGHT, 0.2)\n        self.discriminator.match_y(image_group)\n        # Move the discriminator to the height of the center of the image_group\n        # self.discriminator.match_y(image_group)\n        # self.ground_truth_layer.next_to(self.fake_image_layer, DOWN, 0.5)\n\n    def make_titles(self):\n        \"\"\"Makes titles for the different entities\"\"\"\n        titles = VGroup()\n\n        self.ground_truth_layer_title = Text(\"Real Image\").scale(0.3)\n        self.ground_truth_layer_title.next_to(self.ground_truth_layer, UP, 0.1)\n        self.add(self.ground_truth_layer_title)\n        titles.add(self.ground_truth_layer_title)\n        self.fake_image_layer_title = Text(\"Fake Image\").scale(0.3)\n        self.fake_image_layer_title.next_to(self.fake_image_layer, UP, 0.1)\n        self.add(self.fake_image_layer_title)\n        titles.add(self.fake_image_layer_title)\n        # Overhead title\n        overhead_title = Text(\"Generative Adversarial Network\").scale(0.75)\n        overhead_title.shift(np.array([0, 3.5, 0]))\n        titles.add(overhead_title)\n        # Probability title\n        self.probability_title = Text(\"Probability\").scale(0.5)\n        self.probability_title.move_to(self.discriminator.input_layers[-2])\n        self.probability_title.shift(UP)\n        self.probability_title.shift(RIGHT * 1.05)\n        titles.add(self.probability_title)\n\n        return titles\n\n    def make_highlight_generator_rectangle(self):\n        \"\"\"Returns animation that highlights the generators contents\"\"\"\n        group = VGroup()\n\n        generator_surrounding_group = Group(self.generator, self.fake_image_layer_title)\n\n        generator_surrounding_rectangle = SurroundingRectangle(\n            generator_surrounding_group, buff=0.1, stroke_width=4.0, color=\"#0FFF50\"\n        )\n        group.add(generator_surrounding_rectangle)\n        title = Text(\"Generator\").scale(0.5)\n        title.next_to(generator_surrounding_rectangle, UP, 0.2)\n        group.add(title)\n\n        return group\n\n    def make_highlight_discriminator_rectangle(self):\n        \"\"\"Makes a rectangle for highlighting the discriminator\"\"\"\n        discriminator_group = Group(\n            self.discriminator,\n            self.fake_image_layer,\n            self.ground_truth_layer,\n            self.fake_image_layer_title,\n            self.probability_title,\n        )\n\n        group = VGroup()\n\n        discriminator_surrounding_rectangle = SurroundingRectangle(\n            discriminator_group, buff=0.05, stroke_width=4.0, color=\"#0FFF50\"\n        )\n        group.add(discriminator_surrounding_rectangle)\n        title = Text(\"Discriminator\").scale(0.5)\n        title.next_to(discriminator_surrounding_rectangle, UP, 0.2)\n        group.add(title)\n\n        return group\n\n    def make_generator_forward_pass(self):\n        \"\"\"Makes forward pass of the generator\"\"\"\n\n        forward_pass = self.generator.make_forward_pass_animation(dist_theme=\"ellipse\")\n\n        return forward_pass\n\n    def make_discriminator_forward_pass(self):\n        \"\"\"Makes forward pass of the discriminator\"\"\"\n\n        disc_forward = self.discriminator.make_forward_pass_animation()\n\n        return disc_forward\n\n    @override_animation(Create)\n    def _create_override(self):\n        \"\"\"Overrides create\"\"\"\n        animation_group = AnimationGroup(\n            Create(self.generator),\n            Create(self.discriminator),\n            Create(self.ground_truth_layer),\n            Create(self.titles),\n        )\n        return animation_group\n\n\nclass GANScene(Scene):\n    \"\"\"GAN Scene\"\"\"\n\n    def construct(self):\n        gan = GAN().scale(1.70)\n        gan.move_to(ORIGIN)\n        gan.shift(DOWN * 0.35)\n        gan.shift(LEFT * 0.1)\n        self.play(Create(gan), run_time=3)\n        # Highlight generator\n        highlight_generator_rectangle = gan.make_highlight_generator_rectangle()\n        self.play(Create(highlight_generator_rectangle), run_time=1)\n        # Generator forward pass\n        gen_forward_pass = gan.make_generator_forward_pass()\n        self.play(gen_forward_pass, run_time=5)\n        # Fade out generator highlight\n        self.play(Uncreate(highlight_generator_rectangle), run_time=1)\n        # Highlight discriminator\n        highlight_discriminator_rectangle = gan.make_highlight_discriminator_rectangle()\n        self.play(Create(highlight_discriminator_rectangle), run_time=1)\n        # Discriminator forward pass\n        discriminator_forward_pass = gan.make_discriminator_forward_pass()\n        self.play(discriminator_forward_pass, run_time=5)\n        # Unhighlight discriminator\n        self.play(Uncreate(highlight_discriminator_rectangle), run_time=1)\n", "file_path": "examples/gan/gan.py"}
{"prompt": "Interpolation Interpolation", "response": "\"\"\"Visualization of VAE Interpolation\"\"\"\nimport sys\nimport os\n\nsys.path.append(os.environ[\"PROJECT_ROOT\"])\nfrom manim import *\nimport pickle\nimport numpy as np\nimport manim_ml.neural_network as neural_network\nimport examples.variational_autoencoder.variational_autoencoder as variational_autoencoder\n\n\"\"\"\n    The VAE Scene for the twitter video. \n\"\"\"\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n# Set random seed so point distribution is constant\nnp.random.seed(1)\n\n\nclass InterpolationScene(MovingCameraScene):\n    \"\"\"Scene object for a Variational Autoencoder and Autoencoder\"\"\"\n\n    def construct(self):\n        # Set Scene config\n        vae = variational_autoencoder.VariationalAutoencoder(\n            dot_radius=0.035, layer_spacing=0.5\n        )\n        vae.move_to(ORIGIN)\n        vae.encoder.shift(LEFT * 0.5)\n        vae.decoder.shift(RIGHT * 0.5)\n        mnist_image_handler = variational_autoencoder.MNISTImageHandler()\n        image_pair = mnist_image_handler.image_pairs[3]\n        # Make forward pass animation and DO NOT run it\n        forward_pass_animation = vae.make_forward_pass_animation(image_pair)\n        # Make the interpolation animation\n        interpolation_images = mnist_image_handler.interpolation_images\n        interpolation_animation = vae.make_interpolation_animation(interpolation_images)\n        embedding_zoom_animation = self.camera.auto_zoom(\n            [vae.embedding, vae.decoder, vae.output_image], margin=0.5\n        )\n        # Make animations\n        forward_pass_animations = []\n        for i in range(7):\n            anim = vae.decoder.make_forward_propagation_animation(run_time=0.5)\n            forward_pass_animations.append(anim)\n        forward_pass_animation_group = AnimationGroup(\n            *forward_pass_animations, lag_ratio=1.0\n        )\n        # Make forward pass animations\n        self.play(Create(vae), run_time=1.5)\n        self.play(FadeOut(vae.encoder), run_time=1.0)\n        self.play(embedding_zoom_animation, run_time=1.5)\n        interpolation_animation = AnimationGroup(\n            forward_pass_animation_group, interpolation_animation\n        )\n        self.play(interpolation_animation, run_time=9.0)\n", "file_path": "examples/interpolation/interpolation.py"}
{"prompt": "Lenet Lenet", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 20.0\nconfig.frame_width = 20.0\n\nROOT_DIR = Path(__file__).parents[2]\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=4.5),\n                Convolutional2DLayer(1, 28),\n                Convolutional2DLayer(6, 28, 5),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(16, 10, 5),\n                MaxPooling2DLayer(kernel_size=2),\n                FeedForwardLayer(8),\n                FeedForwardLayer(3),\n                FeedForwardLayer(2),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # Group it all\n        # group = Group(nn, code)\n        # group.move_to(ORIGIN)\n        nn.move_to(ORIGIN)\n        # Play animation\n        # forward_pass = nn.make_forward_pass_animation()\n        # self.wait(1)\n        # self.play(forward_pass)\n", "file_path": "examples/lenet/lenet.py"}
{"prompt": "Logo Logo", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\n\nimport manim_ml\nmanim_ml.config.color_scheme = \"light_mode\"\n\nfrom manim_ml.neural_network.architectures.feed_forward import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 1000\nconfig.pixel_width = 1000\nconfig.frame_height = 4.0\nconfig.frame_width = 4.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.text = Text(\"ManimML\", color=manim_ml.config.color_scheme.text_color)\n        self.text.scale(1.0)\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 6, 3], layer_spacing=0.3, node_color=BLUE\n        )\n        self.neural_network.scale(1.0)\n        self.neural_network.move_to(self.text.get_bottom())\n        self.neural_network.shift(1.25 * DOWN)\n        self.logo_group = Group(self.text, self.neural_network)\n        self.logo_group.scale(1.0)\n        self.logo_group.move_to(ORIGIN)\n        self.play(Write(self.text), run_time=1.0)\n        self.play(Create(self.neural_network), run_time=3.0)\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            # Create(self.surrounding_rectangle)\n        )\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            #    Create(self.surrounding_rectangle)\n        )\n        self.play(animation_group, runtime=5.0)\n        self.wait(5)\n", "file_path": "examples/logo/logo.py"}
{"prompt": "Logo Website Logo", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\nfrom manim_ml.neural_network.neural_network import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 400\nconfig.pixel_width = 600\nconfig.frame_height = 8.0\nconfig.frame_width = 10.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 5], layer_spacing=0.6, node_color=BLUE, edge_width=6\n        )\n        self.neural_network.scale(3)\n        self.neural_network.move_to(ORIGIN)\n        self.play(Create(self.neural_network))\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n        )\n        self.play(animation_group)\n        self.wait(5)\n", "file_path": "examples/logo/website_logo.py"}
{"prompt": "Logo Wide Logo", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\n\nimport manim_ml\nmanim_ml.config.color_scheme = \"light_mode\"\n\nfrom manim_ml.neural_network.architectures.feed_forward import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 1000\nconfig.pixel_width = 2000\nconfig.frame_height = 4.0\nconfig.frame_width = 8.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.text = Text(\"ManimML\", color=manim_ml.config.color_scheme.text_color)\n        self.text.scale(1.0)\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 6, 3], layer_spacing=0.3, node_color=BLUE\n        )\n        self.neural_network.scale(0.8)\n        self.neural_network.next_to(self.text, RIGHT, buff=0.5)\n        # self.neural_network.move_to(self.text.get_right())\n        # self.neural_network.shift(1.25 * DOWN)\n        self.logo_group = Group(self.text, self.neural_network)\n        self.logo_group.scale(1.0)\n        self.logo_group.move_to(ORIGIN)\n        self.play(Write(self.text), run_time=1.0)\n        self.play(Create(self.neural_network), run_time=3.0)\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            # Create(self.surrounding_rectangle)\n        )\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            #    Create(self.surrounding_rectangle)\n        )\n        self.play(animation_group, runtime=5.0)\n        self.wait(5)\n", "file_path": "examples/logo/wide_logo.py"}
{"prompt": "Mcmc Warmup Mcmc", "response": "from manim import *\n\nimport scipy.stats\nfrom manim_ml.diffusion.mcmc import MCMCAxes\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use('dark_background')\n\n# Make the specific scene\nconfig.pixel_height = 720\nconfig.pixel_width = 720\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\nclass MCMCWarmupScene(Scene):\n\n    def construct(self):\n        # Define the Gaussian Mixture likelihood\n        def gaussian_mm_logpdf(x):\n            \"\"\"Gaussian Mixture Model Log PDF\"\"\"\n            # Choose two arbitrary Gaussians\n            # Big Gaussian\n            big_gaussian_pdf = scipy.stats.multivariate_normal(\n                mean=[-0.5, -0.5],\n                cov=[1.0, 1.0]\n            ).pdf(x)\n            # Little Gaussian\n            little_gaussian_pdf = scipy.stats.multivariate_normal(\n                mean=[2.3, 1.9],\n                cov=[0.3, 0.3]\n            ).pdf(x)\n            # Sum their likelihoods and take the log\n            logpdf = np.log(big_gaussian_pdf + little_gaussian_pdf)\n\n            return logpdf\n\n        # Generate a bunch of true samples\n        true_samples = []\n        # Generate samples for little gaussian\n        little_gaussian_samples = np.random.multivariate_normal(\n            mean=[2.3, 1.9],\n            cov=[[0.3, 0.0], [0.0, 0.3]],\n            size=(10000)\n        )\n        big_gaussian_samples = np.random.multivariate_normal(\n            mean=[-0.5, -0.5],\n            cov=[[1.0, 0.0], [0.0, 1.0]],\n            size=(10000)\n        )\n        true_samples = np.concatenate((little_gaussian_samples, big_gaussian_samples))\n        # Make the MCMC axes\n        axes = MCMCAxes(\n            x_range=[-5, 5],\n            y_range=[-5, 5],\n            x_length=7.0,\n            y_length=7.0\n        )\n        axes.move_to(ORIGIN)\n        self.play(\n            Create(axes)\n        )\n        # Make the chain sampling animation \n        chain_sampling_animation = axes.visualize_metropolis_hastings_chain_sampling(\n            log_prob_fn=gaussian_mm_logpdf, \n            true_samples=true_samples,\n            sampling_kwargs={\n                \"iterations\": 2000,\n                \"warm_up\": 50,\n                \"initial_location\": np.array([-3.5, 3.5]),\n                \"sampling_seed\": 4\n            },\n        )\n        self.play(chain_sampling_animation)\n        self.wait(3)\n", "file_path": "examples/mcmc/warmup_mcmc.py"}
{"prompt": "Neocognitron Neocognitron", "response": "from manim import *\n\nfrom manim_ml.neural_network import NeuralNetwork\nfrom manim_ml.neural_network.layers.parent_layers import NeuralNetworkLayer, ConnectiveLayer, ThreeDLayer\nimport manim_ml\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 10.5\nconfig.frame_width = 10.5\n\nclass NeocognitronFilter(VGroup):\n    \"\"\"\n        Filter connecting the S and C Cells of a neocognitron layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_cell, \n        output_cell, \n        receptive_field_radius, \n        outline_color=BLUE,\n        active_color=ORANGE, \n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.outline_color = outline_color\n        self.active_color = active_color\n        self.input_cell = input_cell\n        self.output_cell = output_cell\n        # Draw the receptive field\n        # Make the points of a equilateral triangle in the plane of the \n        # cell about a random center\n        # center_point = input_cell.get_center()\n        shift_amount_x = np.random.uniform(\n            -(input_cell.get_height()/2 - receptive_field_radius - 0.01),\n            input_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        shift_amount_y = np.random.uniform(\n            -(input_cell.get_height()/2 - receptive_field_radius - 0.01),\n            input_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        # center_point += np.array([shift_amount_x, shift_amount_y, 0])\n        # # Make the triangle points\n        # side_length = np.sqrt(3) * receptive_field_radius\n        # normal_vector = np.cross(\n        #     input_cell.get_left() - input_cell.get_center(),\n        #     input_cell.get_top() - input_cell.get_center(),\n        # )\n        # Get vector in the plane of the cell\n        # vector_in_plane = input_cell.get_left() - input_cell.get_center()\n        # point_a = center_point + vector_in_plane * receptive_field_radius\n        # # rotate the vector 120 degrees about the vector normal to the cell\n        # vector_in_plane = rotate_vector(vector_in_plane, PI/3, normal_vector)\n        # point_b = center_point + vector_in_plane * receptive_field_radius\n        # # rotate the vector 120 degrees about the vector normal to the cell\n        # vector_in_plane = rotate_vector(vector_in_plane, PI/3, normal_vector)\n        # point_c = center_point + vector_in_plane * receptive_field_radius\n        # self.receptive_field = Circle.from_three_points(\n        #     point_a, \n        #     point_b,\n        #     point_c,\n        #     color=outline_color,\n        #     stroke_width=2.0,\n        # )\n        self.receptive_field = Circle(\n            radius=receptive_field_radius,\n            color=outline_color,\n            stroke_width=3.0,\n        )\n        self.add(self.receptive_field)\n        # Move receptive field to a random point within the cell\n        # minus the radius of the receptive field\n        self.receptive_field.move_to(\n            input_cell\n        )\n        # Shift a random amount in the x and y direction within \n        self.receptive_field.shift(\n            np.array([shift_amount_x, shift_amount_y, 0])\n        )\n        # Choose a random point on the c_cell\n        shift_amount_x = np.random.uniform(\n            -(output_cell.get_height()/2 - receptive_field_radius - 0.01),\n            output_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        shift_amount_y = np.random.uniform(\n            -(output_cell.get_height()/2 - receptive_field_radius - 0.01),\n            output_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        self.dot = Dot(\n            color=outline_color,\n            radius=0.04\n        )\n        self.dot.move_to(output_cell.get_center() + np.array([shift_amount_x, shift_amount_y, 0]))\n        self.add(self.dot)\n        # Make lines connecting receptive field to the dot\n        self.lines = VGroup()\n        self.lines.add(\n            Line(\n                self.receptive_field.get_center() + np.array([0, receptive_field_radius, 0]),\n                self.dot,\n                color=outline_color,\n                stroke_width=3.0,\n            )\n        )\n        self.lines.add(\n            Line(\n                self.receptive_field.get_center() - np.array([0, receptive_field_radius, 0]),\n                self.dot,\n                color=outline_color,\n                stroke_width=3.0,\n            )\n        )\n        self.add(self.lines)\n\n    def make_filter_pulse_animation(self, **kwargs):\n        succession = Succession(\n            ApplyMethod(\n                self.receptive_field.set_color, \n                self.active_color, \n                run_time=0.25\n            ),\n            ApplyMethod(\n                self.receptive_field.set_color, \n                self.outline_color, \n                run_time=0.25\n            ),\n            ShowPassingFlash(\n                self.lines.copy().set_color(self.active_color),\n                time_width=0.5,\n            ),\n            ApplyMethod(\n                self.dot.set_color, \n                self.active_color, \n                run_time=0.25\n            ),\n            ApplyMethod(\n                self.dot.set_color, \n                self.outline_color, \n                run_time=0.25\n            ),\n        )\n\n        return succession\n\nclass NeocognitronLayer(NeuralNetworkLayer, ThreeDLayer):\n    \n    def __init__(\n        self, \n        num_cells, \n        cell_height,\n        receptive_field_radius,\n        layer_name,\n        active_color=manim_ml.config.color_scheme.active_color,\n        cell_color=manim_ml.config.color_scheme.secondary_color,\n        outline_color=manim_ml.config.color_scheme.primary_color,\n        show_outline=True,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.num_cells = num_cells\n        self.cell_height = cell_height\n        self.receptive_field_radius = receptive_field_radius\n        self.layer_name = layer_name\n        self.active_color = active_color\n        self.cell_color = cell_color\n        self.outline_color = outline_color\n        self.show_outline = show_outline\n\n        self.plane_label = Text(layer_name).scale(0.4)\n\n    def make_cell_plane(self, layer_name, cell_buffer=0.1, stroke_width=2.0):\n        \"\"\"Makes a plane of cells. \n        \"\"\"\n        cell_plane = VGroup()\n        for cell_index in range(self.num_cells):\n            # Draw the cell box\n            cell_box = Rectangle(\n                width=self.cell_height,\n                height=self.cell_height,\n                color=self.cell_color,\n                fill_color=self.cell_color,\n                fill_opacity=0.3,\n                stroke_width=stroke_width,\n            )\n            if cell_index > 0:\n                cell_box.next_to(cell_plane[-1], DOWN, buff=cell_buffer)\n            cell_plane.add(cell_box)\n        # Draw the outline\n        if self.show_outline:\n            self.plane_outline = SurroundingRectangle(\n                cell_plane,\n                color=self.cell_color,\n                buff=cell_buffer,\n                stroke_width=stroke_width,\n            )\n            cell_plane.add(\n                self.plane_outline\n            )\n        # Draw a label above the container box\n        self.plane_label.next_to(cell_plane, UP, buff=0.2)\n        cell_plane.add(self.plane_label)\n\n        return cell_plane\n\n    def construct_layer(self, input_layer, output_layer, **kwargs):\n        # Make the Cell layer\n        self.cell_plane = self.make_cell_plane(self.layer_name)\n        self.add(self.cell_plane)\n        \n    def make_forward_pass_animation(self, layer_args={}, **kwargs):\n        \"\"\"Forward pass for query\"\"\"\n        # # Pulse and un pulse the cell plane rectangle\n        flash_outline_animations = []\n        for cell in self.cell_plane:\n            flash_outline_animations.append(\n                Succession(\n                    ApplyMethod(\n                        cell.set_stroke_color,\n                        self.active_color,\n                        run_time=0.25\n                    ),\n                    ApplyMethod(\n                        cell.set_stroke_color,\n                        self.outline_color,\n                        run_time=0.25\n                    )\n                )\n            )\n        \n        return AnimationGroup(\n            *flash_outline_animations,\n            lag_ratio=0.0\n        )\n\nclass NeocognitronToNeocognitronLayer(ConnectiveLayer):\n    input_class = NeocognitronLayer\n    output_class = NeocognitronLayer\n\n    def __init__(self, input_layer, output_layer, **kwargs):\n        super().__init__(input_layer, output_layer, **kwargs)\n\n    def construct_layer(self, input_layer, output_layer, **kwargs):\n        self.filters = []\n        for cell_index in range(input_layer.num_cells):\n            input_cell = input_layer.cell_plane[cell_index]\n            # Randomly choose a cell from the output layer\n            output_cell = output_layer.cell_plane[\n                np.random.randint(0, output_layer.num_cells)\n            ]\n            # Make the filter\n            filter = NeocognitronFilter(\n                input_cell=input_cell,\n                output_cell=output_cell,\n                receptive_field_radius=input_layer.receptive_field_radius,\n                outline_color=self.input_layer.outline_color\n            )\n            # filter = NeocognitronFilter(\n            #     outline_color=input_layer.outline_color\n            # )\n            self.filters.append(filter)\n\n        self.add(VGroup(*self.filters))\n\n    def make_forward_pass_animation(self, layer_args={}, **kwargs):\n        \"\"\"Forward pass for query\"\"\"\n        filter_pulses = []\n        for filter in self.filters:\n            filter_pulses.append(\n                filter.make_filter_pulse_animation()\n            )\n        return AnimationGroup(\n            *filter_pulses\n        )\n\nmanim_ml.neural_network.layers.util.register_custom_connective_layer(\n    NeocognitronToNeocognitronLayer,\n)\n\nclass Neocognitron(NeuralNetwork):\n\n    def __init__(\n        self, \n        camera,\n        cells_per_layer=[4, 5, 4, 4, 3, 3, 5, 5],\n        cell_heights=[0.8, 0.8, 0.8*0.75, 0.8*0.75, 0.8*0.75**2, 0.8*0.75**2, 0.8*0.75**3, 0.02],\n        layer_names=[\"S1\", \"C1\", \"S2\", \"C2\", \"S3\", \"C3\", \"S4\", \"C4\"],\n        receptive_field_sizes=[0.2, 0.2, 0.2*0.75, 0.2*0.75, 0.2*0.75**2, 0.2*0.75**2, 0.2*0.75**3, 0.0],\n    ):\n        self.cells_per_layer = cells_per_layer\n        self.cell_heights = cell_heights\n        self.layer_names = layer_names\n        self.receptive_field_sizes = receptive_field_sizes\n        # Make the neo-cognitron input layer\n        input_layers = []\n        input_layers.append(\n            NeocognitronLayer(\n                1,\n                1.5,\n                0.2,\n                \"U0\",\n                show_outline=False,\n            )\n        )\n        # Make the neo-cognitron layers\n        for i in range(len(cells_per_layer)):\n            layer = NeocognitronLayer(\n                cells_per_layer[i],\n                cell_heights[i],\n                receptive_field_sizes[i],\n                layer_names[i]\n            )\n            input_layers.append(layer)\n        \n        # Make all of the layer labels fixed in frame\n        for layer in input_layers:\n            if isinstance(layer, NeocognitronLayer):\n                # camera.add_fixed_orientation_mobjects(layer.plane_label)\n                pass\n\n        all_layers = []\n        for layer_index in range(len(input_layers) - 1):\n            input_layer = input_layers[layer_index]\n            all_layers.append(input_layer)\n            output_layer = input_layers[layer_index + 1]\n            connective_layer = NeocognitronToNeocognitronLayer(\n                input_layer,\n                output_layer\n            )\n            all_layers.append(connective_layer)\n        all_layers.append(input_layers[-1])\n\n        def custom_layout_func(neural_network):\n            # Insert the connective layers\n            # Pass the layers to neural network super class\n            # Rotate pairs of layers\n            layer_index = 1\n            while layer_index < len(input_layers):\n                prev_layer = input_layers[layer_index - 1]\n                s_layer = input_layers[layer_index]\n                s_layer.next_to(prev_layer, RIGHT, buff=0.0)\n                s_layer.shift(RIGHT * 0.4)\n                c_layer = input_layers[layer_index + 1]\n                c_layer.next_to(s_layer, RIGHT, buff=0.0)\n                c_layer.shift(RIGHT * 0.2)\n                # Rotate the pair of layers\n                group = Group(s_layer, c_layer)\n                group.move_to(np.array([group.get_center()[0], 0, 0]))\n\n                # group.shift(layer_index * 3 * np.array([0, 0, 1.0]))\n                # group.rotate(40 * DEGREES, axis=UP, about_point=group.get_center())\n                # c_layer.rotate(40*DEGREES, axis=UP, about_point=group.get_center())\n                # s_layer.shift(\n                #     layer_index // 2 * np.array([0, 0, 0.3])\n                # )\n                # c_layer.shift(\n                #     layer_index // 2 * np.array([0, 0, 0.3])\n                # )\n                layer_index += 2\n            neural_network.move_to(ORIGIN)\n\n        super().__init__(\n            all_layers,\n            layer_spacing=0.5,\n            custom_layout_func=custom_layout_func\n        )\n\nclass Scene(ThreeDScene):\n\n    def construct(self):\n        neocognitron = Neocognitron(self.camera)\n        neocognitron.shift(DOWN*0.5)\n        self.add(neocognitron)\n        title = Text(\"Neocognitron\").scale(1)\n        self.add_fixed_in_frame_mobjects(title)\n        title.next_to(neocognitron, UP, buff=0.4)\n        self.add(title)\n        \"\"\"\n        self.play(\n            neocognitron.make_forward_pass_animation()\n        )\n        \"\"\"\n        print(self.camera.gamma)\n        print(self.camera.theta)\n        print(self.camera.phi)\n        neocognitron.rotate(90*DEGREES, axis=RIGHT)\n        neocognitron.shift(np.array([0, 0, -0.2]))\n        # title.rotate(90*DEGREES, axis=RIGHT)\n        # self.set_camera_orientation(phi=-15*DEGREES)\n        # vec = np.array([0.0, 0.2, 0.0])\n        # vec /= np.linalg.norm(vec)\n        # x, y, z = vec[0], vec[1], vec[2]\n        # theta = np.arccos(z)\n        # phi = np.arctan(y / x)\n        self.set_camera_orientation(phi=90 * DEGREES, theta=-70*DEGREES, gamma=0*DEGREES)\n        # self.set_camera_orientation(theta=theta, phi=phi)\n\n        forward_pass = neocognitron.make_forward_pass_animation()\n        self.play(forward_pass)\n\n", "file_path": "examples/neocognitron/neocognitron.py"}
{"prompt": "Paper Visualizations Oracle Guidance Oracle Guidance", "response": "\"\"\"\n    Here is a animated explanatory figure for the \"Oracle Guided Image Synthesis with Relative Queries\" paper. \n\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\nfrom manim_ml.neural_network.layers import triplet\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.paired_query import PairedQueryLayer\nfrom manim_ml.neural_network.layers.triplet import TripletLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom manim_ml.neural_network.layers import FeedForwardLayer, EmbeddingLayer\nfrom manim_ml.neural_network.layers.util import get_connective_layer\nimport os\n\nfrom manim_ml.utils.mobjects.probability import GaussianDistribution\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\nROOT_DIR = Path(__file__).parents[3]\n\n\nclass Localizer:\n    \"\"\"\n    Holds the localizer object, which contains the queries, images, etc.\n    needed to represent a localization run.\n    \"\"\"\n\n    def __init__(self, axes):\n        # Set dummy values for these\n        self.index = -1\n        self.axes = axes\n        self.num_queries = 3\n        self.assets_path = ROOT_DIR / \"assets/oracle_guidance\"\n        self.ground_truth_image_path = self.assets_path / \"ground_truth.jpg\"\n        self.ground_truth_location = np.array([2, 3])\n        # Prior distribution\n        print(\"initial gaussian\")\n        self.prior_distribution = GaussianDistribution(\n            self.axes,\n            mean=np.array([0.0, 0.0]),\n            cov=np.array([[3, 0], [0, 3]]),\n            dist_theme=\"ellipse\",\n            color=GREEN,\n        )\n        # Define the query images and embedded locations\n        # Contains image paths [(positive_path, negative_path), ...]\n        self.query_image_paths = [\n            (\n                os.path.join(self.assets_path, \"positive_1.jpg\"),\n                os.path.join(self.assets_path, \"negative_1.jpg\"),\n            ),\n            (\n                os.path.join(self.assets_path, \"positive_2.jpg\"),\n                os.path.join(self.assets_path, \"negative_2.jpg\"),\n            ),\n            (\n                os.path.join(self.assets_path, \"positive_3.jpg\"),\n                os.path.join(self.assets_path, \"negative_3.jpg\"),\n            ),\n        ]\n        # Contains 2D locations for each image [([2, 3], [2, 4]), ...]\n        self.query_locations = [\n            (np.array([-1, -1]), np.array([1, 1])),\n            (np.array([1, -1]), np.array([-1, 1])),\n            (np.array([0.3, -0.6]), np.array([-0.5, 0.7])),\n        ]\n        # Make the covariances for each query\n        self.query_covariances = [\n            (np.array([[0.3, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n            (np.array([[0.2, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n            (np.array([[0.2, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n        ]\n        # Posterior distributions over time GaussianDistribution objects\n        self.posterior_distributions = [\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([-0.3, -0.3]),\n                cov=np.array([[5, -4], [-4, 6]]),\n            ).scale(0.6),\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([0.25, -0.25]),\n                cov=np.array([[3, -2], [-2, 4]]),\n            ).scale(0.35),\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([0.4, -0.35]),\n                cov=np.array([[1, 0], [0, 1]]),\n            ).scale(0.3),\n        ]\n        # Some assumptions\n        assert len(self.query_locations) == len(self.query_image_paths)\n        assert len(self.query_locations) == len(self.posterior_distributions)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"Steps through each localization time instance\"\"\"\n        if self.index < len(self.query_image_paths):\n            self.index += 1\n        else:\n            raise StopIteration\n\n        # Return query_paths, query_locations, posterior\n        out_tuple = (\n            self.query_image_paths[self.index],\n            self.query_locations[self.index],\n            self.posterior_distributions[self.index],\n            self.query_covariances[self.index],\n        )\n\n        return out_tuple\n\n\nclass OracleGuidanceVisualization(Scene):\n    def __init__(self):\n        super().__init__()\n        self.neural_network, self.embedding_layer = self.make_vae()\n        self.localizer = iter(Localizer(self.embedding_layer.axes))\n        self.subtitle = None\n        self.title = None\n        # Set image paths\n        # VAE embedding animation image paths\n        self.assets_path = ROOT_DIR / \"assets/oracle_guidance\"\n        self.input_embed_image_path = os.path.join(self.assets_path, \"input_image.jpg\")\n        self.output_embed_image_path = os.path.join(\n            self.assets_path, \"output_image.jpg\"\n        )\n\n    def make_vae(self):\n        \"\"\"Makes a simple VAE architecture\"\"\"\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\")\n        self.encoder = NeuralNetwork(\n            [\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n            ]\n        )\n\n        self.decoder = NeuralNetwork(\n            [\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n            ]\n        )\n\n        neural_network = NeuralNetwork([self.encoder, self.decoder])\n\n        neural_network.shift(DOWN * 0.4)\n        return neural_network, embedding_layer\n\n    @override_animation(Create)\n    def _create_animation(self):\n        animation_group = AnimationGroup(Create(self.neural_network))\n\n        return animation_group\n\n    def insert_at_start(self, layer, create=True):\n        \"\"\"Inserts a layer at the beggining of the network\"\"\"\n        # Note: Does not move the rest of the network\n        current_first_layer = self.encoder.all_layers[0]\n        # Get connective layer\n        connective_layer = get_connective_layer(layer, current_first_layer)\n        # Insert both layers\n        self.encoder.all_layers.insert(0, layer)\n        self.encoder.all_layers.insert(1, connective_layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        layer.shift(DOWN * 0.4)\n        layer.shift(LEFT * 2.35)\n        # Make insert animation\n        if not create:\n            animation_group = AnimationGroup(Create(connective_layer))\n        else:\n            animation_group = AnimationGroup(Create(layer), Create(connective_layer))\n        self.play(animation_group)\n\n    def remove_start_layer(self):\n        \"\"\"Removes the first layer of the network\"\"\"\n        first_layer = self.encoder.all_layers.remove_at_index(0)\n        first_connective = self.encoder.all_layers.remove_at_index(0)\n        # Make remove animations\n        animation_group = AnimationGroup(\n            FadeOut(first_layer), FadeOut(first_connective)\n        )\n\n        self.play(animation_group)\n\n    def insert_at_end(self, layer):\n        \"\"\"Inserts the given layer at the end of the network\"\"\"\n        current_last_layer = self.decoder.all_layers[-1]\n        # Get connective layer\n        connective_layer = get_connective_layer(current_last_layer, layer)\n        # Insert both layers\n        self.decoder.all_layers.add(connective_layer)\n        self.decoder.all_layers.add(layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        layer.shift(DOWN * 0.4)\n        layer.shift(RIGHT * 2.35)\n        # Make insert animation\n        animation_group = AnimationGroup(Create(layer), Create(connective_layer))\n        self.play(animation_group)\n\n    def remove_end_layer(self):\n        \"\"\"Removes the lsat layer of the network\"\"\"\n        first_layer = self.decoder.all_layers.remove_at_index(-1)\n        first_connective = self.decoder.all_layers.remove_at_index(-1)\n        # Make remove animations\n        animation_group = AnimationGroup(\n            FadeOut(first_layer), FadeOut(first_connective)\n        )\n\n        self.play(animation_group)\n\n    def change_title(self, text, title_location=np.array([0, 1.25, 0]), font_size=24):\n        \"\"\"Changes title to the given text\"\"\"\n        if self.title is None:\n            self.title = Text(text, font_size=font_size)\n            self.title.move_to(title_location)\n            self.add(self.title)\n            self.play(Write(self.title), run_time=1)\n            self.wait(1)\n            return\n\n        self.play(Unwrite(self.title))\n        new_title = Text(text, font_size=font_size)\n        new_title.move_to(self.title)\n        self.title = new_title\n        self.wait(0.1)\n        self.play(Write(self.title))\n\n    def change_subtitle(self, text, title_location=np.array([0, 0.9, 0]), font_size=20):\n        \"\"\"Changes subtitle to the next algorithm step\"\"\"\n        if self.subtitle is None:\n            self.subtitle = Text(text, font_size=font_size)\n            self.subtitle.move_to(title_location)\n            self.play(Write(self.subtitle))\n            return\n\n        self.play(Unwrite(self.subtitle))\n        new_title = Text(text, font_size=font_size)\n        new_title.move_to(title_location)\n        self.subtitle = new_title\n        self.wait(0.1)\n        self.play(Write(self.subtitle))\n\n    def make_embed_input_image_animation(self, input_image_path, output_image_path):\n        \"\"\"Makes embed input image animation\"\"\"\n        # insert the input image at the begginging\n        input_image_layer = ImageLayer.from_path(input_image_path)\n        input_image_layer.scale(0.6)\n        current_first_layer = self.encoder.all_layers[0]\n        # Get connective layer\n        connective_layer = get_connective_layer(input_image_layer, current_first_layer)\n        # Insert both layers\n        self.encoder.all_layers.insert(0, input_image_layer)\n        self.encoder.all_layers.insert(1, connective_layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        input_image_layer.shift(DOWN * 0.4)\n        input_image_layer.shift(LEFT * 2.35)\n        # Play full forward pass\n        forward_pass = self.neural_network.make_forward_pass_animation(\n            layer_args={\n                self.encoder: {\n                    self.embedding_layer: {\n                        \"dist_args\": {\n                            \"cov\": np.array([[1.5, 0], [0, 1.5]]),\n                            \"mean\": np.array([0.5, 0.5]),\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": ORANGE,\n                        }\n                    }\n                }\n            }\n        )\n        self.play(forward_pass)\n        # insert the output image at the end\n        output_image_layer = ImageLayer.from_path(output_image_path)\n        output_image_layer.scale(0.6)\n        self.insert_at_end(output_image_layer)\n        # Remove the input and output layers\n        self.remove_start_layer()\n        self.remove_end_layer()\n        # Remove the latent distribution\n        self.play(FadeOut(self.embedding_layer.latent_distribution))\n\n    def make_localization_time_step(self, old_posterior):\n        \"\"\"\n        Performs one query update for the localization procedure\n\n        Procedure:\n        a. Embed query input images\n        b. Oracle is asked a query\n        c. Query is embedded\n        d. Show posterior update\n        e. Show current recomendation\n        \"\"\"\n        # Helper functions\n        def embed_query_to_latent_space(query_locations, query_covariance):\n            \"\"\"Makes animation for a paired query\"\"\"\n            # Assumes first layer of neural network is a PairedQueryLayer\n            # Make the embedding animation\n            # Wait\n            self.play(Wait(1))\n            # Embed the query to latent space\n            self.change_subtitle(\"3. Embed the Query in Latent Space\")\n            # Make forward pass animation\n            self.embedding_layer.paired_query_mode = True\n            # Make embedding embed query animation\n            embed_query_animation = self.encoder.make_forward_pass_animation(\n                run_time=5,\n                layer_args={\n                    self.embedding_layer: {\n                        \"positive_dist_args\": {\n                            \"cov\": query_covariance[0],\n                            \"mean\": query_locations[0],\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": BLUE,\n                        },\n                        \"negative_dist_args\": {\n                            \"cov\": query_covariance[1],\n                            \"mean\": query_locations[1],\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": RED,\n                        },\n                    }\n                },\n            )\n            self.play(embed_query_animation)\n\n        # Access localizer information\n        query_paths, query_locations, posterior_distribution, query_covariances = next(\n            self.localizer\n        )\n        positive_path, negative_path = query_paths\n        # Make subtitle for present user with query\n        self.change_subtitle(\"2. Present User with Query\")\n        # Insert the layer into the encoder\n        query_layer = PairedQueryLayer.from_paths(\n            positive_path, negative_path, grayscale=False\n        )\n        query_layer.scale(0.5)\n        self.insert_at_start(query_layer)\n        # Embed query to latent space\n        query_to_latent_space_animation = embed_query_to_latent_space(\n            query_locations, query_covariances\n        )\n        # Wait\n        self.play(Wait(1))\n        # Update the posterior\n        self.change_subtitle(\"4. Update the Posterior\")\n        # Remove the old posterior\n        self.play(ReplacementTransform(old_posterior, posterior_distribution))\n        \"\"\"\n        self.play(\n            self.embedding_layer.remove_gaussian_distribution(self.localizer.posterior_distribution)\n        )\n        \"\"\"\n        # self.embedding_layer.add_gaussian_distribution(posterior_distribution)\n        # self.localizer.posterior_distribution = posterior_distribution\n        # Remove query layer\n        self.remove_start_layer()\n        # Remove query ellipses\n\n        fade_outs = []\n        for dist in self.embedding_layer.gaussian_distributions:\n            self.embedding_layer.gaussian_distributions.remove(dist)\n            fade_outs.append(FadeOut(dist))\n\n        if not len(fade_outs) == 0:\n            fade_outs = AnimationGroup(*fade_outs)\n            self.play(fade_outs)\n\n        return posterior_distribution\n\n    def make_generate_estimate_animation(self, estimate_image_path):\n        \"\"\"Makes the generate estimate animation\"\"\"\n        # Change embedding layer mode\n        self.embedding_layer.paired_query_mode = False\n        # Sample from posterior distribution\n        # self.embedding_layer.latent_distribution = self.localizer.posterior_distribution\n        emb_to_ff_ind = self.neural_network.all_layers.index_of(self.encoder)\n        embedding_to_ff = self.neural_network.all_layers[emb_to_ff_ind + 1]\n        self.play(embedding_to_ff.make_forward_pass_animation())\n        # Pass through decoder\n        self.play(self.decoder.make_forward_pass_animation(), run_time=1)\n        # Create Image layer after the decoder\n        output_image_layer = ImageLayer.from_path(estimate_image_path)\n        output_image_layer.scale(0.5)\n        self.insert_at_end(output_image_layer)\n        # Wait\n        self.wait(1)\n        # Remove the image at the end\n        print(self.neural_network)\n        self.remove_end_layer()\n\n    def make_triplet_forward_animation(self):\n        \"\"\"Make triplet forward animation\"\"\"\n        # Make triplet layer\n        anchor_path = os.path.join(self.assets_path, \"anchor.jpg\")\n        positive_path = os.path.join(self.assets_path, \"positive.jpg\")\n        negative_path = os.path.join(self.assets_path, \"negative.jpg\")\n        triplet_layer = TripletLayer.from_paths(\n            anchor_path,\n            positive_path,\n            negative_path,\n            grayscale=False,\n            font_size=100,\n            buff=1.05,\n        )\n        triplet_layer.scale(0.10)\n        self.insert_at_start(triplet_layer)\n        # Make latent triplet animation\n        self.play(\n            self.encoder.make_forward_pass_animation(\n                layer_args={\n                    self.embedding_layer: {\n                        \"triplet_args\": {\n                            \"anchor_dist\": {\n                                \"cov\": np.array([[0.3, 0], [0, 0.3]]),\n                                \"mean\": np.array([0.7, 1.4]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": BLUE,\n                            },\n                            \"positive_dist\": {\n                                \"cov\": np.array([[0.2, 0], [0, 0.2]]),\n                                \"mean\": np.array([0.8, -0.4]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": GREEN,\n                            },\n                            \"negative_dist\": {\n                                \"cov\": np.array([[0.4, 0], [0, 0.25]]),\n                                \"mean\": np.array([-1, -1.2]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": RED,\n                            },\n                        }\n                    }\n                },\n                run_time=3,\n            )\n        )\n\n    def construct(self):\n        \"\"\"\n        Makes the whole visualization.\n\n        1. Create the Architecture\n            a. Create the traditional VAE architecture with images\n        2. The Localization Procedure\n        3. The Training Procedure\n        \"\"\"\n        # 1. Create the Architecture\n        self.neural_network.scale(1.2)\n        create_vae = Create(self.neural_network)\n        self.play(create_vae, run_time=3)\n        # Make changing title\n        self.change_title(\"Oracle Guided Image Synthesis\\n      with Relative Queries\")\n        # 2. The Localization Procedure\n        self.change_title(\"The Localization Procedure\")\n        # Make algorithm subtitle\n        self.change_subtitle(\"Algorithm Steps\")\n        # Wait\n        self.play(Wait(1))\n        # Make prior distribution subtitle\n        self.change_subtitle(\"1. Calculate Prior Distribution\")\n        # Draw the prior distribution\n        self.play(Create(self.localizer.prior_distribution))\n        old_posterior = self.localizer.prior_distribution\n        # For N queries update the posterior\n        for query_index in range(self.localizer.num_queries):\n            # Make localization iteration\n            old_posterior = self.make_localization_time_step(old_posterior)\n            self.play(Wait(1))\n            if not query_index == self.localizer.num_queries - 1:\n                # Repeat\n                self.change_subtitle(\"5. Repeat\")\n                # Wait a second\n                self.play(Wait(1))\n        # Generate final estimate\n        self.change_subtitle(\"5. Generate Estimate Image\")\n        # Generate an estimate image\n        estimate_image_path = os.path.join(self.assets_path, \"estimate_image.jpg\")\n        self.make_generate_estimate_animation(estimate_image_path)\n        self.wait(1)\n        # Remove old posterior\n        self.play(FadeOut(old_posterior))\n        # 3. The Training Procedure\n        self.change_title(\"The Training Procedure\")\n        # Make training animation\n        # Do an Image forward pass\n        self.change_subtitle(\"1. Unsupervised Image Reconstruction\")\n        self.make_embed_input_image_animation(\n            self.input_embed_image_path, self.output_embed_image_path\n        )\n        self.wait(1)\n        # Do triplet forward pass\n        self.change_subtitle(\"2. Triplet Loss in Latent Space\")\n        self.make_triplet_forward_animation()\n        self.wait(1)\n", "file_path": "examples/paper_visualizations/oracle_guidance/oracle_guidance.py"}
{"prompt": "Readme Example A Simple Feed Forward Network", "response": "from manim import *\n\nfrom manim_ml.neural_network import FeedForwardLayer, NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1200\nconfig.frame_height = 4.0\nconfig.frame_width = 4.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                FeedForwardLayer(num_nodes=3),\n                FeedForwardLayer(num_nodes=5),\n                FeedForwardLayer(num_nodes=3),\n            ]\n        )\n        self.add(nn)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n", "file_path": "examples/readme_example/a_simple_feed_forward_network.py"}
{"prompt": "Readme Example Activation Functions", "response": "from manim import *\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, filter_spacing=0.32),\n                Convolutional2DLayer(\n                    3, 5, 3, filter_spacing=0.32, activation_function=\"ReLU\"\n                ),\n                FeedForwardLayer(3, activation_function=\"Sigmoid\"),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "file_path": "examples/readme_example/activation_functions.py"}
{"prompt": "Readme Example Animating The Forward Pass", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "file_path": "examples/readme_example/animating_the_forward_pass.py"}
{"prompt": "Readme Example Convolutional Neural Network With Images", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n    ImageLayer,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "file_path": "examples/readme_example/convolutional_neural_network_with_images.py"}
{"prompt": "Readme Example Convolutional Neural Networks", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n", "file_path": "examples/readme_example/convolutional_neural_networks.py"}
{"prompt": "Readme Example Example", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "file_path": "examples/readme_example/example.py"}
{"prompt": "Readme Example First Neural Network", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "file_path": "examples/readme_example/first_neural_network.py"}
{"prompt": "Readme Example Max Pooling", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass MaxPoolingScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 8),\n                Convolutional2DLayer(3, 6, 3),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(5, 2, 2),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "file_path": "examples/readme_example/max_pooling.py"}
{"prompt": "Readme Example Neural Network Dropout", "response": "from manim import *\nfrom manim_ml.neural_network.animations.dropout import (\n    make_neural_network_dropout_animation,\n)\nfrom manim_ml.neural_network import FeedForwardLayer, NeuralNetwork\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 5.0\nconfig.frame_width = 5.0\n\n\nclass DropoutNeuralNetworkScene(Scene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                FeedForwardLayer(3, rectangle_color=BLUE),\n                FeedForwardLayer(5, rectangle_color=BLUE),\n                FeedForwardLayer(3, rectangle_color=BLUE),\n                FeedForwardLayer(5, rectangle_color=BLUE),\n                FeedForwardLayer(4, rectangle_color=BLUE),\n            ],\n            layer_spacing=0.4,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        self.play(\n            make_neural_network_dropout_animation(\n                nn, dropout_rate=0.25, do_forward_pass=True\n            )\n        )\n        self.wait(1)\n", "file_path": "examples/readme_example/neural_network_dropout.py"}
{"prompt": "Readme Example Old Example", "response": "from manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass ConvolutionalNetworkScene(Scene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        self.play(nn.make_forward_pass_animation())\n", "file_path": "examples/readme_example/old_example.py"}
{"prompt": "Readme Example Setting Up A Scene", "response": "from manim import *\n\n# Import modules here\n\n\nclass BasicScene(ThreeDScene):\n    def construct(self):\n        # Your code goes here\n        text = Text(\"Your first scene!\")\n        self.add(text)\n", "file_path": "examples/readme_example/setting_up_a_scene.py"}
{"prompt": "Translation Equivariance Translation Equivariance", "response": "from manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.parent_layers import ThreeDLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 800\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/doggo.jpeg\")\n        numpy_image = np.asarray(image)\n        # Rotate the Three D layer position\n        ThreeDLayer.rotation_angle = 15 * DEGREES\n        ThreeDLayer.rotation_axis = [1, -1.0, 0]\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, 7, 3, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 5, 1, 1, filter_spacing=0.18),\n            ],\n            layer_spacing=0.25,\n            layout_direction=\"top_to_bottom\",\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.scale(1.5)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation(\n            highlight_active_feature_map=True,\n        )\n        self.wait(1)\n        self.play(forward_pass)\n", "file_path": "examples/translation_equivariance/translation_equivariance.py"}
{"prompt": "Variational Autoencoder Autoencoder Models   Init  ", "response": "", "file_path": "examples/variational_autoencoder/autoencoder_models/__init__.py"}
{"prompt": "Variational Autoencoder Autoencoder Models Generate Disentanglement", "response": "import pickle\nimport sys\nimport os\n\nsys.path.append(os.environ[\"PROJECT_ROOT\"])\nfrom autoencoder_models.variational_autoencoder import (\n    VAE,\n    load_dataset,\n    load_vae_from_path,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport scipy\nimport scipy.stats\nimport cv2\n\n\ndef binned_images(model_path, num_x_bins=6, plot=False):\n    latent_dim = 2\n    model = load_vae_from_path(model_path, latent_dim)\n    image_dataset = load_dataset(digit=2)\n    # Compute embedding\n    num_images = 500\n    embedding = []\n    images = []\n    for i in range(num_images):\n        image, _ = image_dataset[i]\n        mean, _, recon, _ = model.forward(image)\n        mean = mean.detach().numpy()\n        recon = recon.detach().numpy()\n        recon = recon.reshape(32, 32)\n        images.append(recon.squeeze())\n        if latent_dim > 2:\n            mean = mean[:2]\n        embedding.append(mean)\n    images = np.stack(images)\n    tsne_points = np.array(embedding)\n    tsne_points = (tsne_points - tsne_points.mean(axis=0)) / (tsne_points.std(axis=0))\n    # make vis\n    num_points = np.shape(tsne_points)[0]\n    x_min = np.amin(tsne_points.T[0])\n    y_min = np.amin(tsne_points.T[1])\n    y_max = np.amax(tsne_points.T[1])\n    x_max = np.amax(tsne_points.T[0])\n    # make the bins from the ranges\n    # to keep it square the same width is used for x and y dim\n    x_bins, step = np.linspace(x_min, x_max, num_x_bins, retstep=True)\n    x_bins = x_bins.astype(float)\n    num_y_bins = np.absolute(np.ceil((y_max - y_min) / step)).astype(int)\n    y_bins = np.linspace(y_min, y_max, num_y_bins)\n    # sort the tsne_points into a 2d histogram\n    tsne_points = tsne_points.squeeze()\n    hist_obj = scipy.stats.binned_statistic_dd(\n        tsne_points,\n        np.arange(num_points),\n        statistic=\"count\",\n        bins=[x_bins, y_bins],\n        expand_binnumbers=True,\n    )\n    # sample one point from each bucket\n    binnumbers = hist_obj.binnumber\n    num_x_bins = np.amax(binnumbers[0]) + 1\n    num_y_bins = np.amax(binnumbers[1]) + 1\n    binnumbers = binnumbers.T\n    # some places have no value in a region\n    used_mask = np.zeros((num_y_bins, num_x_bins))\n    image_bins = np.zeros(\n        (num_y_bins, num_x_bins, 3, np.shape(images)[2], np.shape(images)[2])\n    )\n    for i, bin_num in enumerate(list(binnumbers)):\n        used_mask[bin_num[1], bin_num[0]] = 1\n        image_bins[bin_num[1], bin_num[0]] = images[i]\n    # plot a grid of the images\n    fig, axs = plt.subplots(\n        nrows=np.shape(y_bins)[0],\n        ncols=np.shape(x_bins)[0],\n        constrained_layout=False,\n        dpi=50,\n    )\n    images = []\n    bin_indices = []\n    for y in range(num_y_bins):\n        for x in range(num_x_bins):\n            if used_mask[y, x] > 0.0:\n                image = np.uint8(image_bins[y][x].squeeze() * 255)\n                image = np.rollaxis(image, 0, 3)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                axs[num_y_bins - 1 - y][x].imshow(image)\n                images.append(image)\n                bin_indices.append((y, x))\n            axs[y, x].axis(\"off\")\n    if plot:\n        plt.axis(\"off\")\n        plt.show()\n    else:\n        return images, bin_indices\n\n\ndef generate_disentanglement(model_path=\"saved_models/model_dim2.pth\"):\n    \"\"\"Generates disentanglement visualization and serializes it\"\"\"\n    # Disentanglement object\n    disentanglement_object = {}\n    # Make Disentanglement\n    images, bin_indices = binned_images(model_path)\n    disentanglement_object[\"images\"] = images\n    disentanglement_object[\"bin_indices\"] = bin_indices\n    # Serialize Images\n    with open(\"disentanglement.pkl\", \"wb\") as f:\n        pickle.dump(disentanglement_object, f)\n\n\nif __name__ == \"__main__\":\n    plot = False\n    if plot:\n        model_path = \"saved_models/model_dim2.pth\"\n        # uniform_image_sample(model_path)\n        binned_images(model_path)\n    else:\n        generate_disentanglement()\n", "file_path": "examples/variational_autoencoder/autoencoder_models/generate_disentanglement.py"}
{"prompt": "Variational Autoencoder Autoencoder Models Generate Images", "response": "import torch\nfrom variational_autoencoder import VAE\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport numpy as np\nimport pickle\n\n# Load model\nvae = VAE(latent_dim=16)\nvae.load_state_dict(torch.load(\"saved_models/model.pth\"))\n# Transforms images to a PyTorch Tensor\ntensor_transform = transforms.ToTensor()\n# Download the MNIST Dataset\ndataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=tensor_transform\n)\n# Generate reconstructions\nnum_recons = 10\nfig, axs = plt.subplots(num_recons, 2, figsize=(2, num_recons))\nimage_pairs = []\nfor i in range(num_recons):\n    base_image, _ = dataset[i]\n    base_image = base_image.reshape(-1, 28 * 28)\n    _, _, recon_image, _ = vae.forward(base_image)\n    base_image = base_image.detach().numpy()\n    base_image = np.reshape(base_image, (28, 28)) * 255\n    recon_image = recon_image.detach().numpy()\n    recon_image = np.reshape(recon_image, (28, 28)) * 255\n    # Add to plot\n    axs[i][0].imshow(base_image)\n    axs[i][1].imshow(recon_image)\n    # image pairs\n    image_pairs.append((base_image, recon_image))\n\nwith open(\"image_pairs.pkl\", \"wb\") as f:\n    pickle.dump(image_pairs, f)\n\nplt.show()\n", "file_path": "examples/variational_autoencoder/autoencoder_models/generate_images.py"}
{"prompt": "Variational Autoencoder Autoencoder Models Generate Interpolation", "response": "import torch\nfrom variational_autoencoder import VAE, load_dataset\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport numpy as np\nimport pickle\n\n# Load model\nvae = VAE(latent_dim=16)\nvae.load_state_dict(torch.load(\"saved_models/model.pth\"))\ndataset = load_dataset()\n# Generate reconstructions\nnum_images = 50\nimage_pairs = []\nsave_object = {\"interpolation_path\": [], \"interpolation_images\": []}\n\n# Make interpolation path\nimage_a, image_b = dataset[0][0], dataset[1][0]\nimage_a = image_a.view(32 * 32)\nimage_b = image_b.view(32 * 32)\nz_a, _, _, _ = vae.forward(image_a)\nz_a = z_a.detach().cpu().numpy()\nz_b, _, _, _ = vae.forward(image_b)\nz_b = z_b.detach().cpu().numpy()\ninterpolation_path = np.linspace(z_a, z_b, num=num_images)\n# interpolation_path[:, 4] = np.linspace(-3, 3, num=num_images)\nsave_object[\"interpolation_path\"] = interpolation_path\n\nfor i in range(num_images):\n    # Generate\n    z = torch.Tensor(interpolation_path[i]).unsqueeze(0)\n    gen_image = vae.decode(z).detach().numpy()\n    gen_image = np.reshape(gen_image, (32, 32)) * 255\n    save_object[\"interpolation_images\"].append(gen_image)\n\nfig, axs = plt.subplots(num_images, 1, figsize=(1, num_images))\nimage_pairs = []\nfor i in range(num_images):\n    recon_image = save_object[\"interpolation_images\"][i]\n    # Add to plot\n    axs[i].imshow(recon_image)\n\n# Perform intrpolations\nwith open(\"interpolations.pkl\", \"wb\") as f:\n    pickle.dump(save_object, f)\n\nplt.show()\n", "file_path": "examples/variational_autoencoder/autoencoder_models/generate_interpolation.py"}
{"prompt": "Variational Autoencoder Autoencoder Models Variational Autoencoder", "response": "import torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport math\n\n\"\"\"\n    These are utility functions that help to calculate the input and output\n    sizes of convolutional neural networks\n\"\"\"\n\n\ndef num2tuple(num):\n    return num if isinstance(num, tuple) else (num, num)\n\n\ndef conv2d_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    h_w, kernel_size, stride, pad, dilation = (\n        num2tuple(h_w),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(pad),\n        num2tuple(dilation),\n    )\n    pad = num2tuple(pad[0]), num2tuple(pad[1])\n\n    h = math.floor(\n        (h_w[0] + sum(pad[0]) - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1\n    )\n    w = math.floor(\n        (h_w[1] + sum(pad[1]) - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1\n    )\n\n    return h, w\n\n\ndef convtransp2d_output_shape(\n    h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0\n):\n    h_w, kernel_size, stride, pad, dilation, out_pad = (\n        num2tuple(h_w),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(pad),\n        num2tuple(dilation),\n        num2tuple(out_pad),\n    )\n    pad = num2tuple(pad[0]), num2tuple(pad[1])\n\n    h = (\n        (h_w[0] - 1) * stride[0]\n        - sum(pad[0])\n        + dialation[0] * (kernel_size[0] - 1)\n        + out_pad[0]\n        + 1\n    )\n    w = (\n        (h_w[1] - 1) * stride[1]\n        - sum(pad[1])\n        + dialation[1] * (kernel_size[1] - 1)\n        + out_pad[1]\n        + 1\n    )\n\n    return h, w\n\n\ndef conv2d_get_padding(h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1):\n    h_w_in, h_w_out, kernel_size, stride, dilation = (\n        num2tuple(h_w_in),\n        num2tuple(h_w_out),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(dilation),\n    )\n\n    p_h = (\n        (h_w_out[0] - 1) * stride[0]\n        - h_w_in[0]\n        + dilation[0] * (kernel_size[0] - 1)\n        + 1\n    )\n    p_w = (\n        (h_w_out[1] - 1) * stride[1]\n        - h_w_in[1]\n        + dilation[1] * (kernel_size[1] - 1)\n        + 1\n    )\n\n    return (math.floor(p_h / 2), math.ceil(p_h / 2)), (\n        math.floor(p_w / 2),\n        math.ceil(p_w / 2),\n    )\n\n\ndef convtransp2d_get_padding(\n    h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1, out_pad=0\n):\n    h_w_in, h_w_out, kernel_size, stride, dilation, out_pad = (\n        num2tuple(h_w_in),\n        num2tuple(h_w_out),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(dilation),\n        num2tuple(out_pad),\n    )\n\n    p_h = (\n        -(\n            h_w_out[0]\n            - 1\n            - out_pad[0]\n            - dilation[0] * (kernel_size[0] - 1)\n            - (h_w_in[0] - 1) * stride[0]\n        )\n        / 2\n    )\n    p_w = (\n        -(\n            h_w_out[1]\n            - 1\n            - out_pad[1]\n            - dilation[1] * (kernel_size[1] - 1)\n            - (h_w_in[1] - 1) * stride[1]\n        )\n        / 2\n    )\n\n    return (math.floor(p_h / 2), math.ceil(p_h / 2)), (\n        math.floor(p_w / 2),\n        math.ceil(p_w / 2),\n    )\n\n\ndef load_dataset(train=True, digit=None):\n    # Transforms images to a PyTorch Tensor\n    tensor_transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()])\n\n    # Download the MNIST Dataset\n    dataset = datasets.MNIST(\n        root=\"./data\", train=train, download=True, transform=tensor_transform\n    )\n    # Load specific image\n    if not digit is None:\n        idx = dataset.train_labels == digit\n        dataset.targets = dataset.targets[idx]\n        dataset.data = dataset.data[idx]\n\n    return dataset\n\n\ndef load_vae_from_path(path, latent_dim):\n    model = VAE(latent_dim)\n    model.load_state_dict(torch.load(path))\n\n    return model\n\n\n# Creating a PyTorch class\n# 28*28 ==> 9 ==> 28*28\nclass VAE(torch.nn.Module):\n    def __init__(self, latent_dim=5, layer_count=4, channels=1):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.in_shape = 32\n        self.layer_count = layer_count\n        self.channels = channels\n        self.d = 128\n        mul = 1\n        inputs = self.channels\n        out_sizes = [(self.in_shape, self.in_shape)]\n        for i in range(self.layer_count):\n            setattr(self, \"conv%d\" % (i + 1), nn.Conv2d(inputs, self.d * mul, 4, 2, 1))\n            setattr(self, \"conv%d_bn\" % (i + 1), nn.BatchNorm2d(self.d * mul))\n            h_w = (out_sizes[-1][-1], out_sizes[-1][-1])\n            out_sizes.append(\n                conv2d_output_shape(h_w, kernel_size=4, stride=2, pad=1, dilation=1)\n            )\n            inputs = self.d * mul\n            mul *= 2\n\n        self.d_max = inputs\n        self.last_size = out_sizes[-1][-1]\n        self.num_linear = self.last_size**2 * self.d_max\n        # Encoder linear layers\n        self.encoder_mean_linear = nn.Linear(self.num_linear, self.latent_dim)\n        self.encoder_logvar_linear = nn.Linear(self.num_linear, self.latent_dim)\n        # Decoder linear layer\n        self.decoder_linear = nn.Linear(self.latent_dim, self.num_linear)\n\n        mul = inputs // self.d // 2\n\n        for i in range(1, self.layer_count):\n            setattr(\n                self,\n                \"deconv%d\" % (i + 1),\n                nn.ConvTranspose2d(inputs, self.d * mul, 4, 2, 1),\n            )\n            setattr(self, \"deconv%d_bn\" % (i + 1), nn.BatchNorm2d(self.d * mul))\n            inputs = self.d * mul\n            mul //= 2\n\n        setattr(\n            self,\n            \"deconv%d\" % (self.layer_count + 1),\n            nn.ConvTranspose2d(inputs, self.channels, 4, 2, 1),\n        )\n\n    def encode(self, x):\n        if len(x.shape) < 3:\n            x = x.unsqueeze(0)\n        if len(x.shape) < 4:\n            x = x.unsqueeze(1)\n        batch_size = x.shape[0]\n\n        for i in range(self.layer_count):\n            x = F.relu(\n                getattr(self, \"conv%d_bn\" % (i + 1))(\n                    getattr(self, \"conv%d\" % (i + 1))(x)\n                )\n            )\n\n        x = x.view(batch_size, -1)\n\n        mean = self.encoder_mean_linear(x)\n        logvar = self.encoder_logvar_linear(x)\n\n        return mean, logvar\n\n    def decode(self, x):\n        x = x.view(x.shape[0], self.latent_dim)\n        x = self.decoder_linear(x)\n        x = x.view(x.shape[0], self.d_max, self.last_size, self.last_size)\n        # x = self.deconv1_bn(x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(1, self.layer_count):\n            x = F.leaky_relu(\n                getattr(self, \"deconv%d_bn\" % (i + 1))(\n                    getattr(self, \"deconv%d\" % (i + 1))(x)\n                ),\n                0.2,\n            )\n        x = getattr(self, \"deconv%d\" % (self.layer_count + 1))(x)\n        x = torch.sigmoid(x)\n        return x\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        mean, logvar = self.encode(x)\n        eps = torch.randn(batch_size, self.latent_dim)\n        z = mean + torch.exp(logvar / 2) * eps\n        reconstructed = self.decode(z)\n        return mean, logvar, reconstructed, x\n\n\ndef train_model(latent_dim=16, plot=True, digit=1, epochs=200):\n    dataset = load_dataset(train=True, digit=digit)\n    # DataLoader is used to load the dataset\n    # for training\n    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n    # Model Initialization\n    model = VAE(latent_dim=latent_dim)\n    # Validation using MSE Loss function\n    def loss_function(mean, log_var, reconstructed, original, kl_beta=0.0001):\n        kl = torch.mean(\n            -0.5 * torch.sum(1 + log_var - mean**2 - log_var.exp(), dim=1), dim=0\n        )\n        recon = torch.nn.functional.mse_loss(reconstructed, original)\n        # print(f\"KL Error {kl}, Recon Error {recon}\")\n        return kl_beta * kl + recon\n\n    # Using an Adam Optimizer with lr = 0.1\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0e-8)\n\n    outputs = []\n    losses = []\n    for epoch in tqdm(range(epochs)):\n        for (image, _) in loader:\n            # Output of Autoencoder\n            mean, log_var, reconstructed, image = model(image)\n            # Calculating the loss function\n            loss = loss_function(mean, log_var, reconstructed, image)\n            # The gradients are set to zero,\n            # the the gradient is computed and stored.\n            # .step() performs parameter update\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # Storing the losses in a list for plotting\n            if torch.isnan(loss):\n                raise Exception()\n            losses.append(loss.detach().cpu())\n            outputs.append((epochs, image, reconstructed))\n\n    torch.save(\n        model.state_dict(),\n        os.path.join(\n            os.environ[\"PROJECT_ROOT\"],\n            f\"examples/variational_autoencoder/autoencoder_model/saved_models/model_dim{latent_dim}.pth\",\n        ),\n    )\n\n    if plot:\n        # Defining the Plot Style\n        plt.style.use(\"fivethirtyeight\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        # Plotting the last 100 values\n        plt.plot(losses)\n        plt.show()\n\n\nif __name__ == \"__main__\":\n    train_model(latent_dim=2, digit=2, epochs=40)\n", "file_path": "examples/variational_autoencoder/autoencoder_models/variational_autoencoder.py"}
{"prompt": "Variational Autoencoder Variational Autoencoder", "response": "\"\"\"Autoencoder Manim Visualizations\n\nIn this module I define Manim visualizations for Variational Autoencoders\nand Traditional Autoencoders.\n\n\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\nimport numpy as np\nfrom PIL import Image\nfrom manim_ml.neural_network.layers import EmbeddingLayer\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.layers import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass VAEScene(Scene):\n    \"\"\"Scene object for a Variational Autoencoder and Autoencoder\"\"\"\n\n    def construct(self):\n        numpy_image = np.asarray(Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\"))\n        vae = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.4),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(dist_theme=\"ellipse\"),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.4),\n            ]\n        )\n\n        self.play(Create(vae))\n        self.play(vae.make_forward_pass_animation(run_time=15))\n", "file_path": "examples/variational_autoencoder/variational_autoencoder.py"}
