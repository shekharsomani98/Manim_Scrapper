{"prompt": "Animate a colorful logo composed of shapes (circle, square, triangle, and M symbol) moving towards the center.", "response": "class ManimCELogo(Scene):\n    def construct(self):\n        self.camera.background_color = \"#ece6e2\"\n        logo_green = \"#87c2a5\"\n        logo_blue = \"#525893\"\n        logo_red = \"#e07a5f\"\n        logo_black = \"#343434\"\n        ds_m = MathTex(r\"\\mathbb{M}\", fill_color=logo_black).scale(7)\n        ds_m.shift(2.25 * LEFT + 1.5 * UP)\n        circle = Circle(color=logo_green, fill_opacity=1).shift(LEFT)\n        square = Square(color=logo_blue, fill_opacity=1).shift(UP)\n        triangle = Triangle(color=logo_red, fill_opacity=1).shift(RIGHT)\n        logo = VGroup(triangle, square, circle, ds_m)  # order matters\n        logo.move_to(ORIGIN)\n        self.add(logo)", "original_prompt": "ManimCELogo"}
{"prompt": "Animate a rotating line segment with two dots and two floating brace annotations.", "response": "class BraceAnnotation(Scene):\n    def construct(self):\n        dot = Dot([-2, -1, 0])\n        dot2 = Dot([2, 1, 0])\n        line = Line(dot.get_center(), dot2.get_center()).set_color(ORANGE)\n        b1 = Brace(line)\n        b1text = b1.get_text(\"Horizontal distance\")\n        b2 = Brace(line, direction=line.copy().rotate(PI / 2).get_unit_vector())\n        b2text = b2.get_tex(\"x-x_1\")\n        self.add(line, dot, dot2, b1, b2, b1text, b2text)", "original_prompt": "BraceAnnotation"}
{"prompt": "Animate a red dot on a number plane that moves from (0, 0) to (2, 2).", "response": "class VectorArrow(Scene):\n    def construct(self):\n        dot = Dot(ORIGIN)\n        arrow = Arrow(ORIGIN, [2, 2, 0], buff=0)\n        numberplane = NumberPlane()\n        origin_text = Text('(0, 0)').next_to(dot, DOWN)\n        tip_text = Text('(2, 2)').next_to(arrow.get_end(), RIGHT)\n        self.add(numberplane, dot, arrow, origin_text, tip_text)", "original_prompt": "VectorArrow"}
{"prompt": "\"Generate a gradient image and display it as a growing circle with green background.\"\n\nHowever, I would like to modify the prompt slightly to better match the Manim animation:\n\n\"Animate a 2D gradient image scaling up into view within a surrounding green rectangle.\"", "response": "class GradientImageFromArray(Scene):\n    def construct(self):\n        n = 256\n        imageArray = np.uint8(\n            [[i * 256 / n for i in range(0, n)] for _ in range(0, n)]\n        )\n        image = ImageMobject(imageArray).scale(2)\n        image.background_rectangle = SurroundingRectangle(image, color=GREEN)\n        self.add(image, image.background_rectangle)", "original_prompt": "GradientImageFromArray"}
{"prompt": "Animate overlapping ellipses to display different boolean operations: Ellipse 1 & Ellipse 2 transform into intersection, union, exclusion, and difference shapes while changing color and size.", "response": "class BooleanOperations(Scene):\n    def construct(self):\n        ellipse1 = Ellipse(\n            width=4.0, height=5.0, fill_opacity=0.5, color=BLUE, stroke_width=10\n        ).move_to(LEFT)\n        ellipse2 = ellipse1.copy().set_color(color=RED).move_to(RIGHT)\n        bool_ops_text = MarkupText(\"Boolean Operation\").next_to(ellipse1, UP * 3)\n        ellipse_group = Group(bool_ops_text, ellipse1, ellipse2).move_to(LEFT * 3)\n        self.play(FadeIn(ellipse_group))\n\n        i = Intersection(ellipse1, ellipse2, color=GREEN, fill_opacity=0.5)\n        self.play(i.animate.scale(0.25).move_to(RIGHT * 5 + UP * 2.5))\n        intersection_text = Text(\"Intersection\", font_size=23).next_to(i, UP)\n        self.play(FadeIn(intersection_text))\n\n        u = Union(ellipse1, ellipse2, color=ORANGE, fill_opacity=0.5)\n        union_text = Text(\"Union\", font_size=23)\n        self.play(u.animate.scale(0.3).next_to(i, DOWN, buff=union_text.height * 3))\n        union_text.next_to(u, UP)\n        self.play(FadeIn(union_text))\n\n        e = Exclusion(ellipse1, ellipse2, color=YELLOW, fill_opacity=0.5)\n        exclusion_text = Text(\"Exclusion\", font_size=23)\n        self.play(e.animate.scale(0.3).next_to(u, DOWN, buff=exclusion_text.height * 3.5))\n        exclusion_text.next_to(e, UP)\n        self.play(FadeIn(exclusion_text))\n\n        d = Difference(ellipse1, ellipse2, color=PINK, fill_opacity=0.5)\n        difference_text = Text(\"Difference\", font_size=23)\n        self.play(d.animate.scale(0.3).next_to(u, LEFT, buff=difference_text.height * 3.5))\n        difference_text.next_to(d, UP)\n        self.play(FadeIn(difference_text))", "original_prompt": "BooleanOperations"}
{"prompt": "\"Animate a dot moving along a growing circle with rotation while showing a static line.\"", "response": "class PointMovingOnShapes(Scene):\n    def construct(self):\n        circle = Circle(radius=1, color=BLUE)\n        dot = Dot()\n        dot2 = dot.copy().shift(RIGHT)\n        self.add(dot)\n\n        line = Line([3, 0, 0], [5, 0, 0])\n        self.add(line)\n\n        self.play(GrowFromCenter(circle))\n        self.play(Transform(dot, dot2))\n        self.play(MoveAlongPath(dot, circle), run_time=2, rate_func=linear)\n        self.play(Rotating(dot, about_point=[2, 0, 0]), run_time=1.5)\n        self.wait()", "original_prompt": "PointMovingOnShapes"}
{"prompt": "\"Fade in a small orange square that rotates around the center, then scales down and shifts left.\"", "response": "class MovingAround(Scene):\n    def construct(self):\n        square = Square(color=BLUE, fill_opacity=1)\n\n        self.play(square.animate.shift(LEFT))\n        self.play(square.animate.set_fill(ORANGE))\n        self.play(square.animate.scale(0.3))\n        self.play(square.animate.rotate(0.4))", "original_prompt": "MovingAround"}
{"prompt": "\"Show a rotating line with an evolving angle notation, gradually changing color and position.\"", "response": "class MovingAngle(Scene):\n    def construct(self):\n        rotation_center = LEFT\n\n        theta_tracker = ValueTracker(110)\n        line1 = Line(LEFT, RIGHT)\n        line_moving = Line(LEFT, RIGHT)\n        line_ref = line_moving.copy()\n        line_moving.rotate(\n            theta_tracker.get_value() * DEGREES, about_point=rotation_center\n        )\n        a = Angle(line1, line_moving, radius=0.5, other_angle=False)\n        tex = MathTex(r\"\\theta\").move_to(\n            Angle(\n                line1, line_moving, radius=0.5 + 3 * SMALL_BUFF, other_angle=False\n            ).point_from_proportion(0.5)\n        )\n\n        self.add(line1, line_moving, a, tex)\n        self.wait()\n\n        line_moving.add_updater(\n            lambda x: x.become(line_ref.copy()).rotate(\n                theta_tracker.get_value() * DEGREES, about_point=rotation_center\n            )\n        )\n\n        a.add_updater(\n            lambda x: x.become(Angle(line1, line_moving, radius=0.5, other_angle=False))\n        )\n        tex.add_updater(\n            lambda x: x.move_to(\n                Angle(\n                    line1, line_moving, radius=0.5 + 3 * SMALL_BUFF, other_angle=False\n                ).point_from_proportion(0.5)\n            )\n        )\n\n        self.play(theta_tracker.animate.set_value(40))\n        self.play(theta_tracker.animate.increment_value(140))\n        self.play(tex.animate.set_color(RED), run_time=0.5)\n        self.play(theta_tracker.animate.set_value(350))", "original_prompt": "MovingAngle"}
{"prompt": "Animate two moving dots that transform into a line connecting them, with a fixed distance.", "response": "class MovingDots(Scene):\n    def construct(self):\n        d1,d2=Dot(color=BLUE),Dot(color=GREEN)\n        dg=VGroup(d1,d2).arrange(RIGHT,buff=1)\n        l1=Line(d1.get_center(),d2.get_center()).set_color(RED)\n        x=ValueTracker(0)\n        y=ValueTracker(0)\n        d1.add_updater(lambda z: z.set_x(x.get_value()))\n        d2.add_updater(lambda z: z.set_y(y.get_value()))\n        l1.add_updater(lambda z: z.become(Line(d1.get_center(),d2.get_center())))\n        self.add(d1,d2,l1)\n        self.play(x.animate.set_value(5))\n        self.play(y.animate.set_value(4))\n        self.wait()", "original_prompt": "MovingDots"}
{"prompt": "\"Move a dot to the center of another dot.\"\n\nThis prompt is short and concise, and it accurately describes the key visual element (dot) and transformation (moving from one dot to another).", "response": "class MovingGroupToDestination(Scene):\n    def construct(self):\n        group = VGroup(Dot(LEFT), Dot(ORIGIN), Dot(RIGHT, color=RED), Dot(2 * RIGHT)).scale(1.4)\n        dest = Dot([4, 3, 0], color=YELLOW)\n        self.add(group, dest)\n        self.play(group.animate.shift(dest.get_center() - group[2].get_center()))\n        self.wait(0.5)", "original_prompt": "MovingGroupToDestination"}
{"prompt": "Animate the derivative formula diagram with a box transforming into its expanded form.", "response": "class MovingFrameBox(Scene):\n    def construct(self):\n        text=MathTex(\n            \"\\\\frac{d}{dx}f(x)g(x)=\",\"f(x)\\\\frac{d}{dx}g(x)\",\"+\",\n            \"g(x)\\\\frac{d}{dx}f(x)\"\n        )\n        self.play(Write(text))\n        framebox1 = SurroundingRectangle(text[1], buff = .1)\n        framebox2 = SurroundingRectangle(text[3], buff = .1)\n        self.play(\n            Create(framebox1),\n        )\n        self.wait()\n        self.play(\n            ReplacementTransform(framebox1,framebox2),\n        )\n        self.wait()", "original_prompt": "MovingFrameBox"}
{"prompt": "\"Rotate a yellow line about the origin, then reverse direction, with white reference line.\"", "response": "class RotationUpdater(Scene):\n    def construct(self):\n        def updater_forth(mobj, dt):\n            mobj.rotate_about_origin(dt)\n        def updater_back(mobj, dt):\n            mobj.rotate_about_origin(-dt)\n        line_reference = Line(ORIGIN, LEFT).set_color(WHITE)\n        line_moving = Line(ORIGIN, LEFT).set_color(YELLOW)\n        line_moving.add_updater(updater_forth)\n        self.add(line_reference, line_moving)\n        self.wait(2)\n        line_moving.remove_updater(updater_forth)\n        line_moving.add_updater(updater_back)\n        self.wait(2)\n        line_moving.remove_updater(updater_back)\n        self.wait(0.5)", "original_prompt": "RotationUpdater"}
{"prompt": "Animate a red dot moving around its center in a circle while showing the path it traces.", "response": "class PointWithTrace(Scene):\n    def construct(self):\n        path = VMobject()\n        dot = Dot()\n        path.set_points_as_corners([dot.get_center(), dot.get_center()])\n        def update_path(path):\n            previous_path = path.copy()\n            previous_path.add_points_as_corners([dot.get_center()])\n            path.become(previous_path)\n        path.add_updater(update_path)\n        self.add(path, dot)\n        self.play(Rotating(dot, radians=PI, about_point=RIGHT, run_time=2))\n        self.wait()\n        self.play(dot.animate.shift(UP))\n        self.play(dot.animate.shift(LEFT))\n        self.wait()", "original_prompt": "PointWithTrace"}
{"prompt": "Animate the sine and cosine graphs on a coordinate plane while showing the x-axis.", "response": "class SinAndCosFunctionPlot(Scene):\n    def construct(self):\n        axes = Axes(\n            x_range=[-10, 10.3, 1],\n            y_range=[-1.5, 1.5, 1],\n            x_length=10,\n            axis_config={\"color\": GREEN},\n            x_axis_config={\n                \"numbers_to_include\": np.arange(-10, 10.01, 2),\n                \"numbers_with_elongated_ticks\": np.arange(-10, 10.01, 2),\n            },\n            tips=False,\n        )\n        axes_labels = axes.get_axis_labels()\n        sin_graph = axes.plot(lambda x: np.sin(x), color=BLUE)\n        cos_graph = axes.plot(lambda x: np.cos(x), color=RED)\n\n        sin_label = axes.get_graph_label(\n            sin_graph, \"\\\\sin(x)\", x_val=-10, direction=UP / 2\n        )\n        cos_label = axes.get_graph_label(cos_graph, label=\"\\\\cos(x)\")\n\n        vert_line = axes.get_vertical_line(\n            axes.i2gp(TAU, cos_graph), color=YELLOW, line_func=Line\n        )\n        line_label = axes.get_graph_label(\n            cos_graph, r\"x=2\\pi\", x_val=TAU, direction=UR, color=WHITE\n        )\n\n        plot = VGroup(axes, sin_graph, cos_graph, vert_line)\n        labels = VGroup(axes_labels, sin_label, cos_label, line_label)\n        self.add(plot, labels)", "original_prompt": "SinAndCosFunctionPlot"}
{"prompt": "\"Animate a red curve that reveals the point where its minimum value is reached along the x-axis.\"", "response": "class ArgMinExample(Scene):\n    def construct(self):\n        ax = Axes(\n            x_range=[0, 10], y_range=[0, 100, 10], axis_config={\"include_tip\": False}\n        )\n        labels = ax.get_axis_labels(x_label=\"x\", y_label=\"f(x)\")\n\n        t = ValueTracker(0)\n\n        def func(x):\n            return 2 * (x - 5) ** 2\n        graph = ax.plot(func, color=MAROON)\n\n        initial_point = [ax.coords_to_point(t.get_value(), func(t.get_value()))]\n        dot = Dot(point=initial_point)\n\n        dot.add_updater(lambda x: x.move_to(ax.c2p(t.get_value(), func(t.get_value()))))\n        x_space = np.linspace(*ax.x_range[:2],200)\n        minimum_index = func(x_space).argmin()\n\n        self.add(ax, labels, graph, dot)\n        self.play(t.animate.set_value(x_space[minimum_index]))\n        self.wait()", "original_prompt": "ArgMinExample"}
{"prompt": "Animate a blue curve that intersects with another green curve while showing Riemann rectangles and bounded areas.", "response": "class GraphAreaPlot(Scene):\n    def construct(self):\n        ax = Axes(\n            x_range=[0, 5],\n            y_range=[0, 6],\n            x_axis_config={\"numbers_to_include\": [2, 3]},\n            tips=False,\n        )\n\n        labels = ax.get_axis_labels()\n\n        curve_1 = ax.plot(lambda x: 4 * x - x ** 2, x_range=[0, 4], color=BLUE_C)\n        curve_2 = ax.plot(\n            lambda x: 0.8 * x ** 2 - 3 * x + 4,\n            x_range=[0, 4],\n            color=GREEN_B,\n        )\n\n        line_1 = ax.get_vertical_line(ax.input_to_graph_point(2, curve_1), color=YELLOW)\n        line_2 = ax.get_vertical_line(ax.i2gp(3, curve_1), color=YELLOW)\n\n        riemann_area = ax.get_riemann_rectangles(curve_1, x_range=[0.3, 0.6], dx=0.03, color=BLUE, fill_opacity=0.5)\n        area = ax.get_area(curve_2, [2, 3], bounded_graph=curve_1, color=GREY, opacity=0.5)\n\n        self.add(ax, labels, curve_1, curve_2, line_1, line_2, riemann_area, area)", "original_prompt": "GraphAreaPlot"}
{"prompt": "Animate a dot moving along a curve that scales to show axes and stroke a polygon as it transforms.", "response": "class PolygonOnAxes(Scene):\n    def get_rectangle_corners(self, bottom_left, top_right):\n        return [\n            (top_right[0], top_right[1]),\n            (bottom_left[0], top_right[1]),\n            (bottom_left[0], bottom_left[1]),\n            (top_right[0], bottom_left[1]),\n        ]\n\n    def construct(self):\n        ax = Axes(\n            x_range=[0, 10],\n            y_range=[0, 10],\n            x_length=6,\n            y_length=6,\n            axis_config={\"include_tip\": False},\n        )\n\n        t = ValueTracker(5)\n        k = 25\n\n        graph = ax.plot(\n            lambda x: k / x,\n            color=YELLOW_D,\n            x_range=[k / 10, 10.0, 0.01],\n            use_smoothing=False,\n        )\n\n        def get_rectangle():\n            polygon = Polygon(\n                *[\n                    ax.c2p(*i)\n                    for i in self.get_rectangle_corners(\n                        (0, 0), (t.get_value(), k / t.get_value())\n                    )\n                ]\n            )\n            polygon.stroke_width = 1\n            polygon.set_fill(BLUE, opacity=0.5)\n            polygon.set_stroke(YELLOW_B)\n            return polygon\n\n        polygon = always_redraw(get_rectangle)\n\n        dot = Dot()\n        dot.add_updater(lambda x: x.move_to(ax.c2p(t.get_value(), k / t.get_value())))\n        dot.set_z_index(10)\n\n        self.add(ax, graph, dot)\n        self.play(Create(polygon))\n        self.play(t.animate.set_value(10))\n        self.play(t.animate.set_value(k / 10))\n        self.play(t.animate.set_value(5))", "original_prompt": "PolygonOnAxes"}
{"prompt": "\"Display a temperature plot with rotating axes and fading tips.\"\n\n(Note: The Manim animation does not explicitly include rotations of the 3D objects or mathematical transformations. It only includes a static heat diagram plot with rotating axis labels.)", "response": "class HeatDiagramPlot(Scene):\n    def construct(self):\n        ax = Axes(\n            x_range=[0, 40, 5],\n            y_range=[-8, 32, 5],\n            x_length=9,\n            y_length=6,\n            x_axis_config={\"numbers_to_include\": np.arange(0, 40, 5)},\n            y_axis_config={\"numbers_to_include\": np.arange(-5, 34, 5)},\n            tips=False,\n        )\n        labels = ax.get_axis_labels(\n            x_label=Tex(r\"$\\Delta Q$\"), y_label=Tex(r\"T[$^\\circ C$]\")\n        )\n\n        x_vals = [0, 8, 38, 39]\n        y_vals = [20, 0, 0, -5]\n        graph = ax.plot_line_graph(x_values=x_vals, y_values=y_vals)\n\n        self.add(ax, labels, graph)", "original_prompt": "HeatDiagramPlot"}
{"prompt": "\"Animate a camera following an orange dot along a sine curve as the axes scale and move in sync.\"", "response": "class FollowingGraphCamera(MovingCameraScene):\n    def construct(self):\n        self.camera.frame.save_state()\n\n        # create the axes and the curve\n        ax = Axes(x_range=[-1, 10], y_range=[-1, 10])\n        graph = ax.plot(lambda x: np.sin(x), color=BLUE, x_range=[0, 3 * PI])\n\n        # create dots based on the graph\n        moving_dot = Dot(ax.i2gp(graph.t_min, graph), color=ORANGE)\n        dot_1 = Dot(ax.i2gp(graph.t_min, graph))\n        dot_2 = Dot(ax.i2gp(graph.t_max, graph))\n\n        self.add(ax, graph, dot_1, dot_2, moving_dot)\n        self.play(self.camera.frame.animate.scale(0.5).move_to(moving_dot))\n\n        def update_curve(mob):\n            mob.move_to(moving_dot.get_center())\n\n        self.camera.frame.add_updater(update_curve)\n        self.play(MoveAlongPath(moving_dot, graph, rate_func=linear))\n        self.camera.frame.remove_updater(update_curve)\n\n        self.play(Restore(self.camera.frame))", "original_prompt": "FollowingGraphCamera"}
{"prompt": "\"Animate a red rectangle unfolding from a purple square while zooming in and scaling down, with a red text label on the bottom.\"", "response": "class MovingZoomedSceneAround(ZoomedScene):\n# contributed by TheoremofBeethoven, www.youtube.com/c/TheoremofBeethoven\n    def __init__(self, **kwargs):\n        ZoomedScene.__init__(\n            self,\n            zoom_factor=0.3,\n            zoomed_display_height=1,\n            zoomed_display_width=6,\n            image_frame_stroke_width=20,\n            zoomed_camera_config={\n                \"default_frame_stroke_width\": 3,\n                },\n            **kwargs\n        )\n\n    def construct(self):\n        dot = Dot().shift(UL * 2)\n        image = ImageMobject(np.uint8([[0, 100, 30, 200],\n                                       [255, 0, 5, 33]]))\n        image.height = 7\n        frame_text = Text(\"Frame\", color=PURPLE, font_size=67)\n        zoomed_camera_text = Text(\"Zoomed camera\", color=RED, font_size=67)\n\n        self.add(image, dot)\n        zoomed_camera = self.zoomed_camera\n        zoomed_display = self.zoomed_display\n        frame = zoomed_camera.frame\n        zoomed_display_frame = zoomed_display.display_frame\n\n        frame.move_to(dot)\n        frame.set_color(PURPLE)\n        zoomed_display_frame.set_color(RED)\n        zoomed_display.shift(DOWN)\n\n        zd_rect = BackgroundRectangle(zoomed_display, fill_opacity=0, buff=MED_SMALL_BUFF)\n        self.add_foreground_mobject(zd_rect)\n\n        unfold_camera = UpdateFromFunc(zd_rect, lambda rect: rect.replace(zoomed_display))\n\n        frame_text.next_to(frame, DOWN)\n\n        self.play(Create(frame), FadeIn(frame_text, shift=UP))\n        self.activate_zooming()\n\n        self.play(self.get_zoomed_display_pop_out_animation(), unfold_camera)\n        zoomed_camera_text.next_to(zoomed_display_frame, DOWN)\n        self.play(FadeIn(zoomed_camera_text, shift=UP))\n        # Scale in        x   y  z\n        scale_factor = [0.5, 1.5, 0]\n        self.play(\n            frame.animate.scale(scale_factor),\n            zoomed_display.animate.scale(scale_factor),\n            FadeOut(zoomed_camera_text),\n            FadeOut(frame_text)\n        )\n        self.wait()\n        self.play(ScaleInPlace(zoomed_display, 2))\n        self.wait()\n        self.play(frame.animate.shift(2.5 * DOWN))\n        self.wait()\n        self.play(self.get_zoomed_display_pop_out_animation(), unfold_camera, rate_func=lambda t: smooth(1 - t))\n        self.play(Uncreate(zoomed_display_frame), FadeOut(frame))\n        self.wait()", "original_prompt": "MovingZoomedSceneAround"}
{"prompt": "\"Show a rotating, three-dimensional coordinate system with fixed 3D text in the corner.\"", "response": "class FixedInFrameMObjectTest(ThreeDScene):\n    def construct(self):\n        axes = ThreeDAxes()\n        self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES)\n        text3d = Text(\"This is a 3D text\")\n        self.add_fixed_in_frame_mobjects(text3d)\n        text3d.to_corner(UL)\n        self.add(axes)\n        self.wait()", "original_prompt": "FixedInFrameMObjectTest"}
{"prompt": "Animate a rotating circle to demonstrate parametric 3D surface generation with coordinate axes.", "response": "class ThreeDLightSourcePosition(ThreeDScene):\n    def construct(self):\n        axes = ThreeDAxes()\n        sphere = Surface(\n            lambda u, v: np.array([\n                1.5 * np.cos(u) * np.cos(v),\n                1.5 * np.cos(u) * np.sin(v),\n                1.5 * np.sin(u)\n            ]), v_range=[0, TAU], u_range=[-PI / 2, PI / 2],\n            checkerboard_colors=[RED_D, RED_E], resolution=(15, 32)\n        )\n        self.renderer.camera.light_source.move_to(3*IN) # changes the source of the light\n        self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)\n        self.add(axes, sphere)", "original_prompt": "ThreeDLightSourcePosition"}
{"prompt": "Rotate the 3D coordinate axes around a fixed point while showing an orbiting circle.", "response": "class ThreeDCameraRotation(ThreeDScene):\n    def construct(self):\n        axes = ThreeDAxes()\n        circle=Circle()\n        self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)\n        self.add(circle,axes)\n        self.begin_ambient_camera_rotation(rate=0.1)\n        self.wait()\n        self.stop_ambient_camera_rotation()\n        self.move_camera(phi=75 * DEGREES, theta=30 * DEGREES)\n        self.wait()", "original_prompt": "ThreeDCameraRotation"}
{"prompt": "\"Spin a circle within 3D axes while maintaining fixed camera orientation and showing the rotation effect\".", "response": "class ThreeDCameraIllusionRotation(ThreeDScene):\n    def construct(self):\n        axes = ThreeDAxes()\n        circle=Circle()\n        self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)\n        self.add(circle,axes)\n        self.begin_3dillusion_camera_rotation(rate=2)\n        self.wait(PI/2)\n        self.stop_3dillusion_camera_rotation()", "original_prompt": "ThreeDCameraIllusionRotation"}
{"prompt": "\"Rotate 3D Gaussian plane around axes and fade with checkerboard fill pattern.\"\n\nOr, a bit more descriptive:\n\nAnimate a rotating 3D Gaussian plane with changing colors using a checkerboard pattern.", "response": "class ThreeDSurfacePlot(ThreeDScene):\n    def construct(self):\n        resolution_fa = 24\n        self.set_camera_orientation(phi=75 * DEGREES, theta=-30 * DEGREES)\n\n        def param_gauss(u, v):\n            x = u\n            y = v\n            sigma, mu = 0.4, [0.0, 0.0]\n            d = np.linalg.norm(np.array([x - mu[0], y - mu[1]]))\n            z = np.exp(-(d ** 2 / (2.0 * sigma ** 2)))\n            return np.array([x, y, z])\n\n        gauss_plane = Surface(\n            param_gauss,\n            resolution=(resolution_fa, resolution_fa),\n            v_range=[-2, +2],\n            u_range=[-2, +2]\n        )\n\n        gauss_plane.scale(2, about_point=ORIGIN)\n        gauss_plane.set_style(fill_opacity=1,stroke_color=GREEN)\n        gauss_plane.set_fill_by_checkerboard(ORANGE, BLUE, opacity=0.5)\n        axes = ThreeDAxes()\n        self.add(axes,gauss_plane)", "original_prompt": "ThreeDSurfacePlot"}
{"prompt": "Animate a rotating LaTeX equation on a grid while transforming into a sine wave.", "response": "class OpeningManim(Scene):\n    def construct(self):\n        title = Tex(r\"This is some \\LaTeX\")\n        basel = MathTex(r\"\\sum_{n=1}^\\infty \\frac{1}{n^2} = \\frac{\\pi^2}{6}\")\n        VGroup(title, basel).arrange(DOWN)\n        self.play(\n            Write(title),\n            FadeIn(basel, shift=DOWN),\n        )\n        self.wait()\n\n        transform_title = Tex(\"That was a transform\")\n        transform_title.to_corner(UP + LEFT)\n        self.play(\n            Transform(title, transform_title),\n            LaggedStart(*[FadeOut(obj, shift=DOWN) for obj in basel]),\n        )\n        self.wait()\n\n        grid = NumberPlane()\n        grid_title = Tex(\"This is a grid\", font_size=72)\n        grid_title.move_to(transform_title)\n\n        self.add(grid, grid_title)  # Make sure title is on top of grid\n        self.play(\n            FadeOut(title),\n            FadeIn(grid_title, shift=UP),\n            Create(grid, run_time=3, lag_ratio=0.1),\n        )\n        self.wait()\n\n        grid_transform_title = Tex(\n            r\"That was a non-linear function \\\\ applied to the grid\"\n        )\n        grid_transform_title.move_to(grid_title, UL)\n        grid.prepare_for_nonlinear_transform()\n        self.play(\n            grid.animate.apply_function(\n                lambda p: p\n                          + np.array(\n                    [\n                        np.sin(p[1]),\n                        np.sin(p[0]),\n                        0,\n                    ]\n                )\n            ),\n            run_time=3,\n        )\n        self.wait()\n        self.play(Transform(grid_title, grid_transform_title))\n        self.wait()", "original_prompt": "OpeningManim"}
{"prompt": "Animate a small yellow dot moving around a unit circle while drawing a blue line and a red line with the dot's position on it.", "response": "class SineCurveUnitCircle(Scene):\n    # contributed by heejin_park, https://infograph.tistory.com/230\n    def construct(self):\n        self.show_axis()\n        self.show_circle()\n        self.move_dot_and_draw_curve()\n        self.wait()\n\n    def show_axis(self):\n        x_start = np.array([-6,0,0])\n        x_end = np.array([6,0,0])\n\n        y_start = np.array([-4,-2,0])\n        y_end = np.array([-4,2,0])\n\n        x_axis = Line(x_start, x_end)\n        y_axis = Line(y_start, y_end)\n\n        self.add(x_axis, y_axis)\n        self.add_x_labels()\n\n        self.origin_point = np.array([-4,0,0])\n        self.curve_start = np.array([-3,0,0])\n\n    def add_x_labels(self):\n        x_labels = [\n            MathTex(r\"\\pi\"), MathTex(r\"2 \\pi\"),\n            MathTex(r\"3 \\pi\"), MathTex(r\"4 \\pi\"),\n        ]\n\n        for i in range(len(x_labels)):\n            x_labels[i].next_to(np.array([-1 + 2*i, 0, 0]), DOWN)\n            self.add(x_labels[i])\n\n    def show_circle(self):\n        circle = Circle(radius=1)\n        circle.move_to(self.origin_point)\n        self.add(circle)\n        self.circle = circle\n\n    def move_dot_and_draw_curve(self):\n        orbit = self.circle\n        origin_point = self.origin_point\n\n        dot = Dot(radius=0.08, color=YELLOW)\n        dot.move_to(orbit.point_from_proportion(0))\n        self.t_offset = 0\n        rate = 0.25\n\n        def go_around_circle(mob, dt):\n            self.t_offset += (dt * rate)\n            # print(self.t_offset)\n            mob.move_to(orbit.point_from_proportion(self.t_offset % 1))\n\n        def get_line_to_circle():\n            return Line(origin_point, dot.get_center(), color=BLUE)\n\n        def get_line_to_curve():\n            x = self.curve_start[0] + self.t_offset * 4\n            y = dot.get_center()[1]\n            return Line(dot.get_center(), np.array([x,y,0]), color=YELLOW_A, stroke_width=2 )\n\n\n        self.curve = VGroup()\n        self.curve.add(Line(self.curve_start,self.curve_start))\n        def get_curve():\n            last_line = self.curve[-1]\n            x = self.curve_start[0] + self.t_offset * 4\n            y = dot.get_center()[1]\n            new_line = Line(last_line.get_end(),np.array([x,y,0]), color=YELLOW_D)\n            self.curve.add(new_line)\n\n            return self.curve\n\n        dot.add_updater(go_around_circle)\n\n        origin_to_circle_line = always_redraw(get_line_to_circle)\n        dot_to_curve_line = always_redraw(get_line_to_curve)\n        sine_curve_line = always_redraw(get_curve)\n\n        self.add(dot)\n        self.add(orbit, origin_to_circle_line, dot_to_curve_line, sine_curve_line)\n        self.wait(8.5)\n\n        dot.remove_updater(go_around_circle)", "original_prompt": "SineCurveUnitCircle"}
{"prompt": "\"Create a neural network with layered nodes animating a forward pass.\"", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass NeuralNetworkScene(Scene):\n    \"\"\"Test Scene for the Neural Network\"\"\"\n\n    def construct(self):\n        # Make the Layer object\n        layers = [FeedForwardLayer(3), FeedForwardLayer(5), FeedForwardLayer(3)]\n        nn = NeuralNetwork(layers)\n        nn.scale(2)\n        nn.move_to(ORIGIN)\n        # Make Animation\n        self.add(nn)\n        # self.play(Create(nn))\n        forward_propagation_animation = nn.make_forward_pass_animation(\n            run_time=5, passing_flash=True\n        )\n\n        self.play(forward_propagation_animation)\n", "original_prompt": "Basic Neural Network Basic Neural Network"}
{"prompt": "\"Visualize a 3-layer neural network transforming through feedforward passes with flash effects.\"", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass NeuralNetworkScene(Scene):\n    \"\"\"Test Scene for the Neural Network\"\"\"\n\n    def construct(self):\n        # Make the Layer object\n        layers = [FeedForwardLayer(3), FeedForwardLayer(5), FeedForwardLayer(3)]\n        nn = NeuralNetwork(layers)\n        nn.scale(2)\n        nn.move_to(ORIGIN)\n        # Make Animation\n        self.add(nn)\n        # self.play(Create(nn))\n        forward_propagation_animation = nn.make_forward_pass_animation(\n            run_time=5, passing_flash=True\n        )\n\n        self.play(forward_propagation_animation)\n", "original_prompt": "Basic Neural Network Basic Neural Network"}
{"prompt": "\"Animate a neural network's activation layers rotating and connecting while showing input and output points.\"", "response": "from manim import *\n\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.math_operation_layer import MathOperationLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        nn = NeuralNetwork({\n            \"feed_forward_1\": FeedForwardLayer(3),\n            \"feed_forward_2\": FeedForwardLayer(3, activation_function=\"ReLU\"),\n            \"feed_forward_3\": FeedForwardLayer(3),\n            \"sum_operation\": MathOperationLayer(\"+\", activation_function=\"ReLU\"),\n        })\n        nn.add_connection(\"feed_forward_1\", \"sum_operation\")\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Add the network\n        nn = NeuralNetwork({\n                \"feed_forward_1\": FeedForwardLayer(3),\n                \"feed_forward_2\": FeedForwardLayer(3, activation_function=\"ReLU\"),\n                \"feed_forward_3\": FeedForwardLayer(3),\n                \"sum_operation\": MathOperationLayer(\"+\", activation_function=\"ReLU\"),\n            },\n            layer_spacing=0.38\n        )\n        # Make connections\n        input_blank_dot = Dot(\n            nn.input_layers_dict[\"feed_forward_1\"].get_left() - np.array([0.65, 0.0, 0.0])\n        )\n        nn.add_connection(input_blank_dot, \"feed_forward_1\", arc_direction=\"straight\")\n        nn.add_connection(\"feed_forward_1\", \"sum_operation\")\n        output_blank_dot = Dot(\n            nn.input_layers_dict[\"sum_operation\"].get_right() + np.array([0.65, 0.0, 0.0])\n        )\n        nn.add_connection(\"sum_operation\", output_blank_dot, arc_direction=\"straight\")\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        # Group it all\n        group = Group(nn, code)\n        group.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)", "original_prompt": "Basic Neural Network Residual Block"}
{"prompt": "Animate a 3D neural network diagram that scales and moves while displaying an image of the MNIST digit \"8\".", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork([\n            # ... Layers at start\n            Convolutional2DLayer(3, 5, 3, activation_function=\"ReLU\"),\n            # ... Layers at end\n        ])\n        # Play the animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        font=\"Monospace\",\n        background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.45)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7),\n                Convolutional2DLayer(3, 5, 3, activation_function=\"ReLU\"),\n                Convolutional2DLayer(5, 3, 3, activation_function=\"ReLU\"),\n                FeedForwardLayer(3),\n                FeedForwardLayer(1),\n            ],\n            layer_spacing=0.25,\n        )\n        # nn.scale(0.7)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # nn.move_to(ORIGIN)\n        # Move everything up\n        # Group(nn, code).move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn Activation Functions"}
{"prompt": "Animate a neural network diagram unfolding from 3D to 2D while showing code snippet.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image, height=1.5),\n            Convolutional2DLayer(1, 7, 3),\n            Convolutional2DLayer(3, 5, 3),\n            Convolutional2DLayer(5, 3, 1),\n            FeedForwardLayer(3),\n            FeedForwardLayer(3),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.50)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # Group it all\n        # group = Group(nn, code)\n        # group.move_to(ORIGIN)\n        nn.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn Cnn"}
{"prompt": "Animate a 3D neural network model being created, with layers unfolding like a flipbook.", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork([\n            ImageLayer(image),\n            Convolutional2DLayer(1, 8),\n            MaxPooling2DLayer(kernel_size=2),\n            Convolutional2DLayer(3, 2, 3),\n        ])\n        # Play the animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        font=\"Monospace\",\n        background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.4)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 8, filter_spacing=0.32),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(3, 2, 3, filter_spacing=0.32),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        Group(code, nn).move_to(ORIGIN)\n        self.add(code)\n        self.wait(5)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn Cnn Max Pool"}
{"prompt": "\"Animate a neural network visualizer that generates an image through convolutional layers.\"", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image, height=1.5),\n            Convolutional2DLayer(1, 5, 5, 1, 1),\n            Convolutional2DLayer(4, 5, 5, 1, 1),\n            Convolutional2DLayer(2, 5, 5),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.50)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 5, 1, filter_spacing=0.32),\n                Convolutional2DLayer(4, 5, 1, filter_spacing=0.32),\n                Convolutional2DLayer(2, 5, 5, filter_spacing=0.32),\n            ],\n            layer_spacing=0.4,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        # Group it all\n        group = Group(nn, code)\n        group.move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Cnn One By One Convolution"}
{"prompt": "\"Animate a 3D neural network diagram unfolding from a code snippet, with layers displaying image and mathematical transformations.\"", "response": "from manim import *\nfrom manim_ml.neural_network.layers.image import ImageLayer\nimport numpy as np\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make nn\n        nn = NeuralNetwork([\n            ImageLayer(numpy_image),\n            Convolutional2DLayer(1, 6, 1, padding=1),\n            Convolutional2DLayer(3, 6, 3),\n            FeedForwardLayer(3),\n            FeedForwardLayer(1),\n        ])\n        # Play animation\n        self.play(nn.make_forward_pass_animation()) \n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(\n                    num_feature_maps=1,\n                    feature_map_size=6,\n                    padding=1,\n                    padding_dashed=True,\n                ),\n                Convolutional2DLayer(\n                    num_feature_maps=3,\n                    feature_map_size=6,\n                    filter_size=3,\n                    padding=0,\n                    padding_dashed=False,\n                ),\n                FeedForwardLayer(3),\n                FeedForwardLayer(1),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        self.add(nn)\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        Group(code, nn).move_to(ORIGIN)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.wait(1)\n        self.play(forward_pass, run_time=20)\n", "original_prompt": "Cnn Padding Example"}
{"prompt": "\"Animate a 3D neural network model with residual connections transforming into a functional graph while displaying the MNIST digit image.\"", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\nfrom manim_ml.neural_network import Convolutional2DLayer, NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\ndef make_code_snippet():\n    code_str = \"\"\"\n        # Make the neural network\n        nn = NeuralNetwork({\n            \"layer1\": Convolutional2DLayer(1, 5, padding=1),\n            \"layer2\": Convolutional2DLayer(1, 5, 3, padding=1),\n            \"layer3\": Convolutional2DLayer(1, 5, 3, padding=1)\n        })\n        # Add the residual connection\n        nn.add_connection(\"layer1\", \"layer3\")\n        # Make the animation\n        self.play(nn.make_forward_pass_animation())\n    \"\"\"\n\n    code = Code(\n        code=code_str,\n        tab_width=4,\n        background_stroke_width=1,\n        background_stroke_color=WHITE,\n        insert_line_no=False,\n        style=\"monokai\",\n        # background=\"window\",\n        language=\"py\",\n    )\n    code.scale(0.38)\n\n    return code\n\n\nclass ConvScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n\n        nn = NeuralNetwork(\n            {\n                \"layer1\": Convolutional2DLayer(1, 5, padding=1),\n                \"layer2\": Convolutional2DLayer(1, 5, 3, padding=1),\n                \"layer3\": Convolutional2DLayer(1, 5, 3, padding=1),\n            },\n            layer_spacing=0.25,\n        )\n\n        nn.add_connection(\"layer1\", \"layer3\")\n\n        self.add(nn)\n\n        code = make_code_snippet()\n        code.next_to(nn, DOWN)\n        self.add(code)\n        Group(code, nn).move_to(ORIGIN)\n\n        self.play(nn.make_forward_pass_animation(), run_time=8)\n", "original_prompt": "Cnn Resnet Block"}
{"prompt": "\"Animate a 2D neural network with layers of image and numerical data, transforming into forward pass animation.\"", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass ImageNeuralNetworkScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make image object\n            image = Image.open('images/image.jpeg')\n            numpy_image = np.asarray(image)\n            # Make Neural Network\n            layers = [\n                ImageLayer(numpy_image, height=1.4),\n                FeedForwardLayer(3), \n                FeedForwardLayer(5),\n                FeedForwardLayer(3)\n            ]\n            nn = NeuralNetwork(layers)\n            self.add(nn)\n            # Play animation\n            self.play(\n                nn.make_forward_pass_animation()\n            )\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            background_stroke_color=WHITE,\n            insert_line_no=False,\n            style=\"monokai\",\n            # background=\"window\",\n            language=\"py\",\n        )\n        code.scale(0.2)\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        layers = [\n            ImageLayer(numpy_image, height=1.4),\n            FeedForwardLayer(3),\n            FeedForwardLayer(5),\n            FeedForwardLayer(3),\n            FeedForwardLayer(6),\n        ]\n        nn = NeuralNetwork(layers)\n        nn.scale(0.9)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.rotate(-PI / 2)\n        nn.layers[0].image_mobject.rotate(PI / 2)\n        nn.layers[0].image_mobject.shift([0, -0.4, 0])\n        nn.shift([1.5, 0.3, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(1.9)\n        code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(run_time=10))\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Feed Forward Neural Network\n    ffnn_scene = FeedForwardNeuralNetworkScene()\n    ffnn_scene.render()\n    # Neural Network\n    nn_scene = NeuralNetworkScene()\n    nn_scene.render()\n", "original_prompt": "Code Snippet Image Nn Code Snippet"}
{"prompt": "Animate a neural network diagram with layers transforming into a 3D model, rotating and shifting along axes.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer, EmbeddingLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 720\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass VAECodeSnippetScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make Neural Network\n            nn = NeuralNetwork([\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ], layer_spacing=0.1)\n            # Play animation\n            self.play(nn.make_forward_pass_animation())\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            # background_stroke_color=WHITE,\n            insert_line_no=False,\n            background=\"window\",\n            # font=\"Monospace\",\n            style=\"one-dark\",\n            language=\"py\",\n        )\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\", point_radius=0.04).scale(\n            1.0\n        )\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ],\n            layer_spacing=0.1,\n        )\n\n        nn.scale(1.1)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        # nn.rotate(-PI/2)\n        # nn.all_layers[0].image_mobject.rotate(PI/2)\n        # nn.all_layers[0].image_mobject.shift([0, -0.4, 0])\n        # nn.all_layers[-1].image_mobject.rotate(PI/2)\n        # nn.all_layers[-1].image_mobject.shift([0, -0.4, 0])\n        nn.shift([0, -1.4, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(0.52)\n        code_snippet.shift([0, 1.25, 0])\n        # code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(), run_time=10)\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Neural Network\n    nn_scene = VAECodeSnippetScene()\n    nn_scene.render()\n", "original_prompt": "Code Snippet Vae Code Landscape"}
{"prompt": "Animate a neural network diagram rotating with code snippet overlay, scaling with image objects.", "response": "from manim import *\nfrom manim_ml.neural_network.layers import FeedForwardLayer, ImageLayer, EmbeddingLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom PIL import Image\nimport numpy as np\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass VAECodeSnippetScene(Scene):\n    def make_code_snippet(self):\n        code_str = \"\"\"\n            # Make image object\n            image = Image.open('images/image.jpeg')\n            numpy_image = np.asarray(image)\n            # Make Neural Network\n            nn = NeuralNetwork([\n                ImageLayer(numpy_image, height=1.2),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.2),\n            ], layer_spacing=0.1)\n            self.add(nn)\n            # Play animation\n            self.play(\n                nn.make_forward_pass_animation()\n            )\n        \"\"\"\n\n        code = Code(\n            code=code_str,\n            tab_width=4,\n            background_stroke_width=1,\n            # background_stroke_color=WHITE,\n            insert_line_no=False,\n            background=\"window\",\n            # font=\"Monospace\",\n            style=\"one-dark\",\n            language=\"py\",\n        )\n        code.scale(0.2)\n\n        return code\n\n    def construct(self):\n        image = Image.open(\"../../tests/images/image.jpeg\")\n        numpy_image = np.asarray(image)\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\", point_radius=0.04).scale(\n            1.0\n        )\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.0),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.0),\n            ],\n            layer_spacing=0.1,\n        )\n\n        nn.scale(0.65)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.rotate(-PI / 2)\n        nn.all_layers[0].image_mobject.rotate(PI / 2)\n        # nn.all_layers[0].image_mobject.shift([0, -0.4, 0])\n        nn.all_layers[-1].image_mobject.rotate(PI / 2)\n        # nn.all_layers[-1].image_mobject.shift([0, -0.4, 0])\n        nn.shift([1.5, 0.0, 0])\n        self.add(nn)\n        # Make code snippet\n        code_snippet = self.make_code_snippet()\n        code_snippet.scale(1.9)\n        code_snippet.shift([-1.25, 0, 0])\n        self.add(code_snippet)\n        # Play animation\n        self.play(nn.make_forward_pass_animation(), run_time=10)\n\n\nif __name__ == \"__main__\":\n    \"\"\"Render all scenes\"\"\"\n    # Neural Network\n    nn_scene = VAECodeSnippetScene()\n    nn_scene.render()\n", "original_prompt": "Code Snippet Vae Nn Code Snippet"}
{"prompt": "Here is a Python script that uses the Pygame library to create an interactive visualization of the given prompt:\n\n```python\nimport pygame\nimport sys\n\n# Initialize Pygame\npygame.init()\n\n# Set up some constants\nWIDTH, HEIGHT = 800, 600\nWHITE = (255, 255, 255)\nBLACK = (0, 0, 0)\n\n# Create the game window\nscreen = pygame.display.set_mode((WIDTH, HEIGHT))\n\n# Define a font for text rendering\nfont = pygame.font.Font(None, 36)\n\nclass Token:\n    def __init__(self, x, y, text):\n        self.x = x\n        self.y = y\n        self.text = text\n\n    def draw(self):\n        text_surface = font.render(self.text, True, WHITE)\n        screen.blit(text_surface, (self.x, self.y))\n\ndef main():\n    # Set up the prompt and tokenizer\n    prompt = '\"Astronaut riding a horse on the moon\"'\n    tokenizer = Tokenizer()\n\n    # Split the prompt into tokens\n    tokens = tokenizer.tokenize(prompt)\n\n    # Initialize variables for tracking the animation state\n    current_token_index = 0\n\n    while True:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit()\n            elif event.type == pygame.KEYDOWN:\n                if event.key == pygame.K_SPACE and current_token_index < len(tokens):\n                    # Move to the next token\n                    current_token_index += 1\n\n        # Draw the background\n        screen.fill(BLACK)\n\n        # Draw the text prompt\n        text_prompt = font.render(prompt, True, WHITE)\n        screen.blit(text_prompt, (WIDTH / 2 - text_prompt.get_width() / 2, HEIGHT / 4))\n\n        # Draw the generated image\n        generated_image = Image()\n        generated_image.draw(screen)\n\n        # Draw the tokenization arrow\n        if current_token_index > 0:\n            previous_token = tokens[current_token_index - 1]\n            next_token = tokens[current_token_index]\n            arrow = Arrow(previous_token.x + font.size(previous_token.text)[0] / 2,\n                          previous_token.y + font.size(previous_token.text)[1] / 2,\n                          next_token.x + font.size(next_token.text)[0] / 2,\n                          next_token.y + font.size(next_token.text)[1] / 2)\n            arrow.draw(screen)\n\n        # Draw the heatmap for the current token\n        if current_token_index < len(tokens):\n            heatmap = Heatmap()\n            heatmap.draw(screen, tokens[current_token_index].text, tokens[current_token_index].x,\n                          tokens[current_token_index].y, tokens[current_token_index].text)\n\n        # Update the screen\n        pygame.display.flip()\n\nclass Tokenizer:\n    def tokenize(self, text):\n        return [Token(i, 0, word) for i, word in enumerate(text.split())]\n\nclass Image:\n    def __init__(self):\n        self.x = WIDTH / 2 - 100\n        self.y = HEIGHT / 4\n\n    def draw(self, screen):\n        pygame.draw.rect(screen, WHITE, (self.x, self.y, 200, 100))\n\nclass Arrow:\n    def __init__(self, x1, y1, x2, y2):\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n\n    def draw(self, screen):\n        pygame.draw.line(screen, WHITE, (self.x1, self.y1), (self.x2, self.y2))\n\nclass Heatmap:\n    def __init__(self):\n        self.width = 0\n        self.height = 0\n        self.x = 0\n        self.y = 0\n\n    def draw(self, screen, text, x, y, width):\n        self.width = width\n        self.height = 100\n        self.x = x\n        self.y = y\n        pygame.draw.rect(screen, WHITE, (self.x, self.y, self.width, self.height))\n        font_surface = font.render(text, True, BLACK)\n        screen.blit(font_surface, (x + text_size[0] / 2, y))\n\ndef get_text_size(text):\n    return font.size(text)\n\ntext_size = font.size(\"\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThis script uses the `Tokenizer` class to split the prompt into tokens and then iterates over the tokens in a loop. For each token, it draws an arrow from the previous token to the current token, as well as a heatmap corresponding to the text of the current token.\n\nPlease note that this is just one possible way to visualize the prompt using Pygame, and you can modify the script to better suit your needs.", "response": "\"\"\"\n    Here I thought it would be interesting to visualize the cross attention\n    maps produced by the UNet of a text-to-image diffusion model. \n\n    The key thing I want to show is how images and text are broken up into tokens (patches for images),\n    and those tokens are used to compute a cross attention score, which can be interpreted\n    as a 2D heatmap over the image patches for each text token. \n    \n\n    Necessary operations:\n        1. [X] Split an image into patches and \"expand\" the patches outward to highlight that they are \n            separate. \n        2. Split text into tokens using the tokenizer and display them. \n        3. Compute the cross attention scores (using DAAM) for each word/token and display them as a heatmap.\n        4. Overlay the heatmap over the image patches. Show the overlaying as a transition animation. \n\"\"\"\nimport torch\nimport cv2\nfrom manim import *\nimport numpy as np\nfrom typing import List\nfrom daam import trace, set_seed\nfrom diffusers import StableDiffusionPipeline\nimport torchvision\nimport matplotlib.pyplot as plt\n\nclass ImagePatches(Group):\n    \"\"\"Container object for a grid of ImageMobjects.\"\"\"\n\n    def __init__(self, image: ImageMobject, grid_width=4):\n        \"\"\"\n        Parameters\n        ----------\n        image : ImageMobject\n            image to split into patches\n        grid_width : int, optional\n            number of patches per row, by default 4\n        \"\"\"\n        self.image = image\n        self.grid_width = grid_width\n        super(Group, self).__init__()\n        self.patch_dict = self._split_image_into_patches(image, grid_width)\n\n    def _split_image_into_patches(self, image, grid_width):\n        \"\"\"Splits the images into a set of patches\n\n        Parameters\n        ----------\n        image : ImageMobject\n            image to split into patches\n        grid_width : int\n            number of patches per row\n        \"\"\"\n        patch_dict = {}\n        # Get a pixel array of the image mobject\n        pixel_array = image.pixel_array\n        # Get the height and width of the image\n        height, width = pixel_array.shape[:2]\n        # Split the image into an equal width grid of patches\n        assert height == width, \"Image must be square\"\n        assert height % grid_width == 0, \"Image width must be divisible by grid_width\"\n        patch_width, patch_height = width // grid_width, height // grid_width\n\n        for patch_i in range(self.grid_width):\n            for patch_j in range(self.grid_width):\n                # Get patch pixels\n                i_start, i_end = patch_i * patch_width, (patch_i + 1) * patch_width\n                j_start, j_end = patch_j * patch_height, (patch_j + 1) * patch_height\n                patch_pixels = pixel_array[i_start:i_end, j_start:j_end, :]\n                # Make the patch\n                image_patch = ImageMobject(patch_pixels)\n                # Move the patch to the correct location on the old image\n                image_width = image.get_width()\n                image_center = image.get_center()\n                image_patch.scale(image_width / grid_width / image_patch.get_width())\n                patch_manim_space_width = image_patch.get_width()\n\n                patch_center = image_center\n                patch_center += (patch_j - self.grid_width / 2 + 0.5) * patch_manim_space_width * RIGHT\n                # patch_center = image_center - (patch_i - self.grid_width / 2 + 0.5) * patch_manim_space_width * RIGHT\n                patch_center -= (patch_i - self.grid_width / 2 + 0.5) * patch_manim_space_width * UP\n                # + (patch_j - self.grid_width / 2) * patch_height / 2 * UP\n\n                image_patch.move_to(patch_center)\n\n                self.add(image_patch)\n                patch_dict[(patch_i, patch_j)] = image_patch\n\n        return patch_dict\n\nclass ExpandPatches(Animation):\n\n    def __init__(self, image_patches: ImagePatches, expansion_scale=2.0):\n        \"\"\"\n        Parameters\n        ----------\n        image_patches : ImagePatches\n            set of image patches\n        expansion_scale : float, optional\n            scale factor for expansion, by default 2.0\n        \"\"\"\n        self.image_patches = image_patches\n        self.expansion_scale = expansion_scale\n        super().__init__(image_patches)\n\n    def interpolate_submobject(self, submobject, starting_submobject, alpha):\n        \"\"\"\n        Parameters\n        ----------\n        submobject : ImageMobject\n            current patch\n        starting_submobject : ImageMobject\n            starting patch\n        alpha : float\n            interpolation alpha\n        \"\"\"\n        patch = submobject\n        starting_patch_center = starting_submobject.get_center()\n        image_center = self.image_patches.image.get_center()\n        # Start image vector\n        starting_patch_vector = starting_patch_center - image_center\n        final_image_vector = image_center + starting_patch_vector * self.expansion_scale\n        # Interpolate vectors\n        current_location = alpha * final_image_vector + (1 - alpha) * starting_patch_center\n        # # Need to compute the direction of expansion as the unit vector from the original image center\n        # # and patch center. \n        patch.move_to(current_location)\n\nclass TokenizedText(Group):\n    \"\"\"Tokenizes the given text and displays the tokens as a list of Text Mobjects.\"\"\"\n\n    def __init__(self, text, tokenizer=None):\n        self.text = text\n        if not tokenizer is None:\n            self.tokenizer = tokenizer\n        else:\n            # TODO load default stable diffusion tokenizer here\n            raise NotImplementedError(\"Tokenizer must be provided\")\n\n        self.token_strings = self._tokenize_text(text)\n        self.token_mobjects = self.make_text_mobjects(self.token_strings)\n\n        super(Group, self).__init__()\n        self.add(*self.token_mobjects)\n        self.arrange(RIGHT, buff=-0.05, aligned_edge=UP)\n\n        token_dict = {} \n        for token_index, token_string in enumerate(self.token_strings):\n            token_dict[token_string] = self.token_mobjects[token_index]\n\n        self.token_dict = token_dict\n\n    def _tokenize_text(self, text):\n        \"\"\"Tokenize the text using the tokenizer.\n\n        Parameters\n        ----------\n        text : str\n            text to tokenize\n        \"\"\"\n        tokens = self.tokenizer.tokenize(text)\n        tokens = [token.replace(\"</w>\", \"\") for token in tokens]\n        return tokens\n\n    def make_text_mobjects(self, tokens_list: List[str]):\n        \"\"\"Converts the tokens into a list of TextMobjects.\"\"\"\n        # Make the tokens\n        # Insert phantom\n        tokens_list = [\"l\" + token + \"g\" for token in tokens_list]\n        token_objects = [\n            Text(token, t2c={'[-1:]': '#000000', '[0:1]': \"#000000\"}, font_size=64) \n            for token in tokens_list\n        ]\n\n        return token_objects\n\ndef compute_stable_diffusion_cross_attention_heatmaps(\n    pipe,\n    prompt: str,\n    seed: int = 2,\n    map_resolution=(32, 32)\n):\n    \"\"\"Computes the cross attention heatmaps for the given prompt.\n\n    Parameters\n    ----------\n    prompt : str\n        the prompt\n    seed : int, optional\n        random seed, by default 0\n    map_resolution : tuple, optional\n        resolution to downscale maps to, by default (16, 16)\n\n    Returns\n    -------\n    _type_\n        _description_\n    \"\"\"\n    # Get tokens\n    tokens = pipe.tokenizer.tokenize(prompt)\n    tokens = [token.replace(\"</w>\", \"\") for token in tokens]\n    # Set torch seeds\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    gen = set_seed(seed)  # for reproducibility\n\n    heatmap_dict = {}\n    image = None\n\n    with torch.cuda.amp.autocast(dtype=torch.float16), torch.no_grad():\n        with trace(pipe) as tc:\n            out = pipe(prompt, num_inference_steps=30, generator=gen)\n            image = out[0][0]\n            global_heat_map = tc.compute_global_heat_map()\n            for token in tokens: \n                word_heat_map = global_heat_map.compute_word_heat_map(token)\n                heatmap = word_heat_map.heatmap\n\n                # Downscale the heatmap\n                heatmap = heatmap.unsqueeze(0).unsqueeze(0)\n                # Save the heatmap\n                heatmap = torchvision.transforms.Resize(\n                    map_resolution, \n                    interpolation=torchvision.transforms.InterpolationMode.NEAREST\n                )(heatmap)\n                heatmap = heatmap.squeeze(0).squeeze(0)\n                # heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n                # plt.imshow(heatmap)\n                # plt.savefig(f\"{token}.png\")\n                # Convert heatmap to rgb color\n                # normalize\n                heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n                cmap = plt.get_cmap('inferno')\n                heatmap = cmap(heatmap) * 255\n                print(heatmap)\n\n                # print(heatmap)\n                # Make an image mobject for each heatmap\n                print(heatmap.shape)\n                heatmap = ImageMobject(heatmap)\n                heatmap.set_resampling_algorithm(RESAMPLING_ALGORITHMS[\"nearest\"])\n                heatmap_dict[token] = heatmap\n\n    return image, heatmap_dict\n\n# Make the scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 30.0\nconfig.frame_width = 30.0\n\nclass StableDiffusionCrossAttentionScene(Scene):\n\n    def construct(self):\n        # Load the pipeline\n        model_id = 'stabilityai/stable-diffusion-2-base'\n        pipe = StableDiffusionPipeline.from_pretrained(model_id)\n        prompt = \"Astronaut riding a horse on the moon\"\n        # Compute the images and heatmaps\n        image, heatmap_dict = compute_stable_diffusion_cross_attention_heatmaps(pipe, prompt)\n        # 1. Show an image appearing\n        image_mobject = ImageMobject(image)\n        image_mobject.shift(DOWN)\n        image_mobject.scale(0.7)\n        image_mobject.shift(LEFT * 7)\n        self.add(image_mobject)\n        # 1. Show a text prompt and the corresponding generated image\n        paragraph_prompt = '\"Astronaut riding a\\nhorse on the moon\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        text_prompt.next_to(image_mobject, UP, buff=1.5)\n        prompt_title = Text(\"Prompt\", font_size=72)\n        prompt_title.next_to(text_prompt, UP, buff=0.5)\n        self.play(Create(prompt_title))\n        self.play(Create(text_prompt))        \n        # Make an arrow from the text to the image\n        arrow = Arrow(text_prompt.get_bottom(), image_mobject.get_top(), buff=0.5)\n        self.play(GrowArrow(arrow))\n        self.wait(1)\n        self.remove(arrow)\n        # 2. Show the image being split into patches\n        # Make the patches\n        patches = ImagePatches(image_mobject, grid_width=32)\n        # Expand and contract \n        self.remove(image_mobject)\n        self.play(ExpandPatches(patches, expansion_scale=1.2))\n        # Play arrows for visual tokens\n        visual_token_label = Text(\"Visual Tokens\", font_size=72)\n        visual_token_label.next_to(image_mobject, DOWN, buff=1.5)\n        # Draw arrows \n        arrow_animations = []\n        arrows = []\n        for i in range(patches.grid_width):\n            patch = patches.patch_dict[(patches.grid_width - 1, i)]\n            arrow = Line(visual_token_label.get_top(), patch.get_bottom(), buff=0.3)\n            arrow_animations.append(\n                Create(\n                    arrow\n                )\n            )\n            arrows.append(arrow)\n        self.play(AnimationGroup(*arrow_animations, FadeIn(visual_token_label), lag_ratio=0))\n        self.wait(1)\n        self.play(FadeOut(*arrows, visual_token_label))\n        self.play(ExpandPatches(patches, expansion_scale=1/1.2))\n        # 3. Show the text prompt and image being tokenized.\n        tokenized_text = TokenizedText(prompt, pipe.tokenizer)\n        tokenized_text.shift(RIGHT * 7)\n        tokenized_text.shift(DOWN * 7.5)\n        self.play(FadeIn(tokenized_text))\n        # Plot token label\n        token_label = Text(\"Textual Tokens\", font_size=72)\n        token_label.next_to(tokenized_text, DOWN, buff=0.5)\n        arrow_animations = []\n        self.play(Create(token_label)) \n        arrows = []\n        for token_name, token_mobject in tokenized_text.token_dict.items():\n            arrow = Line(token_label.get_top(), token_mobject.get_bottom(), buff=0.3)\n            arrow_animations.append(\n                Create(\n                    arrow\n                )\n            )\n            arrows.append(arrow)\n        self.play(AnimationGroup(*arrow_animations, lag_ratio=0))\n        self.wait(1)\n        self.play(FadeOut(*arrows, token_label))\n        # 4. Show the heatmap corresponding to the cross attention map for each image. \n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"astronaut\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"astronaut\"\n        astronaut_heatmap = heatmap_dict[\"astronaut\"]\n        astronaut_heatmap.height = image_mobject.get_height()\n        astronaut_heatmap.shift(RIGHT * 7)\n        astronaut_heatmap.shift(DOWN)\n        self.play(FadeIn(astronaut_heatmap))\n        self.wait(3)\n        self.remove(surrounding_rectangle)\n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"horse\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"horse\"\n        horse_heatmap = heatmap_dict[\"horse\"]\n        horse_heatmap.height = image_mobject.get_height()\n        horse_heatmap.move_to(astronaut_heatmap)\n        self.play(FadeOut(astronaut_heatmap))\n        self.play(FadeIn(horse_heatmap))\n        self.wait(3)\n        self.remove(surrounding_rectangle)\n        surrounding_rectangle = SurroundingRectangle(tokenized_text.token_dict[\"riding\"], buff=0.1)\n        self.add(surrounding_rectangle)\n        # Show the heatmap for \"riding\"\n        riding_heatmap = heatmap_dict[\"riding\"]\n        riding_heatmap.height = image_mobject.get_height()\n        riding_heatmap.move_to(horse_heatmap)\n        self.play(FadeOut(horse_heatmap))\n        self.play(FadeIn(riding_heatmap))\n        self.wait(3)\n", "original_prompt": "Cross Attention Vis Cross Attention Vis"}
{"prompt": "\"Graphically illustrate a decision tree's bounding boxes with overlapping rectangles of different colors.\"", "response": "import numpy as np\nfrom collections import deque\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import _tree as ctree\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n\nclass AABB:\n    \"\"\"Axis-aligned bounding box\"\"\"\n\n    def __init__(self, n_features):\n        self.limits = np.array([[-np.inf, np.inf]] * n_features)\n\n    def split(self, f, v):\n        left = AABB(self.limits.shape[0])\n        right = AABB(self.limits.shape[0])\n        left.limits = self.limits.copy()\n        right.limits = self.limits.copy()\n        left.limits[f, 1] = v\n        right.limits[f, 0] = v\n\n        return left, right\n\n\ndef tree_bounds(tree, n_features=None):\n    \"\"\"Compute final decision rule for each node in tree\"\"\"\n    if n_features is None:\n        n_features = np.max(tree.feature) + 1\n    aabbs = [AABB(n_features) for _ in range(tree.node_count)]\n    queue = deque([0])\n    while queue:\n        i = queue.pop()\n        l = tree.children_left[i]\n        r = tree.children_right[i]\n        if l != ctree.TREE_LEAF:\n            aabbs[l], aabbs[r] = aabbs[i].split(tree.feature[i], tree.threshold[i])\n            queue.extend([l, r])\n    return aabbs\n\n\ndef decision_areas(tree_classifier, maxrange, x=0, y=1, n_features=None):\n    \"\"\"Extract decision areas.\n\n    tree_classifier: Instance of a sklearn.tree.DecisionTreeClassifier\n    maxrange: values to insert for [left, right, top, bottom] if the interval is open (+/-inf)\n    x: index of the feature that goes on the x axis\n    y: index of the feature that goes on the y axis\n    n_features: override autodetection of number of features\n    \"\"\"\n    tree = tree_classifier.tree_\n    aabbs = tree_bounds(tree, n_features)\n    maxrange = np.array(maxrange)\n    rectangles = []\n    for i in range(len(aabbs)):\n        if tree.children_left[i] != ctree.TREE_LEAF:\n            continue\n        l = aabbs[i].limits\n        r = [l[x, 0], l[x, 1], l[y, 0], l[y, 1], np.argmax(tree.value[i])]\n        # clip out of bounds indices\n        \"\"\"\n        if r[0] < maxrange[0]:\n            r[0] = maxrange[0]\n        if r[1] > maxrange[1]:\n            r[1] = maxrange[1]\n        if r[2] < maxrange[2]:\n            r[2] = maxrange[2]\n        if r[3] > maxrange[3]:\n            r[3] = maxrange[3]\n        print(r)\n        \"\"\"\n        rectangles.append(r)\n    rectangles = np.array(rectangles)\n    rectangles[:, [0, 2]] = np.maximum(rectangles[:, [0, 2]], maxrange[0::2])\n    rectangles[:, [1, 3]] = np.minimum(rectangles[:, [1, 3]], maxrange[1::2])\n    return rectangles\n\n\ndef plot_areas(rectangles):\n    for rect in rectangles:\n        color = [\"b\", \"r\"][int(rect[4])]\n        print(rect[0], rect[1], rect[2] - rect[0], rect[3] - rect[1])\n        rp = Rectangle(\n            [rect[0], rect[2]],\n            rect[1] - rect[0],\n            rect[3] - rect[2],\n            color=color,\n            alpha=0.3,\n        )\n        plt.gca().add_artist(rp)\n", "original_prompt": "Decision Tree Decision Tree Surface"}
{"prompt": "It appears that you have a long piece of code written in Python, specifically using the Pygame library for animation and mathematical operations on iris dataset.\n\nThe provided code seems to be constructing an animation of a decision tree classifier. Here's a high-level overview of what it does:\n\n1. Creates a `IrisDatasetPlot` object to plot the iris dataset.\n2. Constructs a decision tree classifier using the `make_decision_tree_classifier` method.\n3. Creates a `DecisionTreeSurface` object to visualize the decision tree.\n4. Animates the creation and destruction of the decision tree surface, as well as other elements like lines and peaks.\n\nHere's some sample Python code that might be used to create this animation:\n\n```python\nimport pygame\nfrom math import sin, cos, pi\n\nclass IrisDatasetPlot:\n    def __init__(self):\n        self.data = []\n        # Initialize iris dataset data here...\n\n    def plot(self):\n        # Plot the iris dataset using Pygame...\n        pass\n\nclass DecisionTreeSurface:\n    def __init__(self, classifier, data, axes_group):\n        self.classifier = classifier\n        self.data = data\n        self.axes_group = axes_group\n\n    def draw_tree(self):\n        # Draw the decision tree using Pygame...\n        pass\n\nclass Animation:\n    def __init__(self):\n        self.pygame_init()\n\n    def pygame_init(self):\n        pygame.init()\n        # Set up Pygame display and clock...\n\n    def construct(self, plot, classifier, data):\n        # Create the animation\n        iris_dataset_plot = IrisDatasetPlot()\n        iris_dataset_plot.data = data\n\n        decision_tree_classifier = classifier\n        decision_tree_surface = DecisionTreeSurface(decision_tree_classifier, data, plot.axes_group)\n\n        self.play(Create(iris_dataset_plot))\n        self.play(Create(decision_tree_surface))\n\n        # Add animations for other elements...\n```\n\nThis code snippet provides a basic structure for creating an animation of a decision tree classifier using Pygame. However, it lacks the actual implementation details, which would depend on the specific requirements and functionality of your project.\n\nTo further assist you with your question, I need more information about what specific parts of this code are causing issues or require clarification.", "response": "from sklearn import datasets\nfrom decision_tree_surface import *\nfrom manim import *\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import entropy\nimport math\nfrom PIL import Image\n\niris = datasets.load_iris()\nfont = \"Source Han Sans\"\nfont_scale = 0.75\n\nimages = [\n    Image.open(\"iris_dataset/SetosaFlower.jpeg\"),\n    Image.open(\"iris_dataset/VeriscolorFlower.jpeg\"),\n    Image.open(\"iris_dataset/VirginicaFlower.jpeg\"),\n]\n\n\ndef entropy(class_labels, base=2):\n    # compute the class counts\n    unique, counts = np.unique(class_labels, return_counts=True)\n    dictionary = dict(zip(unique, counts))\n    total = 0.0\n    num_samples = len(class_labels)\n    for class_index in range(0, 3):\n        if not class_index in dictionary:\n            continue\n        prob = dictionary[class_index] / num_samples\n        total += prob * math.log(prob, base)\n    # higher set\n    return -total\n\n\ndef merge_overlapping_polygons(all_polygons, colors=[BLUE, GREEN, ORANGE]):\n    # get all polygons of each color\n    polygon_dict = {\n        str(BLUE).lower(): [],\n        str(GREEN).lower(): [],\n        str(ORANGE).lower(): [],\n    }\n    for polygon in all_polygons:\n        print(polygon_dict)\n        polygon_dict[str(polygon.color).lower()].append(polygon)\n\n    return_polygons = []\n    for color in colors:\n        color = str(color).lower()\n        polygons = polygon_dict[color]\n        points = set()\n        for polygon in polygons:\n            vertices = polygon.get_vertices().tolist()\n            vertices = [tuple(vert) for vert in vertices]\n            for pt in vertices:\n                if pt in points:  # Shared vertice, remove it.\n                    points.remove(pt)\n                else:\n                    points.add(pt)\n        points = list(points)\n        sort_x = sorted(points)\n        sort_y = sorted(points, key=lambda x: x[1])\n\n        edges_h = {}\n        edges_v = {}\n\n        i = 0\n        while i < len(points):\n            curr_y = sort_y[i][1]\n            while i < len(points) and sort_y[i][1] == curr_y:\n                edges_h[sort_y[i]] = sort_y[i + 1]\n                edges_h[sort_y[i + 1]] = sort_y[i]\n                i += 2\n        i = 0\n        while i < len(points):\n            curr_x = sort_x[i][0]\n            while i < len(points) and sort_x[i][0] == curr_x:\n                edges_v[sort_x[i]] = sort_x[i + 1]\n                edges_v[sort_x[i + 1]] = sort_x[i]\n                i += 2\n\n        # Get all the polygons.\n        while edges_h:\n            # We can start with any point.\n            polygon = [(edges_h.popitem()[0], 0)]\n            while True:\n                curr, e = polygon[-1]\n                if e == 0:\n                    next_vertex = edges_v.pop(curr)\n                    polygon.append((next_vertex, 1))\n                else:\n                    next_vertex = edges_h.pop(curr)\n                    polygon.append((next_vertex, 0))\n                if polygon[-1] == polygon[0]:\n                    # Closed polygon\n                    polygon.pop()\n                    break\n            # Remove implementation-markers from the polygon.\n            poly = [point for point, _ in polygon]\n            for vertex in poly:\n                if vertex in edges_h:\n                    edges_h.pop(vertex)\n                if vertex in edges_v:\n                    edges_v.pop(vertex)\n            polygon = Polygon(*poly, color=color, fill_opacity=0.3, stroke_opacity=1.0)\n            return_polygons.append(polygon)\n    return return_polygons\n\n\nclass IrisDatasetPlot(VGroup):\n    def __init__(self):\n        points = iris.data[:, 0:2]\n        labels = iris.feature_names\n        targets = iris.target\n        # Make points\n        self.point_group = self._make_point_group(points, targets)\n        # Make axes\n        self.axes_group = self._make_axes_group(points, labels)\n        # Make legend\n        self.legend_group = self._make_legend(\n            [BLUE, ORANGE, GREEN], iris.target_names, self.axes_group\n        )\n        # Make title\n        # title_text = \"Iris Dataset Plot\"\n        # self.title = Text(title_text).match_y(self.axes_group).shift([0.5, self.axes_group.height / 2 + 0.5, 0])\n        # Make all group\n        self.all_group = Group(self.point_group, self.axes_group, self.legend_group)\n        # scale the groups\n        self.point_group.scale(1.6)\n        self.point_group.match_x(self.axes_group)\n        self.point_group.match_y(self.axes_group)\n        self.point_group.shift([0.2, 0, 0])\n        self.axes_group.scale(0.7)\n        self.all_group.shift([0, 0.2, 0])\n\n    @override_animation(Create)\n    def create_animation(self):\n        animation_group = AnimationGroup(\n            # Perform the animations\n            Create(self.point_group, run_time=2),\n            Wait(0.5),\n            Create(self.axes_group, run_time=2),\n            # add title\n            # Create(self.title),\n            Create(self.legend_group),\n        )\n        return animation_group\n\n    def _make_point_group(self, points, targets, class_colors=[BLUE, ORANGE, GREEN]):\n        point_group = VGroup()\n        for point_index, point in enumerate(points):\n            # draw the dot\n            current_target = targets[point_index]\n            color = class_colors[current_target]\n            dot = Dot(point=np.array([point[0], point[1], 0])).set_color(color)\n            dot.scale(0.5)\n            point_group.add(dot)\n        return point_group\n\n    def _make_legend(self, class_colors, feature_labels, axes):\n        legend_group = VGroup()\n        # Make Text\n        setosa = Text(\"Setosa\", color=BLUE)\n        verisicolor = Text(\"Verisicolor\", color=ORANGE)\n        virginica = Text(\"Virginica\", color=GREEN)\n        labels = VGroup(setosa, verisicolor, virginica).arrange(\n            direction=RIGHT, aligned_edge=LEFT, buff=2.0\n        )\n        labels.scale(0.5)\n        legend_group.add(labels)\n        # surrounding rectangle\n        surrounding_rectangle = SurroundingRectangle(labels, color=WHITE)\n        surrounding_rectangle.move_to(labels)\n        legend_group.add(surrounding_rectangle)\n        # shift the legend group\n        legend_group.move_to(axes)\n        legend_group.shift([0, -3.0, 0])\n        legend_group.match_x(axes[0][0])\n\n        return legend_group\n\n    def _make_axes_group(self, points, labels):\n        axes_group = VGroup()\n        # make the axes\n        x_range = [\n            np.amin(points, axis=0)[0] - 0.2,\n            np.amax(points, axis=0)[0] - 0.2,\n            0.5,\n        ]\n        y_range = [np.amin(points, axis=0)[1] - 0.2, np.amax(points, axis=0)[1], 0.5]\n        axes = Axes(\n            x_range=x_range,\n            y_range=y_range,\n            x_length=9,\n            y_length=6.5,\n            # axis_config={\"number_scale_value\":0.75, \"include_numbers\":True},\n            tips=False,\n        ).shift([0.5, 0.25, 0])\n        axes_group.add(axes)\n        # make axis labels\n        # x_label\n        x_label = (\n            Text(labels[0], font=font)\n            .match_y(axes.get_axes()[0])\n            .shift([0.5, -0.75, 0])\n            .scale(font_scale)\n        )\n        axes_group.add(x_label)\n        # y_label\n        y_label = (\n            Text(labels[1], font=font)\n            .match_x(axes.get_axes()[1])\n            .shift([-0.75, 0, 0])\n            .rotate(np.pi / 2)\n            .scale(font_scale)\n        )\n        axes_group.add(y_label)\n\n        return axes_group\n\n\nclass DecisionTreeSurface(VGroup):\n    def __init__(self, tree_clf, data, axes, class_colors=[BLUE, ORANGE, GREEN]):\n        # take the tree and construct the surface from it\n        self.tree_clf = tree_clf\n        self.data = data\n        self.axes = axes\n        self.class_colors = class_colors\n        self.surface_rectangles = self.generate_surface_rectangles()\n\n    def generate_surface_rectangles(self):\n        # compute data bounds\n        left = np.amin(self.data[:, 0]) - 0.2\n        right = np.amax(self.data[:, 0]) - 0.2\n        top = np.amax(self.data[:, 1])\n        bottom = np.amin(self.data[:, 1]) - 0.2\n        maxrange = [left, right, bottom, top]\n        rectangles = decision_areas(self.tree_clf, maxrange, x=0, y=1, n_features=2)\n        # turn the rectangle objects into manim rectangles\n        def convert_rectangle_to_polygon(rect):\n            # get the points for the rectangle in the plot coordinate frame\n            bottom_left = [rect[0], rect[3]]\n            bottom_right = [rect[1], rect[3]]\n            top_right = [rect[1], rect[2]]\n            top_left = [rect[0], rect[2]]\n            # convert those points into the entire manim coordinates\n            bottom_left_coord = self.axes.coords_to_point(*bottom_left)\n            bottom_right_coord = self.axes.coords_to_point(*bottom_right)\n            top_right_coord = self.axes.coords_to_point(*top_right)\n            top_left_coord = self.axes.coords_to_point(*top_left)\n            points = [\n                bottom_left_coord,\n                bottom_right_coord,\n                top_right_coord,\n                top_left_coord,\n            ]\n            # construct a polygon object from those manim coordinates\n            rectangle = Polygon(\n                *points, color=color, fill_opacity=0.3, stroke_opacity=0.0\n            )\n            return rectangle\n\n        manim_rectangles = []\n        for rect in rectangles:\n            color = self.class_colors[int(rect[4])]\n            rectangle = convert_rectangle_to_polygon(rect)\n            manim_rectangles.append(rectangle)\n\n        manim_rectangles = merge_overlapping_polygons(\n            manim_rectangles, colors=[BLUE, GREEN, ORANGE]\n        )\n\n        return manim_rectangles\n\n    @override_animation(Create)\n    def create_override(self):\n        # play a reveal of all of the surface rectangles\n        animations = []\n        for rectangle in self.surface_rectangles:\n            animations.append(Create(rectangle))\n        animation_group = AnimationGroup(*animations)\n\n        return animation_group\n\n    @override_animation(Uncreate)\n    def uncreate_override(self):\n        # play a reveal of all of the surface rectangles\n        animations = []\n        for rectangle in self.surface_rectangles:\n            animations.append(Uncreate(rectangle))\n        animation_group = AnimationGroup(*animations)\n\n        return animation_group\n\n\nclass DecisionTree:\n    \"\"\"\n    Draw a single tree node\n    \"\"\"\n\n    def _make_node(\n        self,\n        feature,\n        threshold,\n        values,\n        is_leaf=False,\n        depth=0,\n        leaf_colors=[BLUE, ORANGE, GREEN],\n    ):\n        if not is_leaf:\n            node_text = f\"{feature}\\n    <= {threshold:.3f} cm\"\n            # draw decision text\n            decision_text = Text(node_text, color=WHITE)\n            # draw a box\n            bounding_box = SurroundingRectangle(decision_text, buff=0.3, color=WHITE)\n            node = VGroup()\n            node.add(bounding_box)\n            node.add(decision_text)\n            # return the node\n        else:\n            # plot the appropriate image\n            class_index = np.argmax(values)\n            # get image\n            pil_image = images[class_index]\n            leaf_group = Group()\n            node = ImageMobject(pil_image)\n            node.scale(1.5)\n            rectangle = Rectangle(\n                width=node.width + 0.05,\n                height=node.height + 0.05,\n                color=leaf_colors[class_index],\n                stroke_width=6,\n            )\n            rectangle.move_to(node.get_center())\n            rectangle.shift([-0.02, 0.02, 0])\n            leaf_group.add(rectangle)\n            leaf_group.add(node)\n            node = leaf_group\n\n        return node\n\n    def _make_connection(self, top, bottom, is_leaf=False):\n        top_node_bottom_location = top.get_center()\n        top_node_bottom_location[1] -= top.height / 2\n        bottom_node_top_location = bottom.get_center()\n        bottom_node_top_location[1] += bottom.height / 2\n        line = Line(top_node_bottom_location, bottom_node_top_location, color=WHITE)\n        return line\n\n    def _make_tree(self, tree, feature_names=[\"Sepal Length\", \"Sepal Width\"]):\n        tree_group = Group()\n        max_depth = tree.max_depth\n        # make the base node\n        feature_name = feature_names[tree.feature[0]]\n        threshold = tree.threshold[0]\n        values = tree.value[0]\n        nodes_map = {}\n        root_node = self._make_node(feature_name, threshold, values, depth=0)\n        nodes_map[0] = root_node\n        tree_group.add(root_node)\n        # save some information\n        node_height = root_node.height\n        node_width = root_node.width\n        scale_factor = 1.0\n        edge_map = {}\n        # tree height\n        tree_height = scale_factor * node_height * max_depth\n        tree_width = scale_factor * 2**max_depth * node_width\n        # traverse tree\n        def recurse(node, depth, direction, parent_object, parent_node):\n            # make sure it is a valid node\n            # make the node object\n            is_leaf = tree.children_left[node] == tree.children_right[node]\n            feature_name = feature_names[tree.feature[node]]\n            threshold = tree.threshold[node]\n            values = tree.value[node]\n            node_object = self._make_node(\n                feature_name, threshold, values, depth=depth, is_leaf=is_leaf\n            )\n            nodes_map[node] = node_object\n            node_height = node_object.height\n            # set the node position\n            direction_factor = -1 if direction == \"left\" else 1\n            shift_right_amount = (\n                0.8 * direction_factor * scale_factor * tree_width / (2**depth) / 2\n            )\n            if is_leaf:\n                shift_down_amount = -1.0 * scale_factor * node_height\n            else:\n                shift_down_amount = -1.8 * scale_factor * node_height\n            node_object.match_x(parent_object).match_y(parent_object).shift(\n                [shift_right_amount, shift_down_amount, 0]\n            )\n            tree_group.add(node_object)\n            # make a connection\n            connection = self._make_connection(\n                parent_object, node_object, is_leaf=is_leaf\n            )\n            edge_name = str(parent_node) + \",\" + str(node)\n            edge_map[edge_name] = connection\n            tree_group.add(connection)\n            # recurse\n            if not is_leaf:\n                recurse(tree.children_left[node], depth + 1, \"left\", node_object, node)\n                recurse(\n                    tree.children_right[node], depth + 1, \"right\", node_object, node\n                )\n\n        recurse(tree.children_left[0], 1, \"left\", root_node, 0)\n        recurse(tree.children_right[0], 1, \"right\", root_node, 0)\n\n        tree_group.scale(0.35)\n        return tree_group, nodes_map, edge_map\n\n    def color_example_path(\n        self, tree_group, nodes_map, tree, edge_map, example, color=YELLOW, thickness=2\n    ):\n        # get decision path\n        decision_path = tree.decision_path(example)[0]\n        path_indices = decision_path.indices\n        # highlight edges\n        for node_index in range(0, len(path_indices) - 1):\n            current_val = path_indices[node_index]\n            next_val = path_indices[node_index + 1]\n            edge_str = str(current_val) + \",\" + str(next_val)\n            edge = edge_map[edge_str]\n            animation_two = AnimationGroup(\n                nodes_map[current_val].animate.set_color(color)\n            )\n            self.play(animation_two, run_time=0.5)\n            animation_one = AnimationGroup(\n                edge.animate.set_color(color),\n                # edge.animate.set_stroke_width(4),\n            )\n            self.play(animation_one, run_time=0.5)\n        # surround the bottom image\n        last_path_index = path_indices[-1]\n        last_path_rectangle = nodes_map[last_path_index][0]\n        self.play(last_path_rectangle.animate.set_color(color))\n\n    def create_sklearn_tree(self, max_tree_depth=1):\n        # learn the decision tree\n        iris = load_iris()\n        tree = learn_iris_decision_tree(iris, max_depth=max_tree_depth)\n        feature_names = iris.feature_names[0:2]\n        return tree.tree_\n\n    def make_tree(self, max_tree_depth=2):\n        sklearn_tree = self.create_sklearn_tree()\n        # make the tree\n        tree_group, nodes_map, edge_map = self._make_tree(\n            sklearn_tree.tree_, feature_names\n        )\n        tree_group.shift([0, 5.5, 0])\n        return tree_group\n        # self.add(tree_group)\n        # self.play(SpinInFromNothing(tree_group), run_time=3)\n        # self.color_example_path(tree_group, nodes_map, tree, edge_map, iris.data[None, 0, 0:2])\n\n\nclass DecisionTreeSplitScene(Scene):\n    def make_decision_tree_classifier(self, max_depth=4):\n        decision_tree = DecisionTreeClassifier(\n            random_state=1, max_depth=max_depth, max_leaf_nodes=8\n        )\n        decision_tree = decision_tree.fit(iris.data[:, :2], iris.target)\n        # output the decisioin tree in some format\n        return decision_tree\n\n    def make_split_animation(self, data, classes, data_labels, main_axes):\n\n        \"\"\"\n        def make_entropy_animation_and_plot(dim=0, num_entropy_values=50):\n            # calculate the entropy values\n            axes_group = VGroup()\n            # make axes\n            range_vals = [np.amin(data, axis=0)[dim], np.amax(data, axis=0)[dim]]\n            axes = Axes(x_range=range_vals,\n                        y_range=[0, 1.0],\n                        x_length=9,\n                        y_length=4,\n            #            axis_config={\"number_scale_value\":0.75, \"include_numbers\":True},\n                        tips=False,\n                    )\n            axes_group.add(axes)\n            # make axis labels\n            # x_label\n            x_label = Text(data_labels[dim], font=font) \\\n                            .match_y(axes.get_axes()[0]) \\\n                            .shift([0.5, -0.75, 0]) \\\n                            .scale(font_scale*1.2)\n            axes_group.add(x_label)\n            # y_label\n            y_label = Text(\"Information Gain\", font=font) \\\n                .match_x(axes.get_axes()[1]) \\\n                .shift([-0.75, 0, 0]) \\\n                .rotate(np.pi / 2) \\\n                .scale(font_scale * 1.2)\n\n            axes_group.add(y_label)\n            # line animation\n            information_gains = []\n            def entropy_function(split_value):\n                # lower entropy\n                lower_set = np.nonzero(data[:, dim] <= split_value)[0]\n                lower_set = classes[lower_set]\n                lower_entropy = entropy(lower_set)\n                # higher entropy\n                higher_set = np.nonzero(data[:, dim] > split_value)[0]\n                higher_set = classes[higher_set]\n                higher_entropy = entropy(higher_set)\n                # calculate entropies\n                all_entropy = entropy(classes, base=2)\n                lower_entropy = entropy(lower_set, base=2)\n                higher_entropy = entropy(higher_set, base=2)\n                mean_entropy = (lower_entropy + higher_entropy) / 2\n                # calculate information gain\n                lower_prob = len(lower_set) / len(data[:, dim])\n                higher_prob = len(higher_set) / len(data[:, dim])\n                info_gain = all_entropy - (lower_prob * lower_entropy + higher_prob * higher_entropy)\n                information_gains.append((split_value, info_gain))\n                return info_gain\n\n            data_range = np.amin(data[:, dim]), np.amax(data[:, dim])\n\n            entropy_graph = axes.get_graph(\n                entropy_function, \n            #    color=RED, \n            #    x_range=data_range\n            )\n            axes_group.add(entropy_graph)\n            axes_group.shift([4.0, 2, 0])\n            axes_group.scale(0.5)\n            dot_animation = Dot(color=WHITE)\n            axes_group.add(dot_animation)\n            # make animations\n            animation_group = AnimationGroup(\n                Create(axes_group, run_time=2),\n                Wait(3),\n                MoveAlongPath(dot_animation, entropy_graph, run_time=20, rate_func=rate_functions.ease_in_out_quad),\n                Wait(2)\n            )\n\n            return axes_group, animation_group, information_gains\n        \"\"\"\n\n        def make_split_line_animation(dim=0):\n            # make a line along one of the dims and move it up and down\n            origin_coord = [\n                np.amin(data, axis=0)[0] - 0.2,\n                np.amin(data, axis=0)[1] - 0.2,\n            ]\n            origin_point = main_axes.coords_to_point(*origin_coord)\n            top_left_coord = [origin_coord[0], np.amax(data, axis=0)[1]]\n            bottom_right_coord = [np.amax(data, axis=0)[0] - 0.2, origin_coord[1]]\n            if dim == 0:\n                other_coord = top_left_coord\n                moving_line_coord = bottom_right_coord\n            else:\n                other_coord = bottom_right_coord\n                moving_line_coord = top_left_coord\n            other_point = main_axes.coords_to_point(*other_coord)\n            moving_line_point = main_axes.coords_to_point(*moving_line_coord)\n            moving_line = Line(origin_point, other_point, color=RED)\n            movement_line = Line(origin_point, moving_line_point)\n            if dim == 0:\n                movement_line.shift([0, moving_line.height / 2, 0])\n            else:\n                movement_line.shift([moving_line.width / 2, 0, 0])\n            # move the moving line along the movement line\n            animation = MoveAlongPath(\n                moving_line,\n                movement_line,\n                run_time=20,\n                rate_func=rate_functions.ease_in_out_quad,\n            )\n            return animation, moving_line\n\n        # plot the line in white then make it invisible\n        # make an animation along the line\n        # make a\n        # axes_one_group, top_animation_group, info_gains = make_entropy_animation_and_plot(dim=0)\n        line_movement, first_moving_line = make_split_line_animation(dim=0)\n        # axes_two_group, bottom_animation_group, _ = make_entropy_animation_and_plot(dim=1)\n        second_line_movement, second_moving_line = make_split_line_animation(dim=1)\n        # axes_two_group.shift([0, -3, 0])\n        animation_group_one = AnimationGroup(\n            #    top_animation_group,\n            line_movement,\n        )\n\n        animation_group_two = AnimationGroup(\n            #    bottom_animation_group,\n            second_line_movement,\n        )\n        \"\"\"\n        both_axes_group = VGroup(\n            axes_one_group,\n            axes_two_group\n        )\n        \"\"\"\n\n        return (\n            animation_group_one,\n            animation_group_two,\n            first_moving_line,\n            second_moving_line,\n            None,\n            None,\n        )\n        #    both_axes_group, \\\n        #    info_gains\n\n    def construct(self):\n        # make the points\n        iris_dataset_plot = IrisDatasetPlot()\n        iris_dataset_plot.all_group.scale(1.0)\n        iris_dataset_plot.all_group.shift([-3, 0.2, 0])\n        # make the entropy line graph\n        # entropy_line_graph = self.draw_entropy_line_graph()\n        # arrange the plots\n        # do animations\n        self.play(Create(iris_dataset_plot))\n        # make the decision tree classifier\n        decision_tree_classifier = self.make_decision_tree_classifier()\n        decision_tree_surface = DecisionTreeSurface(\n            decision_tree_classifier, iris.data, iris_dataset_plot.axes_group[0]\n        )\n        self.play(Create(decision_tree_surface))\n        self.wait(3)\n        self.play(Uncreate(decision_tree_surface))\n        main_axes = iris_dataset_plot.axes_group[0]\n        (\n            split_animation_one,\n            split_animation_two,\n            first_moving_line,\n            second_moving_line,\n            both_axes_group,\n            info_gains,\n        ) = self.make_split_animation(\n            iris.data[:, 0:2], iris.target, iris.feature_names, main_axes\n        )\n        self.play(split_animation_one)\n        self.wait(0.1)\n        self.play(Uncreate(first_moving_line))\n        self.wait(3)\n        self.play(split_animation_two)\n        self.wait(0.1)\n        self.play(Uncreate(second_moving_line))\n        self.wait(0.1)\n        # highlight the maximum on top\n        # sort by second key\n        \"\"\"\n        highest_info_gain = sorted(info_gains, key=lambda x: x[1])[-1]\n        highest_info_gain_point = both_axes_group[0][0].coords_to_point(*highest_info_gain)\n        highlighted_peak = Dot(highest_info_gain_point, color=YELLOW)\n        # get location of highest info gain point\n        highest_info_gain_point_in_iris_graph = iris_dataset_plot.axes_group[0].coords_to_point(*[highest_info_gain[0], 0])\n        first_moving_line.start[0] = highest_info_gain_point_in_iris_graph[0]\n        first_moving_line.end[0] = highest_info_gain_point_in_iris_graph[0]\n        self.play(Create(highlighted_peak))\n        self.play(Create(first_moving_line))\n        text = Text(\"Highest Information Gain\")\n        text.scale(0.4)\n        text.move_to(highlighted_peak)\n        text.shift([0, 0.5, 0])\n        self.play(Create(text))\n        \"\"\"\n        self.wait(1)\n        # draw the basic tree\n        decision_tree_classifier = self.make_decision_tree_classifier(max_depth=1)\n        decision_tree_surface = DecisionTreeSurface(\n            decision_tree_classifier, iris.data, iris_dataset_plot.axes_group[0]\n        )\n        decision_tree_graph, _, _ = DecisionTree()._make_tree(\n            decision_tree_classifier.tree_\n        )\n        decision_tree_graph.match_y(iris_dataset_plot.axes_group)\n        decision_tree_graph.shift([4, 0, 0])\n        self.play(Create(decision_tree_surface))\n        uncreate_animation = AnimationGroup(\n            #    Uncreate(both_axes_group),\n            #    Uncreate(highlighted_peak),\n            Uncreate(second_moving_line),\n            #    Unwrite(text)\n        )\n        self.play(uncreate_animation)\n        self.wait(0.5)\n        self.play(FadeIn(decision_tree_graph))\n        # self.play(FadeIn(highlighted_peak))\n        self.wait(5)\n", "original_prompt": "Decision Tree Split Scene"}
{"prompt": "Here is the code with comments and improvements:\n\n```python\n# Create a new animation group for the forward diffusion process\nforward_random_walk_animation = RandomWalk(forward_chain, axes=axes).animate_random_walk()\n\n# Create an empty list to store the animations\nsynced_animations = []\n\n# Iterate over each timestep in the forward chain\nfor timestep, transition_animation in enumerate(forward_random_walk_animation.animations):\n    # Make an animation moving the time bar\n    time_bar_animation = current_time.animate.set_value(timestep)\n\n    # Make an animation replacing the current image with the timestep image\n    new_image = ImageMobject(stable_diffusion_images[timestep])\n    new_image = new_image.set_height(image_mobject.get_height())\n    new_image.move_to(image_mobject)\n    replace_image_animation = AnimationGroup(\n        FadeOut(image_mobject),\n        FadeIn(new_image),\n        lag_ratio=1.0\n    )\n\n    # Append the animations to the list\n    synced_animations.append(\n        Succession(\n            transition_animation, \n            time_bar_animation,\n            replace_image_animation,\n            lag_ratio=0.0\n        )\n    )\n\n# Play all the animations in succession\nself.play(\n    Succession(*synced_animations),\n)\n\n# Fade out the random walk\nself.play(forward_random_walk.fade_out_random_walk())\n\n# Update the title\nnew_title = Text(\"Forward Diffusion Process\", font_size=64)\nnew_title.move_to(title)\nself.play(ReplacementTransform(title, new_title))\n\n# Create a new animation group for the reverse diffusion process\nbackward_random_walk_animation = RandomWalk(reverse_chain, axes=axes).animate_random_walk()\n\n# Iterate over each timestep in the reverse chain\nfor timestep, transition_animation in enumerate(backward_random_walk_animation.animations):\n    # Make an animation moving the time bar\n    time_bar_animation = current_time.animate.set_value(timestep)\n\n    # Make an animation replacing the current image with the timestep image\n    new_image = ImageMobject(stable_diffusion_images[timestep])\n    new_image = new_image.set_height(image_mobject.get_height())\n    new_image.move_to(image_mobject)\n    replace_image_animation = AnimationGroup(\n        FadeOut(image_mobject),\n        FadeIn(new_image),\n        lag_ratio=1.0\n    )\n\n    # Append the animations to the list\n    synced_animations.append(\n        Succession(\n            transition_animation, \n            time_bar_animation,\n            replace_image_animation,\n            lag_ratio=0.0\n        )\n    )\n\n# Play all the animations in succession\nself.play(\n    Succession(*synced_animations),\n)\n```\n\nChanges:\n\n*   Added comments to explain what each section of code is doing.\n*   Used `enumerate` to iterate over both the timestep and transition animation in the loop.\n*   Moved the animation creation for each step inside the loop, so that it creates a new animation group for each step instead of reusing the same one.\n\nNote: There was also an issue with updating the `time_dot` value which I did not correct in this example. The line should be updated to use `time_bar_number_to_point(timestep)` instead of `time_bar.number_to_point(0)`.", "response": "\"\"\"\n    Shows video of diffusion process. \n\"\"\"\nimport manim_ml\nfrom manim import *\nfrom PIL import Image\nimport os\nfrom diffusers import StableDiffusionPipeline\nimport numpy as np\nimport scipy\n\nfrom manim_ml.diffusion.mcmc import metropolis_hastings_sampler, gaussian_proposal\nfrom manim_ml.diffusion.random_walk import RandomWalk\nfrom manim_ml.utils.mobjects.probability import make_dist_image_mobject_from_samples\n\ndef generate_stable_diffusion_images(prompt, num_inference_steps=30):\n    \"\"\"Generates a list of progressively denoised images using Stable Diffusion\n\n    Parameters\n    ----------\n    num_inference_steps : int, optional\n        _description_, by default 30\n    \"\"\"\n    pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n    image_list = [] \n    # Save image callback\n    def save_image_callback(step, timestep, latents):\n        print(\"Saving image callback\")\n        # Decode the latents\n        image = pipeline.vae.decode(latents)\n        image_list.append(image)\n\n    # Generate the image\n    pipeline(prompt, num_inference_steps=num_inference_steps, callback=save_image_callback)\n\n    return image_list\n\ndef make_time_schedule_bar(num_time_steps=30):\n    \"\"\"Makes a bar that gets moved back and forth according to the diffusion model time\"\"\"\n    # Draw time bar with initial value\n    time_bar = NumberLine(\n        [0, num_time_steps], length=25, stroke_width=10, include_ticks=False, include_numbers=False,\n        color=manim_ml.config.color_scheme.secondary_color\n\n    )\n    time_bar.shift(4.5 * DOWN)\n    current_time = ValueTracker(0.3)\n    time_point = time_bar.number_to_point(current_time.get_value())\n    time_dot = Dot(time_point, color=manim_ml.config.color_scheme.secondary_color, radius=0.2)\n    label_location = time_bar.number_to_point(1.0)\n    label_location -= DOWN * 0.1\n    label_text = MathTex(\"t\", color=manim_ml.config.color_scheme.text_color).scale(1.5)\n    # label_text = Text(\"time\")\n    label_text.move_to(time_bar.get_center())\n    label_text.shift(DOWN * 0.5)\n    # Make an updater for the dot\n    def dot_updater(time_dot):\n        # Get location on time_bar\n        point_loc = time_bar.number_to_point(current_time.get_value())\n        time_dot.move_to(point_loc)\n\n    time_dot.add_updater(dot_updater)\n\n    return time_bar, time_dot, label_text, current_time\n\ndef make_2d_diffusion_space():\n    \"\"\"Makes a 2D axis where the diffusion random walk happens in. \n    There is also a distribution of points representing the true distribution\n    of images. \n    \"\"\"\n    axes_group = Group()\n    x_range = [-8, 8]\n    y_range = [-8, 8]\n    y_length = 13\n    x_length = 13\n    # Make an axis\n    axes = Axes(\n        x_range=x_range,\n        y_range=y_range,\n        x_length=x_length,\n        y_length=y_length,\n        # x_axis_config={\"stroke_opacity\": 1.0, \"stroke_color\": WHITE},\n        # y_axis_config={\"stroke_opacity\": 1.0, \"stroke_color\": WHITE},\n        # tips=True,\n    )\n    # Make the distribution for the true images as some gaussian mixture in \n    # the bottom right\n    gaussian_a = np.random.multivariate_normal(\n        mean=[3.0, -3.2],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(100)\n    )\n    gaussian_b = np.random.multivariate_normal(\n        mean=[3.0, 3.0],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(100)\n    )\n    gaussian_c = np.random.multivariate_normal(\n        mean=[0.0, -1.6],\n        cov=[[1.0, 0.0], [0.0, 1.0]],\n        size=(200)\n    )\n    all_gaussian_samples = np.concatenate([gaussian_a, gaussian_b, gaussian_c], axis=0)\n    print(f\"Shape of all gaussian samples: {all_gaussian_samples.shape}\")\n\n    image_mobject = make_dist_image_mobject_from_samples(\n        all_gaussian_samples,\n        xlim=(x_range[0], x_range[1]),\n        ylim=(y_range[0], y_range[1])\n    )\n    image_mobject.scale_to_fit_height(\n        axes.get_height()     \n    )\n    image_mobject.move_to(axes)\n\n    axes_group.add(axes)\n    axes_group.add(image_mobject)\n\n    return axes_group\n\ndef generate_forward_and_reverse_chains(start_point, target_point, num_inference_steps=30):\n    \"\"\"Here basically we want to generate a forward and reverse chain\n    for the diffusion model where the reverse chain starts at the end of the forward\n    chain, and the end of the reverse chain ends somewhere within a certain radius of the \n    reverse chain.\n\n    This can be done in a sortof hacky way by doing metropolis hastings from a start point\n    to a distribution centered about the start point and vica versa.\n\n    Parameters\n    ----------\n    start_point : _type_\n        _description_\n    \"\"\"\n    def start_dist_log_prob_fn(x):\n        \"\"\"Log probability of the start distribution\"\"\"\n        # Make it a gaussian in top left of the 2D space\n        gaussian_pdf = scipy.stats.multivariate_normal(\n            mean=target_point,\n            cov=[1.0, 1.0]\n        ).pdf(x)\n\n        return np.log(gaussian_pdf)\n    \n    def end_dist_log_prob_fn(x):\n        gaussian_pdf = scipy.stats.multivariate_normal(\n            mean=start_point,\n            cov=[1.0, 1.0]\n        ).pdf(x)\n\n        return np.log(gaussian_pdf)\n\n    forward_chain, _, _ = metropolis_hastings_sampler(\n        log_prob_fn=start_dist_log_prob_fn, \n        prop_fn=gaussian_proposal,\n        iterations=num_inference_steps,\n        initial_location=start_point\n    )\n\n    end_point = forward_chain[-1]\n\n    reverse_chain, _, _ = metropolis_hastings_sampler(\n        log_prob_fn=end_dist_log_prob_fn,\n        prop_fn=gaussian_proposal,\n        iterations=num_inference_steps,\n        initial_location=end_point\n    )\n\n    return forward_chain, reverse_chain\n\n# Make the scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 30.0\nconfig.frame_width = 30.0\n\nclass DiffusionProcess(Scene):\n\n    def construct(self):\n        # Parameters\n        num_inference_steps = 50\n        image_save_dir = \"diffusion_images\"\n        prompt = \"An oil painting of a dragon.\"\n        start_time = 0\n        # Compute the stable diffusion images\n        if len(os.listdir(image_save_dir)) < num_inference_steps:\n            stable_diffusion_images = generate_stable_diffusion_images(\n                prompt,\n                num_inference_steps=num_inference_steps\n            )\n            for index, image in enumerate(stable_diffusion_images):\n                image.save(f\"{image_save_dir}/{index}.png\")\n        else:\n            stable_diffusion_images = [Image.open(f\"{image_save_dir}/{i}.png\") for i in range(num_inference_steps)]\n        # Reverse order of list to be in line with theory timesteps\n        stable_diffusion_images = stable_diffusion_images[::-1]\n        # Add the initial location of the first image\n        start_image = stable_diffusion_images[start_time]\n        image_mobject = ImageMobject(start_image)\n        image_mobject.scale(0.55)\n        image_mobject.shift(LEFT * 7.5)\n        self.add(image_mobject)\n        # Make the time schedule bar\n        time_bar, time_dot, time_label, current_time = make_time_schedule_bar(num_time_steps=num_inference_steps)\n        # Place the bar at the bottom of the screen\n        time_bar.move_to(image_mobject.get_bottom() + DOWN * 2)\n        time_bar.set_x(0)\n        self.add(time_bar)\n        # Place the time label below the time bar\n        self.add(time_label)\n        time_label.next_to(time_bar, DOWN, buff=0.5)\n        # Add 0 and T labels above the bar\n        zero_label = Text(\"0\")\n        zero_label.next_to(time_bar.get_left(), UP, buff=0.5)\n        self.add(zero_label)\n        t_label = Text(\"T\")\n        t_label.next_to(time_bar.get_right(), UP, buff=0.5)\n        self.add(t_label)\n        # Move the time dot to zero\n        time_dot.set_value(0)\n        time_dot.move_to(time_bar.number_to_point(0))\n        self.add(time_dot)\n        # Add the prompt above the image\n        paragraph_prompt = '\"An oil painting of\\n a flaming dragon.\"'\n        text_prompt = Paragraph(paragraph_prompt, alignment=\"center\", font_size=64)\n        text_prompt.next_to(image_mobject, UP, buff=0.6)\n        self.add(text_prompt)\n        # Generate the chain data\n        forward_chain, reverse_chain = generate_forward_and_reverse_chains(\n            start_point=[3.0, -3.0],\n            target_point=[-7.0, -4.0],\n            num_inference_steps=num_inference_steps\n        )\n        # Make the axes that the distribution and chains go on\n        axes, axes_group = make_2d_diffusion_space()\n        # axes_group.match_height(image_mobject)\n        axes_group.shift(RIGHT * 7.5)\n        axes.shift(RIGHT * 7.5)\n        self.add(axes_group)\n        # Add title below distribution\n        title = Text(\"Distribution of Real Images\", font_size=48, color=RED)\n        title.move_to(axes_group)\n        title.shift(DOWN * 6)\n        line = Line(\n            title.get_top(),\n            axes_group.get_bottom() - 4 * DOWN,\n        )\n        self.add(title)\n        self.add(line)\n        # Add a title above the plot\n        title = Text(\"Reverse Diffusion Process\", font_size=64)\n        title.move_to(axes)\n        title.match_y(text_prompt)\n        self.add(title)\n        # First do the forward noise process of adding noise to an image\n        forward_random_walk = RandomWalk(forward_chain, axes=axes)\n        # Sync up forward random walk\n        synced_animations = []\n        forward_random_walk_animation = forward_random_walk.animate_random_walk()\n        print(len(forward_random_walk_animation.animations))\n        for timestep, transition_animation in enumerate(forward_random_walk_animation.animations):\n            # Make an animation moving the time bar\n            # time_bar_animation = current_time.animate.set_value(timestep)\n            time_bar_animation = current_time.animate.set_value(timestep)\n            # Make an animation replacing the current image with the timestep image\n            new_image = ImageMobject(stable_diffusion_images[timestep])\n            new_image = new_image.set_height(image_mobject.get_height())\n            new_image.move_to(image_mobject)\n            image_mobject = new_image\n            replace_image_animation = AnimationGroup(\n                FadeOut(image_mobject),\n                FadeIn(new_image),\n                lag_ratio=1.0\n            )\n            # Sync them together\n            # synced_animations.append(\n            self.play(\n                Succession(\n                    transition_animation, \n                    time_bar_animation,\n                    replace_image_animation,\n                    lag_ratio=0.0\n                )\n            )\n\n        # Fade out the random walk\n        self.play(forward_random_walk.fade_out_random_walk())\n        # self.play(\n        # Succession(\n        # *synced_animations,\n        # )\n        # )\n        new_title = Text(\"Forward Diffusion Process\", font_size=64)\n        new_title.move_to(title)\n        self.play(ReplacementTransform(title, new_title))\n        # Second do the reverse noise process of removing noise from the image\n        backward_random_walk = RandomWalk(reverse_chain, axes=axes)\n        # Sync up forward random walk\n        synced_animations = []\n        backward_random_walk_animation = backward_random_walk.animate_random_walk()\n        for timestep, transition_animation in enumerate(backward_random_walk_animation.animations):\n            timestep = num_inference_steps - timestep - 1\n            if timestep == num_inference_steps - 1:\n                continue\n            # Make an animation moving the time bar\n            # time_bar_animation = time_dot.animate.set_value(timestep)\n            time_bar_animation = current_time.animate.set_value(timestep)\n            # Make an animation replacing the current image with the timestep image\n            new_image = ImageMobject(stable_diffusion_images[timestep])\n            new_image = new_image.set_height(image_mobject.get_height())\n            new_image.move_to(image_mobject)\n            image_mobject = new_image\n            replace_image_animation = AnimationGroup(\n                FadeOut(image_mobject),\n                FadeIn(new_image),\n                lag_ratio=1.0\n            )\n            # Sync them together\n            # synced_animations.append(\n            self.play(\n                Succession(\n                    transition_animation, \n                    time_bar_animation,\n                    replace_image_animation,\n                    lag_ratio=0.0\n                )\n            )\n        # self.play(\n        # Succession(\n        # *synced_animations,\n        # )\n        # )\n", "original_prompt": "Diffusion Process Diffusion Process"}
{"prompt": "Animate an embedded 2D Gaussian distribution while creating a fading animation of a disentangled VAE image sequence.", "response": "\"\"\"This module is dedicated to visualizing VAE disentanglement\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\n\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nimport pickle\n\nROOT_DIR = Path(__file__).parents[2]\n\n\ndef construct_image_mobject(input_image, height=2.3):\n    \"\"\"Constructs an ImageMobject from a numpy grayscale image\"\"\"\n    # Convert image to rgb\n    if len(input_image.shape) == 2:\n        input_image = np.repeat(input_image, 3, axis=0)\n        input_image = np.rollaxis(input_image, 0, start=3)\n    #  Make the ImageMobject\n    image_mobject = ImageMobject(input_image, image_mode=\"RGB\")\n    image_mobject.set_resampling_algorithm(RESAMPLING_ALGORITHMS[\"nearest\"])\n    image_mobject.height = height\n\n    return image_mobject\n\n\nclass DisentanglementVisualization(VGroup):\n    def __init__(\n        self,\n        model_path=ROOT_DIR\n        / \"examples/variational_autoencoder/autoencoder_models/saved_models/model_dim2.pth\",\n        image_height=0.35,\n    ):\n        self.model_path = model_path\n        self.image_height = image_height\n        # Load disentanglement image objects\n        with open(\n            ROOT_DIR\n            / \"examples/variational_autoencoder/autoencoder_models/disentanglement.pkl\",\n            \"rb\",\n        ) as f:\n            self.image_handler = pickle.load(f)\n\n    def make_disentanglement_generation_animation(self):\n        animation_list = []\n        for image_index, image in enumerate(self.image_handler[\"images\"]):\n            image_mobject = construct_image_mobject(image, height=self.image_height)\n            r, c = self.image_handler[\"bin_indices\"][image_index]\n            # Move the image to the correct location\n            r_offset = -1.2\n            c_offset = 0.25\n            image_location = [\n                c_offset + c * self.image_height,\n                r_offset + r * self.image_height,\n                0,\n            ]\n            image_mobject.move_to(image_location)\n            animation_list.append(FadeIn(image_mobject))\n\n        generation_animation = AnimationGroup(*animation_list[::-1], lag_ratio=1.0)\n        return generation_animation\n\n\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 5.0\nconfig.frame_width = 5.0\n\n\nclass DisentanglementScene(Scene):\n    \"\"\"Disentanglement Scene Object\"\"\"\n\n    def _construct_embedding(self, point_color=BLUE, dot_radius=0.05):\n        \"\"\"Makes a Gaussian-like embedding\"\"\"\n        embedding = VGroup()\n        # Sample points from a Gaussian\n        num_points = 200\n        standard_deviation = [0.6, 0.8]\n        mean = [0, 0]\n        points = np.random.normal(mean, standard_deviation, size=(num_points, 2))\n        # Make an axes\n        embedding.axes = Axes(\n            x_range=[-3, 3],\n            y_range=[-3, 3],\n            x_length=2.2,\n            y_length=2.2,\n            tips=False,\n        )\n        # Add each point to the axes\n        self.point_dots = VGroup()\n        for point in points:\n            point_location = embedding.axes.coords_to_point(*point)\n            dot = Dot(point_location, color=point_color, radius=dot_radius / 2)\n            self.point_dots.add(dot)\n\n        embedding.add(self.point_dots)\n        return embedding\n\n    def construct(self):\n        # Make the VAE decoder\n        vae_decoder = NeuralNetwork(\n            [\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n            ],\n            layer_spacing=0.55,\n        )\n\n        vae_decoder.shift([-0.55, 0, 0])\n        self.play(Create(vae_decoder), run_time=1)\n        # Make the embedding\n        embedding = self._construct_embedding()\n        embedding.scale(0.9)\n        embedding.move_to(vae_decoder.get_left())\n        embedding.shift([-0.85, 0, 0])\n        self.play(Create(embedding))\n        # Make disentanglment visualization\n        disentanglement = DisentanglementVisualization()\n        disentanglement_animation = (\n            disentanglement.make_disentanglement_generation_animation()\n        )\n        self.play(disentanglement_animation, run_time=3)\n        self.play(Wait(2))\n", "original_prompt": "Disentanglement Disentanglement"}
{"prompt": "Animate a set of dots that form a half-moon shape, displaying an epsilon ball animation with color changes and clustering, while showing the underlying axes and coordinate system transformations.", "response": "\"\"\"\n    Example where I draw an epsilon nearest neighbor graph animation\n\"\"\"\nfrom cProfile import label\nfrom manim import *\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import SpectralClustering\nimport numpy as np\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1200\nconfig.frame_height = 12.0\nconfig.frame_width = 12.0\n\n\ndef make_moon_points(num_samples=100, noise=0.1, random_seed=1):\n    \"\"\"Make two half moon point shapes\"\"\"\n    # Make sure the points are normalized\n    X, y = make_moons(n_samples=num_samples, noise=noise, random_state=random_seed)\n    X -= np.mean(X, axis=0)\n    X /= np.std(X, axis=0)\n    X[:, 1] += 0.3\n    # X[:, 0] /= 2 # squeeze width\n\n    return X\n\n\ndef make_epsilon_balls(epsilon_value, points, axes, ball_color=RED, opacity=0.0):\n    \"\"\"Draws epsilon balls\"\"\"\n    balls = []\n    for point in points:\n        ball = Circle(epsilon_value, color=ball_color, fill_opacity=opacity)\n        global_location = axes.coords_to_point(*point)\n        ball.move_to(global_location)\n        balls.append(ball)\n\n    return VGroup(*balls)\n\n\ndef make_epsilon_graph(epsilon_value, dots, points, edge_color=ORANGE):\n    \"\"\"Makes an epsilon nearest neighbor graph for the given dots\"\"\"\n    # First compute the adjacency matrix from the epsilon value and the points\n    num_dots = len(dots)\n    adjacency_matrix = np.zeros((num_dots, num_dots))\n    # Note: just doing lower triangular matrix\n    for i in range(num_dots):\n        for j in range(i):\n            dist = np.linalg.norm(dots[i].get_center() - dots[j].get_center())\n            is_connected = 1 if dist < epsilon_value else 0\n            adjacency_matrix[i, j] = is_connected\n    # Draw a graph based on the adjacency matrix\n    edges = []\n    for i in range(num_dots):\n        for j in range(i):\n            is_connected = adjacency_matrix[i, j]\n            if is_connected:\n                # Draw a connection between the corresponding dots\n                dot_a = dots[i]\n                dot_b = dots[j]\n                edge = Line(\n                    dot_a.get_center(),\n                    dot_b.get_center(),\n                    color=edge_color,\n                    stroke_width=3,\n                )\n                edges.append(edge)\n\n    return VGroup(*edges), adjacency_matrix\n\n\ndef perform_spectral_clustering(adjacency_matrix):\n    \"\"\"Performs spectral clustering given adjacency matrix\"\"\"\n    clustering = SpectralClustering(\n        n_clusters=2, affinity=\"precomputed\", random_state=0\n    ).fit(adjacency_matrix)\n    labels = clustering.labels_\n\n    return labels\n\n\ndef make_color_change_animation(labels, dots, colors=[ORANGE, GREEN]):\n    \"\"\"Makes a color change animation\"\"\"\n    anims = []\n\n    for index in range(len(labels)):\n        color = colors[labels[index]]\n        dot = dots[index]\n        anims.append(dot.animate.set_color(color))\n\n    return AnimationGroup(*anims, lag_ratio=0.0)\n\n\nclass EpsilonNearestNeighborScene(Scene):\n    def construct(\n        self,\n        num_points=200,\n        dot_radius=0.1,\n        dot_color=BLUE,\n        ball_color=WHITE,\n        noise=0.1,\n        ball_opacity=0.0,\n        random_seed=2,\n    ):\n        # Make moon shape points\n        # Note: dot is the drawing object and point is the math concept\n        moon_points = make_moon_points(\n            num_samples=num_points, noise=noise, random_seed=random_seed\n        )\n        # Make an axes\n        axes = Axes(\n            x_range=[-6, 6, 1],\n            y_range=[-6, 6, 1],\n            x_length=12,\n            y_length=12,\n            tips=False,\n            axis_config={\"stroke_color\": \"#000000\"},\n        )\n        axes.scale(2.2)\n        self.add(axes)\n        # Draw points\n        dots = []\n        for point in moon_points:\n            axes_location = axes.coords_to_point(*point)\n            dot = Dot(axes_location, color=dot_color, radius=dot_radius, z_index=1)\n            dots.append(dot)\n\n        dots = VGroup(*dots)\n        self.play(Create(dots))\n        # Draw epsilon bar with initial value\n        epsilon_bar = NumberLine(\n            [0, 2], length=8, stroke_width=2, include_ticks=False, include_numbers=False\n        )\n        epsilon_bar.shift(4.5 * DOWN)\n        self.play(Create(epsilon_bar))\n        current_epsilon = ValueTracker(0.3)\n        epsilon_point = epsilon_bar.number_to_point(current_epsilon.get_value())\n        epsilon_dot = Dot(epsilon_point)\n        self.add(epsilon_dot)\n        label_location = epsilon_bar.number_to_point(1.0)\n        label_location -= DOWN * 0.1\n        label_text = MathTex(\"\\epsilon\").scale(1.5)\n        # label_text = Text(\"Epsilon\")\n        label_text.move_to(epsilon_bar.get_center())\n        label_text.shift(DOWN * 0.5)\n        self.add(label_text)\n        # Make an updater for the dot\n        def dot_updater(epsilon_dot):\n            # Get location on epsilon_bar\n            point_loc = epsilon_bar.number_to_point(current_epsilon.get_value())\n            epsilon_dot.move_to(point_loc)\n\n        epsilon_dot.add_updater(dot_updater)\n        # Make the epsilon balls\n        epsilon_balls = make_epsilon_balls(\n            current_epsilon.get_value(),\n            moon_points,\n            axes,\n            ball_color=ball_color,\n            opacity=ball_opacity,\n        )\n        # Set up updater for radius of balls\n        def epsilon_balls_updater(epsilon_balls):\n            for ball in epsilon_balls:\n                ball.set_width(current_epsilon.get_value())\n\n        # Turn epsilon up and down\n        epsilon_balls.add_updater(epsilon_balls_updater)\n        # Fade in the initial balls\n        self.play(FadeIn(epsilon_balls), lag_ratio=0.0)\n        # Iterate through different values of epsilon\n        for value in [1.5, 0.5, 0.9]:\n            self.play(current_epsilon.animate.set_value(value), run_time=2.5)\n        # Perform clustering\n        epsilon_value = 0.9\n        # Show connecting graph\n        epsilon_graph, adjacency_matrix = make_epsilon_graph(\n            current_epsilon.get_value(), dots, moon_points, edge_color=WHITE\n        )\n        self.play(FadeOut(epsilon_balls))\n        self.play(FadeIn(epsilon_graph))\n        # Fade out balls\n        self.play(Wait(1.5))\n        # Perform clustering\n        labels = perform_spectral_clustering(adjacency_matrix)\n        # Change the colors of the dots\n        color_change_animation = make_color_change_animation(labels, dots)\n        self.play(color_change_animation)\n        # Fade out graph edges\n        self.play(FadeOut(epsilon_graph))\n        self.play(Wait(5.0))\n", "original_prompt": "Epsilon Nn Graph Epsilon Nn Graph"}
{"prompt": "Animate a rotating neural network diagram that highlights generator and discriminator rectangles with fading, showing real and fake images.", "response": "import random\nfrom pathlib import Path\n\nfrom PIL import Image\nfrom manim import *\nfrom manim_ml.neural_network.layers.embedding import EmbeddingLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.vector import VectorLayer\n\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\nconfig.pixel_height = 1080\nconfig.pixel_width = 1080\nconfig.frame_height = 8.3\nconfig.frame_width = 8.3\n\n\nclass GAN(Mobject):\n    \"\"\"Generative Adversarial Network\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.make_entities()\n        self.place_entities()\n        self.titles = self.make_titles()\n\n    def make_entities(self, image_height=1.2):\n        \"\"\"Makes all of the network entities\"\"\"\n        # Make the fake image layer\n        default_image = Image.open(ROOT_DIR / \"assets/gan/fake_image.png\")\n        numpy_image = np.asarray(default_image)\n        self.fake_image_layer = ImageLayer(\n            numpy_image, height=image_height, show_image_on_create=False\n        )\n        # Make the Generator Network\n        self.generator = NeuralNetwork(\n            [\n                EmbeddingLayer(covariance=np.array([[3.0, 0], [0, 3.0]])).scale(1.3),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                self.fake_image_layer,\n            ],\n            layer_spacing=0.1,\n        )\n\n        self.add(self.generator)\n        # Make the Discriminator\n        self.discriminator = NeuralNetwork(\n            [\n                FeedForwardLayer(5),\n                FeedForwardLayer(1),\n                VectorLayer(1, value_func=lambda: random.uniform(0, 1)),\n            ],\n            layer_spacing=0.1,\n        )\n        self.add(self.discriminator)\n        # Make Ground Truth Dataset\n        default_image = Image.open(ROOT_DIR / \"assets/gan/real_image.jpg\")\n        numpy_image = np.asarray(default_image)\n        self.ground_truth_layer = ImageLayer(numpy_image, height=image_height)\n        self.add(self.ground_truth_layer)\n\n        self.scale(1)\n\n    def place_entities(self):\n        \"\"\"Positions entities in correct places\"\"\"\n        # Place relative to generator\n        # Place the ground_truth image layer\n        self.ground_truth_layer.next_to(self.fake_image_layer, DOWN, 0.8)\n        # Group the images\n        image_group = Group(self.ground_truth_layer, self.fake_image_layer)\n        # Move the discriminator to the right of thee generator\n        self.discriminator.next_to(self.generator, RIGHT, 0.2)\n        self.discriminator.match_y(image_group)\n        # Move the discriminator to the height of the center of the image_group\n        # self.discriminator.match_y(image_group)\n        # self.ground_truth_layer.next_to(self.fake_image_layer, DOWN, 0.5)\n\n    def make_titles(self):\n        \"\"\"Makes titles for the different entities\"\"\"\n        titles = VGroup()\n\n        self.ground_truth_layer_title = Text(\"Real Image\").scale(0.3)\n        self.ground_truth_layer_title.next_to(self.ground_truth_layer, UP, 0.1)\n        self.add(self.ground_truth_layer_title)\n        titles.add(self.ground_truth_layer_title)\n        self.fake_image_layer_title = Text(\"Fake Image\").scale(0.3)\n        self.fake_image_layer_title.next_to(self.fake_image_layer, UP, 0.1)\n        self.add(self.fake_image_layer_title)\n        titles.add(self.fake_image_layer_title)\n        # Overhead title\n        overhead_title = Text(\"Generative Adversarial Network\").scale(0.75)\n        overhead_title.shift(np.array([0, 3.5, 0]))\n        titles.add(overhead_title)\n        # Probability title\n        self.probability_title = Text(\"Probability\").scale(0.5)\n        self.probability_title.move_to(self.discriminator.input_layers[-2])\n        self.probability_title.shift(UP)\n        self.probability_title.shift(RIGHT * 1.05)\n        titles.add(self.probability_title)\n\n        return titles\n\n    def make_highlight_generator_rectangle(self):\n        \"\"\"Returns animation that highlights the generators contents\"\"\"\n        group = VGroup()\n\n        generator_surrounding_group = Group(self.generator, self.fake_image_layer_title)\n\n        generator_surrounding_rectangle = SurroundingRectangle(\n            generator_surrounding_group, buff=0.1, stroke_width=4.0, color=\"#0FFF50\"\n        )\n        group.add(generator_surrounding_rectangle)\n        title = Text(\"Generator\").scale(0.5)\n        title.next_to(generator_surrounding_rectangle, UP, 0.2)\n        group.add(title)\n\n        return group\n\n    def make_highlight_discriminator_rectangle(self):\n        \"\"\"Makes a rectangle for highlighting the discriminator\"\"\"\n        discriminator_group = Group(\n            self.discriminator,\n            self.fake_image_layer,\n            self.ground_truth_layer,\n            self.fake_image_layer_title,\n            self.probability_title,\n        )\n\n        group = VGroup()\n\n        discriminator_surrounding_rectangle = SurroundingRectangle(\n            discriminator_group, buff=0.05, stroke_width=4.0, color=\"#0FFF50\"\n        )\n        group.add(discriminator_surrounding_rectangle)\n        title = Text(\"Discriminator\").scale(0.5)\n        title.next_to(discriminator_surrounding_rectangle, UP, 0.2)\n        group.add(title)\n\n        return group\n\n    def make_generator_forward_pass(self):\n        \"\"\"Makes forward pass of the generator\"\"\"\n\n        forward_pass = self.generator.make_forward_pass_animation(dist_theme=\"ellipse\")\n\n        return forward_pass\n\n    def make_discriminator_forward_pass(self):\n        \"\"\"Makes forward pass of the discriminator\"\"\"\n\n        disc_forward = self.discriminator.make_forward_pass_animation()\n\n        return disc_forward\n\n    @override_animation(Create)\n    def _create_override(self):\n        \"\"\"Overrides create\"\"\"\n        animation_group = AnimationGroup(\n            Create(self.generator),\n            Create(self.discriminator),\n            Create(self.ground_truth_layer),\n            Create(self.titles),\n        )\n        return animation_group\n\n\nclass GANScene(Scene):\n    \"\"\"GAN Scene\"\"\"\n\n    def construct(self):\n        gan = GAN().scale(1.70)\n        gan.move_to(ORIGIN)\n        gan.shift(DOWN * 0.35)\n        gan.shift(LEFT * 0.1)\n        self.play(Create(gan), run_time=3)\n        # Highlight generator\n        highlight_generator_rectangle = gan.make_highlight_generator_rectangle()\n        self.play(Create(highlight_generator_rectangle), run_time=1)\n        # Generator forward pass\n        gen_forward_pass = gan.make_generator_forward_pass()\n        self.play(gen_forward_pass, run_time=5)\n        # Fade out generator highlight\n        self.play(Uncreate(highlight_generator_rectangle), run_time=1)\n        # Highlight discriminator\n        highlight_discriminator_rectangle = gan.make_highlight_discriminator_rectangle()\n        self.play(Create(highlight_discriminator_rectangle), run_time=1)\n        # Discriminator forward pass\n        discriminator_forward_pass = gan.make_discriminator_forward_pass()\n        self.play(discriminator_forward_pass, run_time=5)\n        # Unhighlight discriminator\n        self.play(Uncreate(highlight_discriminator_rectangle), run_time=1)\n", "original_prompt": "Gan Gan"}
{"prompt": "Animate a Variational Autoencoder's decoder making multiple forward passes with zooming embeddings.", "response": "\"\"\"Visualization of VAE Interpolation\"\"\"\nimport sys\nimport os\n\nsys.path.append(os.environ[\"PROJECT_ROOT\"])\nfrom manim import *\nimport pickle\nimport numpy as np\nimport manim_ml.neural_network as neural_network\nimport examples.variational_autoencoder.variational_autoencoder as variational_autoencoder\n\n\"\"\"\n    The VAE Scene for the twitter video. \n\"\"\"\nconfig.pixel_height = 720\nconfig.pixel_width = 1280\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n# Set random seed so point distribution is constant\nnp.random.seed(1)\n\n\nclass InterpolationScene(MovingCameraScene):\n    \"\"\"Scene object for a Variational Autoencoder and Autoencoder\"\"\"\n\n    def construct(self):\n        # Set Scene config\n        vae = variational_autoencoder.VariationalAutoencoder(\n            dot_radius=0.035, layer_spacing=0.5\n        )\n        vae.move_to(ORIGIN)\n        vae.encoder.shift(LEFT * 0.5)\n        vae.decoder.shift(RIGHT * 0.5)\n        mnist_image_handler = variational_autoencoder.MNISTImageHandler()\n        image_pair = mnist_image_handler.image_pairs[3]\n        # Make forward pass animation and DO NOT run it\n        forward_pass_animation = vae.make_forward_pass_animation(image_pair)\n        # Make the interpolation animation\n        interpolation_images = mnist_image_handler.interpolation_images\n        interpolation_animation = vae.make_interpolation_animation(interpolation_images)\n        embedding_zoom_animation = self.camera.auto_zoom(\n            [vae.embedding, vae.decoder, vae.output_image], margin=0.5\n        )\n        # Make animations\n        forward_pass_animations = []\n        for i in range(7):\n            anim = vae.decoder.make_forward_propagation_animation(run_time=0.5)\n            forward_pass_animations.append(anim)\n        forward_pass_animation_group = AnimationGroup(\n            *forward_pass_animations, lag_ratio=1.0\n        )\n        # Make forward pass animations\n        self.play(Create(vae), run_time=1.5)\n        self.play(FadeOut(vae.encoder), run_time=1.0)\n        self.play(embedding_zoom_animation, run_time=1.5)\n        interpolation_animation = AnimationGroup(\n            forward_pass_animation_group, interpolation_animation\n        )\n        self.play(interpolation_animation, run_time=9.0)\n", "original_prompt": "Interpolation Interpolation"}
{"prompt": "Animate a neural network diagram unfolding vertically from the center, with images and layers transforming into their respective components.", "response": "from pathlib import Path\n\nfrom manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 20.0\nconfig.frame_width = 20.0\n\nROOT_DIR = Path(__file__).parents[2]\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=4.5),\n                Convolutional2DLayer(1, 28),\n                Convolutional2DLayer(6, 28, 5),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(16, 10, 5),\n                MaxPooling2DLayer(kernel_size=2),\n                FeedForwardLayer(8),\n                FeedForwardLayer(3),\n                FeedForwardLayer(2),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Make code snippet\n        # code = make_code_snippet()\n        # code.next_to(nn, DOWN)\n        # self.add(code)\n        # Group it all\n        # group = Group(nn, code)\n        # group.move_to(ORIGIN)\n        nn.move_to(ORIGIN)\n        # Play animation\n        # forward_pass = nn.make_forward_pass_animation()\n        # self.wait(1)\n        # self.play(forward_pass)\n", "original_prompt": "Lenet Lenet"}
{"prompt": "Animate a 3D neural network diagram appearing under the text \"ManimML\", rotating and moving into view with a dark underline.", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\n\nimport manim_ml\nmanim_ml.config.color_scheme = \"light_mode\"\n\nfrom manim_ml.neural_network.architectures.feed_forward import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 1000\nconfig.pixel_width = 1000\nconfig.frame_height = 4.0\nconfig.frame_width = 4.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.text = Text(\"ManimML\", color=manim_ml.config.color_scheme.text_color)\n        self.text.scale(1.0)\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 6, 3], layer_spacing=0.3, node_color=BLUE\n        )\n        self.neural_network.scale(1.0)\n        self.neural_network.move_to(self.text.get_bottom())\n        self.neural_network.shift(1.25 * DOWN)\n        self.logo_group = Group(self.text, self.neural_network)\n        self.logo_group.scale(1.0)\n        self.logo_group.move_to(ORIGIN)\n        self.play(Write(self.text), run_time=1.0)\n        self.play(Create(self.neural_network), run_time=3.0)\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            # Create(self.surrounding_rectangle)\n        )\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            #    Create(self.surrounding_rectangle)\n        )\n        self.play(animation_group, runtime=5.0)\n        self.wait(5)\n", "original_prompt": "Logo Logo"}
{"prompt": "\"Animate a large blue neural network diagram that emerges and runs for 5 seconds.\"", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\nfrom manim_ml.neural_network.neural_network import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 400\nconfig.pixel_width = 600\nconfig.frame_height = 8.0\nconfig.frame_width = 10.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 5], layer_spacing=0.6, node_color=BLUE, edge_width=6\n        )\n        self.neural_network.scale(3)\n        self.neural_network.move_to(ORIGIN)\n        self.play(Create(self.neural_network))\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n        )\n        self.play(animation_group)\n        self.wait(5)\n", "original_prompt": "Logo Website Logo"}
{"prompt": "Animate a logo of a neural network with equations appearing under it as it scales and transforms into a visualization of a feed-forward neural network.", "response": "\"\"\"\n    Logo for Manim Machine Learning\n\"\"\"\nfrom manim import *\n\nimport manim_ml\nmanim_ml.config.color_scheme = \"light_mode\"\n\nfrom manim_ml.neural_network.architectures.feed_forward import FeedForwardNeuralNetwork\n\nconfig.pixel_height = 1000\nconfig.pixel_width = 2000\nconfig.frame_height = 4.0\nconfig.frame_width = 8.0\n\n\nclass ManimMLLogo(Scene):\n    def construct(self):\n        self.text = Text(\"ManimML\", color=manim_ml.config.color_scheme.text_color)\n        self.text.scale(1.0)\n        self.neural_network = FeedForwardNeuralNetwork(\n            [3, 5, 3, 6, 3], layer_spacing=0.3, node_color=BLUE\n        )\n        self.neural_network.scale(0.8)\n        self.neural_network.next_to(self.text, RIGHT, buff=0.5)\n        # self.neural_network.move_to(self.text.get_right())\n        # self.neural_network.shift(1.25 * DOWN)\n        self.logo_group = Group(self.text, self.neural_network)\n        self.logo_group.scale(1.0)\n        self.logo_group.move_to(ORIGIN)\n        self.play(Write(self.text), run_time=1.0)\n        self.play(Create(self.neural_network), run_time=3.0)\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            # Create(self.surrounding_rectangle)\n        )\n        # self.surrounding_rectangle = SurroundingRectangle(self.logo_group, buff=0.3, color=BLUE)\n        underline = Underline(self.text, color=BLACK)\n        animation_group = AnimationGroup(\n            self.neural_network.make_forward_pass_animation(run_time=5),\n            Create(underline),\n            #    Create(self.surrounding_rectangle)\n        )\n        self.play(animation_group, runtime=5.0)\n        self.wait(5)\n", "original_prompt": "Logo Wide Logo"}
{"prompt": "Animate a Gaussian Mixture Model's log PDF being calculated with sample generation and visualization of Metropolis Hastings chain sampling.", "response": "from manim import *\n\nimport scipy.stats\nfrom manim_ml.diffusion.mcmc import MCMCAxes\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use('dark_background')\n\n# Make the specific scene\nconfig.pixel_height = 720\nconfig.pixel_width = 720\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\nclass MCMCWarmupScene(Scene):\n\n    def construct(self):\n        # Define the Gaussian Mixture likelihood\n        def gaussian_mm_logpdf(x):\n            \"\"\"Gaussian Mixture Model Log PDF\"\"\"\n            # Choose two arbitrary Gaussians\n            # Big Gaussian\n            big_gaussian_pdf = scipy.stats.multivariate_normal(\n                mean=[-0.5, -0.5],\n                cov=[1.0, 1.0]\n            ).pdf(x)\n            # Little Gaussian\n            little_gaussian_pdf = scipy.stats.multivariate_normal(\n                mean=[2.3, 1.9],\n                cov=[0.3, 0.3]\n            ).pdf(x)\n            # Sum their likelihoods and take the log\n            logpdf = np.log(big_gaussian_pdf + little_gaussian_pdf)\n\n            return logpdf\n\n        # Generate a bunch of true samples\n        true_samples = []\n        # Generate samples for little gaussian\n        little_gaussian_samples = np.random.multivariate_normal(\n            mean=[2.3, 1.9],\n            cov=[[0.3, 0.0], [0.0, 0.3]],\n            size=(10000)\n        )\n        big_gaussian_samples = np.random.multivariate_normal(\n            mean=[-0.5, -0.5],\n            cov=[[1.0, 0.0], [0.0, 1.0]],\n            size=(10000)\n        )\n        true_samples = np.concatenate((little_gaussian_samples, big_gaussian_samples))\n        # Make the MCMC axes\n        axes = MCMCAxes(\n            x_range=[-5, 5],\n            y_range=[-5, 5],\n            x_length=7.0,\n            y_length=7.0\n        )\n        axes.move_to(ORIGIN)\n        self.play(\n            Create(axes)\n        )\n        # Make the chain sampling animation \n        chain_sampling_animation = axes.visualize_metropolis_hastings_chain_sampling(\n            log_prob_fn=gaussian_mm_logpdf, \n            true_samples=true_samples,\n            sampling_kwargs={\n                \"iterations\": 2000,\n                \"warm_up\": 50,\n                \"initial_location\": np.array([-3.5, 3.5]),\n                \"sampling_seed\": 4\n            },\n        )\n        self.play(chain_sampling_animation)\n        self.wait(3)\n", "original_prompt": "Mcmc Warmup Mcmc"}
{"prompt": "It seems like you have a Python code snippet related to visualizing a Neocognitron model using the Manim library. The code defines classes for the Neocognitron model and its visualization, as well as a scene that displays the model.\n\nTo improve this code, here are some suggestions:\n\n1. **Use meaningful variable names**: Some of the variable names are not very descriptive, making it harder to understand what they represent. For example, `i` could be renamed to `layer_index`.\n\n2. **Consider adding comments**: Comments can help explain complex parts of the code and make it easier for others (or yourself in the future) to understand.\n\n3. **Use consistent naming conventions**: The code uses both camelCase and underscore notation for variable names. It's better to stick with one convention throughout the code.\n\n4. **Error handling**: The code assumes that all inputs are valid, but in a real-world scenario, you should always expect potential errors. Consider adding try-except blocks to handle any unexpected issues.\n\n5. **Consider using more descriptive docstrings**: Docstrings can provide information about what each function or class does and how to use it.\n\n6. **Use Manim's built-in features**: Manim provides a lot of built-in features that can simplify your code, such as `Manim`, `Scene`, `VGroup`, etc.\n\n7. **Consider adding animations**: Animations can make the visualization more engaging. You could add animations to show how the Neocognitron model processes input or performs calculations.\n\nHere is an updated version of your code with some of these suggestions applied:\n\n```python\nclass NeocognitronLayer:\n    \"\"\"A layer in the Neocognitron model.\"\"\"\n    \n    def __init__(self, cells_per_layer, cell_heights, receptive_field_sizes, layer_name):\n        # Initialize the layer properties\n        self.cells_per_layer = cells_per_layer\n        self.cell_heights = cell_heights\n        self.receptive_field_sizes = receptive_field_sizes\n        self.layer_name = layer_name\n\nclass Neocognitron:\n    \"\"\"A Neocognitron model.\"\"\"\n    \n    def __init__(self, layers):\n        # Initialize the layers\n        self.layers = layers\n    \n    def make_forward_pass_animation(self):\n        # Create an animation of the forward pass\n        for i, layer in enumerate(self.layers[:-1]):\n            input_layer = self.layers[i]\n            output_layer = self.layers[i + 1]\n            \n            # Rotate the pair of layers\n            group = VGroup(input_layer.plane_label, output_layer.plane_label)\n            group.move_to(np.array([group.get_center()[0], 0, 0]))\n            group.rotate(40 * DEGREES, axis=UP, about_point=group.get_center())\n            \n            # Shift the pair of layers\n            c_layer = Group(input_layer, output_layer)\n            c_layer.shift(layer_index // 2 * np.array([0, 0, 0.3]))\n\n        return VGroup(*self.layers[-1].plane_label)\n\nclass Scene(ThreeDScene):\n    \"\"\"A scene for visualizing the Neocognitron model.\"\"\"\n    \n    def construct(self):\n        # Create a Neocognitron model\n        neocognitron = Neocognitron([NeocognitronLayer(cells_per_layer, cell_heights, receptive_field_sizes, layer_name) \n                                     for layer_index, (cells_per_layer, cell_heights, receptive_field_sizes, layer_name) \n                                     in enumerate(zip(neocognitron.cells_per_layer, neocognitron.cell_heights, neocognitron.receptive_field_sizes, neocognitron.layer_names))])\n        \n        # Shift the Neocognitron model\n        neocognitron.shift(DOWN*0.5)\n        \n        # Create a title for the scene\n        title = Text(\"Neocognitron\").scale(1)\n        \n        # Add the title to the scene\n        self.add_fixed_in_frame_mobjects(title)\n        title.next_to(neocognitron, UP, buff=0.4)\n        \n        # Play an animation of the forward pass\n        self.play(neocognitron.make_forward_pass_animation())\n```\n\nThis updated code includes more descriptive variable names and docstrings for clarity. Additionally, it uses some Manim's built-in features to simplify the code. The animations are also simplified by using VGroup and shift methods instead of manual transformations.", "response": "from manim import *\n\nfrom manim_ml.neural_network import NeuralNetwork\nfrom manim_ml.neural_network.layers.parent_layers import NeuralNetworkLayer, ConnectiveLayer, ThreeDLayer\nimport manim_ml\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 10.5\nconfig.frame_width = 10.5\n\nclass NeocognitronFilter(VGroup):\n    \"\"\"\n        Filter connecting the S and C Cells of a neocognitron layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_cell, \n        output_cell, \n        receptive_field_radius, \n        outline_color=BLUE,\n        active_color=ORANGE, \n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.outline_color = outline_color\n        self.active_color = active_color\n        self.input_cell = input_cell\n        self.output_cell = output_cell\n        # Draw the receptive field\n        # Make the points of a equilateral triangle in the plane of the \n        # cell about a random center\n        # center_point = input_cell.get_center()\n        shift_amount_x = np.random.uniform(\n            -(input_cell.get_height()/2 - receptive_field_radius - 0.01),\n            input_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        shift_amount_y = np.random.uniform(\n            -(input_cell.get_height()/2 - receptive_field_radius - 0.01),\n            input_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        # center_point += np.array([shift_amount_x, shift_amount_y, 0])\n        # # Make the triangle points\n        # side_length = np.sqrt(3) * receptive_field_radius\n        # normal_vector = np.cross(\n        #     input_cell.get_left() - input_cell.get_center(),\n        #     input_cell.get_top() - input_cell.get_center(),\n        # )\n        # Get vector in the plane of the cell\n        # vector_in_plane = input_cell.get_left() - input_cell.get_center()\n        # point_a = center_point + vector_in_plane * receptive_field_radius\n        # # rotate the vector 120 degrees about the vector normal to the cell\n        # vector_in_plane = rotate_vector(vector_in_plane, PI/3, normal_vector)\n        # point_b = center_point + vector_in_plane * receptive_field_radius\n        # # rotate the vector 120 degrees about the vector normal to the cell\n        # vector_in_plane = rotate_vector(vector_in_plane, PI/3, normal_vector)\n        # point_c = center_point + vector_in_plane * receptive_field_radius\n        # self.receptive_field = Circle.from_three_points(\n        #     point_a, \n        #     point_b,\n        #     point_c,\n        #     color=outline_color,\n        #     stroke_width=2.0,\n        # )\n        self.receptive_field = Circle(\n            radius=receptive_field_radius,\n            color=outline_color,\n            stroke_width=3.0,\n        )\n        self.add(self.receptive_field)\n        # Move receptive field to a random point within the cell\n        # minus the radius of the receptive field\n        self.receptive_field.move_to(\n            input_cell\n        )\n        # Shift a random amount in the x and y direction within \n        self.receptive_field.shift(\n            np.array([shift_amount_x, shift_amount_y, 0])\n        )\n        # Choose a random point on the c_cell\n        shift_amount_x = np.random.uniform(\n            -(output_cell.get_height()/2 - receptive_field_radius - 0.01),\n            output_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        shift_amount_y = np.random.uniform(\n            -(output_cell.get_height()/2 - receptive_field_radius - 0.01),\n            output_cell.get_height()/2 - receptive_field_radius - 0.01,\n        )\n        self.dot = Dot(\n            color=outline_color,\n            radius=0.04\n        )\n        self.dot.move_to(output_cell.get_center() + np.array([shift_amount_x, shift_amount_y, 0]))\n        self.add(self.dot)\n        # Make lines connecting receptive field to the dot\n        self.lines = VGroup()\n        self.lines.add(\n            Line(\n                self.receptive_field.get_center() + np.array([0, receptive_field_radius, 0]),\n                self.dot,\n                color=outline_color,\n                stroke_width=3.0,\n            )\n        )\n        self.lines.add(\n            Line(\n                self.receptive_field.get_center() - np.array([0, receptive_field_radius, 0]),\n                self.dot,\n                color=outline_color,\n                stroke_width=3.0,\n            )\n        )\n        self.add(self.lines)\n\n    def make_filter_pulse_animation(self, **kwargs):\n        succession = Succession(\n            ApplyMethod(\n                self.receptive_field.set_color, \n                self.active_color, \n                run_time=0.25\n            ),\n            ApplyMethod(\n                self.receptive_field.set_color, \n                self.outline_color, \n                run_time=0.25\n            ),\n            ShowPassingFlash(\n                self.lines.copy().set_color(self.active_color),\n                time_width=0.5,\n            ),\n            ApplyMethod(\n                self.dot.set_color, \n                self.active_color, \n                run_time=0.25\n            ),\n            ApplyMethod(\n                self.dot.set_color, \n                self.outline_color, \n                run_time=0.25\n            ),\n        )\n\n        return succession\n\nclass NeocognitronLayer(NeuralNetworkLayer, ThreeDLayer):\n    \n    def __init__(\n        self, \n        num_cells, \n        cell_height,\n        receptive_field_radius,\n        layer_name,\n        active_color=manim_ml.config.color_scheme.active_color,\n        cell_color=manim_ml.config.color_scheme.secondary_color,\n        outline_color=manim_ml.config.color_scheme.primary_color,\n        show_outline=True,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.num_cells = num_cells\n        self.cell_height = cell_height\n        self.receptive_field_radius = receptive_field_radius\n        self.layer_name = layer_name\n        self.active_color = active_color\n        self.cell_color = cell_color\n        self.outline_color = outline_color\n        self.show_outline = show_outline\n\n        self.plane_label = Text(layer_name).scale(0.4)\n\n    def make_cell_plane(self, layer_name, cell_buffer=0.1, stroke_width=2.0):\n        \"\"\"Makes a plane of cells. \n        \"\"\"\n        cell_plane = VGroup()\n        for cell_index in range(self.num_cells):\n            # Draw the cell box\n            cell_box = Rectangle(\n                width=self.cell_height,\n                height=self.cell_height,\n                color=self.cell_color,\n                fill_color=self.cell_color,\n                fill_opacity=0.3,\n                stroke_width=stroke_width,\n            )\n            if cell_index > 0:\n                cell_box.next_to(cell_plane[-1], DOWN, buff=cell_buffer)\n            cell_plane.add(cell_box)\n        # Draw the outline\n        if self.show_outline:\n            self.plane_outline = SurroundingRectangle(\n                cell_plane,\n                color=self.cell_color,\n                buff=cell_buffer,\n                stroke_width=stroke_width,\n            )\n            cell_plane.add(\n                self.plane_outline\n            )\n        # Draw a label above the container box\n        self.plane_label.next_to(cell_plane, UP, buff=0.2)\n        cell_plane.add(self.plane_label)\n\n        return cell_plane\n\n    def construct_layer(self, input_layer, output_layer, **kwargs):\n        # Make the Cell layer\n        self.cell_plane = self.make_cell_plane(self.layer_name)\n        self.add(self.cell_plane)\n        \n    def make_forward_pass_animation(self, layer_args={}, **kwargs):\n        \"\"\"Forward pass for query\"\"\"\n        # # Pulse and un pulse the cell plane rectangle\n        flash_outline_animations = []\n        for cell in self.cell_plane:\n            flash_outline_animations.append(\n                Succession(\n                    ApplyMethod(\n                        cell.set_stroke_color,\n                        self.active_color,\n                        run_time=0.25\n                    ),\n                    ApplyMethod(\n                        cell.set_stroke_color,\n                        self.outline_color,\n                        run_time=0.25\n                    )\n                )\n            )\n        \n        return AnimationGroup(\n            *flash_outline_animations,\n            lag_ratio=0.0\n        )\n\nclass NeocognitronToNeocognitronLayer(ConnectiveLayer):\n    input_class = NeocognitronLayer\n    output_class = NeocognitronLayer\n\n    def __init__(self, input_layer, output_layer, **kwargs):\n        super().__init__(input_layer, output_layer, **kwargs)\n\n    def construct_layer(self, input_layer, output_layer, **kwargs):\n        self.filters = []\n        for cell_index in range(input_layer.num_cells):\n            input_cell = input_layer.cell_plane[cell_index]\n            # Randomly choose a cell from the output layer\n            output_cell = output_layer.cell_plane[\n                np.random.randint(0, output_layer.num_cells)\n            ]\n            # Make the filter\n            filter = NeocognitronFilter(\n                input_cell=input_cell,\n                output_cell=output_cell,\n                receptive_field_radius=input_layer.receptive_field_radius,\n                outline_color=self.input_layer.outline_color\n            )\n            # filter = NeocognitronFilter(\n            #     outline_color=input_layer.outline_color\n            # )\n            self.filters.append(filter)\n\n        self.add(VGroup(*self.filters))\n\n    def make_forward_pass_animation(self, layer_args={}, **kwargs):\n        \"\"\"Forward pass for query\"\"\"\n        filter_pulses = []\n        for filter in self.filters:\n            filter_pulses.append(\n                filter.make_filter_pulse_animation()\n            )\n        return AnimationGroup(\n            *filter_pulses\n        )\n\nmanim_ml.neural_network.layers.util.register_custom_connective_layer(\n    NeocognitronToNeocognitronLayer,\n)\n\nclass Neocognitron(NeuralNetwork):\n\n    def __init__(\n        self, \n        camera,\n        cells_per_layer=[4, 5, 4, 4, 3, 3, 5, 5],\n        cell_heights=[0.8, 0.8, 0.8*0.75, 0.8*0.75, 0.8*0.75**2, 0.8*0.75**2, 0.8*0.75**3, 0.02],\n        layer_names=[\"S1\", \"C1\", \"S2\", \"C2\", \"S3\", \"C3\", \"S4\", \"C4\"],\n        receptive_field_sizes=[0.2, 0.2, 0.2*0.75, 0.2*0.75, 0.2*0.75**2, 0.2*0.75**2, 0.2*0.75**3, 0.0],\n    ):\n        self.cells_per_layer = cells_per_layer\n        self.cell_heights = cell_heights\n        self.layer_names = layer_names\n        self.receptive_field_sizes = receptive_field_sizes\n        # Make the neo-cognitron input layer\n        input_layers = []\n        input_layers.append(\n            NeocognitronLayer(\n                1,\n                1.5,\n                0.2,\n                \"U0\",\n                show_outline=False,\n            )\n        )\n        # Make the neo-cognitron layers\n        for i in range(len(cells_per_layer)):\n            layer = NeocognitronLayer(\n                cells_per_layer[i],\n                cell_heights[i],\n                receptive_field_sizes[i],\n                layer_names[i]\n            )\n            input_layers.append(layer)\n        \n        # Make all of the layer labels fixed in frame\n        for layer in input_layers:\n            if isinstance(layer, NeocognitronLayer):\n                # camera.add_fixed_orientation_mobjects(layer.plane_label)\n                pass\n\n        all_layers = []\n        for layer_index in range(len(input_layers) - 1):\n            input_layer = input_layers[layer_index]\n            all_layers.append(input_layer)\n            output_layer = input_layers[layer_index + 1]\n            connective_layer = NeocognitronToNeocognitronLayer(\n                input_layer,\n                output_layer\n            )\n            all_layers.append(connective_layer)\n        all_layers.append(input_layers[-1])\n\n        def custom_layout_func(neural_network):\n            # Insert the connective layers\n            # Pass the layers to neural network super class\n            # Rotate pairs of layers\n            layer_index = 1\n            while layer_index < len(input_layers):\n                prev_layer = input_layers[layer_index - 1]\n                s_layer = input_layers[layer_index]\n                s_layer.next_to(prev_layer, RIGHT, buff=0.0)\n                s_layer.shift(RIGHT * 0.4)\n                c_layer = input_layers[layer_index + 1]\n                c_layer.next_to(s_layer, RIGHT, buff=0.0)\n                c_layer.shift(RIGHT * 0.2)\n                # Rotate the pair of layers\n                group = Group(s_layer, c_layer)\n                group.move_to(np.array([group.get_center()[0], 0, 0]))\n\n                # group.shift(layer_index * 3 * np.array([0, 0, 1.0]))\n                # group.rotate(40 * DEGREES, axis=UP, about_point=group.get_center())\n                # c_layer.rotate(40*DEGREES, axis=UP, about_point=group.get_center())\n                # s_layer.shift(\n                #     layer_index // 2 * np.array([0, 0, 0.3])\n                # )\n                # c_layer.shift(\n                #     layer_index // 2 * np.array([0, 0, 0.3])\n                # )\n                layer_index += 2\n            neural_network.move_to(ORIGIN)\n\n        super().__init__(\n            all_layers,\n            layer_spacing=0.5,\n            custom_layout_func=custom_layout_func\n        )\n\nclass Scene(ThreeDScene):\n\n    def construct(self):\n        neocognitron = Neocognitron(self.camera)\n        neocognitron.shift(DOWN*0.5)\n        self.add(neocognitron)\n        title = Text(\"Neocognitron\").scale(1)\n        self.add_fixed_in_frame_mobjects(title)\n        title.next_to(neocognitron, UP, buff=0.4)\n        self.add(title)\n        \"\"\"\n        self.play(\n            neocognitron.make_forward_pass_animation()\n        )\n        \"\"\"\n        print(self.camera.gamma)\n        print(self.camera.theta)\n        print(self.camera.phi)\n        neocognitron.rotate(90*DEGREES, axis=RIGHT)\n        neocognitron.shift(np.array([0, 0, -0.2]))\n        # title.rotate(90*DEGREES, axis=RIGHT)\n        # self.set_camera_orientation(phi=-15*DEGREES)\n        # vec = np.array([0.0, 0.2, 0.0])\n        # vec /= np.linalg.norm(vec)\n        # x, y, z = vec[0], vec[1], vec[2]\n        # theta = np.arccos(z)\n        # phi = np.arctan(y / x)\n        self.set_camera_orientation(phi=90 * DEGREES, theta=-70*DEGREES, gamma=0*DEGREES)\n        # self.set_camera_orientation(theta=theta, phi=phi)\n\n        forward_pass = neocognitron.make_forward_pass_animation()\n        self.play(forward_pass)\n\n", "original_prompt": "Neocognitron Neocognitron"}
{"prompt": "It appears that the code is a part of a larger visualization script, possibly used for explaining or demonstrating a neural network architecture, specifically a Variational Autoencoder (VAE) with an Oracle-guided image synthesis procedure. The provided snippet seems to be a part of the `construct` method, which initializes and sets up various components of the visualization.\n\nTo make this code more readable and maintainable, here are some suggestions:\n\n1.  **Break down the long `construct` method**: It would be beneficial to break this method into smaller, more manageable functions, each responsible for a specific aspect of the visualization (e.g., creating the VAE architecture, implementing the localization procedure, setting up the training loop).\n\n2.  **Use meaningful variable names**: Some variable names, such as `emb_to_ff_ind`, are not very descriptive. Consider renaming them to something like `encoder_to_ff_layer_index` to improve readability.\n\n3.  **Consider adding comments or docstrings**: While the code is relatively self-explanatory for someone familiar with PyTorch and its associated libraries, it would be helpful to include comments or docstrings that explain what each section of the code does, especially for others who may not be as familiar.\n\n4.  **Use type hints**: Adding type hints can make the code more readable and help catch potential type-related errors earlier in the development process.\n\n5.  **Consider using a more modular approach**: Instead of having all the logic in one place, consider breaking this into separate modules or classes that each handle a specific aspect of the visualization (e.g., `VAE`, `localization_procedure`, `training_loop`).\n\nHere is an example of how you might break down the `construct` method:\n\n```python\ndef construct(self):\n    \"\"\"\n    Makes the whole visualization.\n    \n    1. Create the Architecture\n        a. Create the traditional VAE architecture with images\n    \n    2. The Localization Procedure\n        b. The Algorithm Steps\n    \n    3. The Training Procedure\n        c. Unsupervised Image Reconstruction\n        d. Triplet Loss in Latent Space\n    \"\"\"\n    \n    # Step 1: Create the VAE Architecture\n    self.neural_network.scale(1.2)\n    create_vae = Create(self.neural_network)\n    self.play(create_vae, run_time=3)\n    \n    # Set up prior distribution and localization procedure\n    self.set_up_prior_distribution_and_localization()\n    \n    # Step 3: The Training Procedure\n    self.train_model()\n\ndef set_up_prior_distribution_and_localization(self):\n    \"\"\"\n    Sets up the prior distribution and localization procedure.\n    \n    This involves:\n        a. Creating the prior distribution\n        b. Implementing the localization procedure (N queries)\n    \"\"\"\n    \n    # Create prior distribution\n    self.localizer.prior_distribution = PriorDistribution()\n    self.play(Create(self.localizer.prior_distribution))\n    \n    # Make localization iteration\n    old_posterior = self.localizer.prior_distribution\n    for query_index in range(self.localizer.num_queries):\n        old_posterior = self.make_localization_time_step(old_posterior)\n        self.play(Wait(1))\n\ndef train_model(self):\n    \"\"\"\n    Trains the model using the oracle-guided image synthesis procedure.\n    \n    This involves:\n        a. Unsupervised Image Reconstruction\n        b. Triplet Loss in Latent Space\n    \"\"\"\n    \n    # Do unsupervised image reconstruction\n    self.make_embed_input_image_animation(\n        self.input_embed_image_path, self.output_embed_image_path\n    )\n    self.wait(1)\n    \n    # Do triplet forward pass\n    self.make_triplet_forward_animation()\n    self.wait(1)\n```\n\nThis approach allows for more modular code that is easier to understand and maintain.", "response": "\"\"\"\n    Here is a animated explanatory figure for the \"Oracle Guided Image Synthesis with Relative Queries\" paper. \n\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\nfrom manim_ml.neural_network.layers import triplet\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.paired_query import PairedQueryLayer\nfrom manim_ml.neural_network.layers.triplet import TripletLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\nfrom manim_ml.neural_network.layers import FeedForwardLayer, EmbeddingLayer\nfrom manim_ml.neural_network.layers.util import get_connective_layer\nimport os\n\nfrom manim_ml.utils.mobjects.probability import GaussianDistribution\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\nROOT_DIR = Path(__file__).parents[3]\n\n\nclass Localizer:\n    \"\"\"\n    Holds the localizer object, which contains the queries, images, etc.\n    needed to represent a localization run.\n    \"\"\"\n\n    def __init__(self, axes):\n        # Set dummy values for these\n        self.index = -1\n        self.axes = axes\n        self.num_queries = 3\n        self.assets_path = ROOT_DIR / \"assets/oracle_guidance\"\n        self.ground_truth_image_path = self.assets_path / \"ground_truth.jpg\"\n        self.ground_truth_location = np.array([2, 3])\n        # Prior distribution\n        print(\"initial gaussian\")\n        self.prior_distribution = GaussianDistribution(\n            self.axes,\n            mean=np.array([0.0, 0.0]),\n            cov=np.array([[3, 0], [0, 3]]),\n            dist_theme=\"ellipse\",\n            color=GREEN,\n        )\n        # Define the query images and embedded locations\n        # Contains image paths [(positive_path, negative_path), ...]\n        self.query_image_paths = [\n            (\n                os.path.join(self.assets_path, \"positive_1.jpg\"),\n                os.path.join(self.assets_path, \"negative_1.jpg\"),\n            ),\n            (\n                os.path.join(self.assets_path, \"positive_2.jpg\"),\n                os.path.join(self.assets_path, \"negative_2.jpg\"),\n            ),\n            (\n                os.path.join(self.assets_path, \"positive_3.jpg\"),\n                os.path.join(self.assets_path, \"negative_3.jpg\"),\n            ),\n        ]\n        # Contains 2D locations for each image [([2, 3], [2, 4]), ...]\n        self.query_locations = [\n            (np.array([-1, -1]), np.array([1, 1])),\n            (np.array([1, -1]), np.array([-1, 1])),\n            (np.array([0.3, -0.6]), np.array([-0.5, 0.7])),\n        ]\n        # Make the covariances for each query\n        self.query_covariances = [\n            (np.array([[0.3, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n            (np.array([[0.2, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n            (np.array([[0.2, 0], [0.0, 0.2]]), np.array([[0.2, 0], [0.0, 0.2]])),\n        ]\n        # Posterior distributions over time GaussianDistribution objects\n        self.posterior_distributions = [\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([-0.3, -0.3]),\n                cov=np.array([[5, -4], [-4, 6]]),\n            ).scale(0.6),\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([0.25, -0.25]),\n                cov=np.array([[3, -2], [-2, 4]]),\n            ).scale(0.35),\n            GaussianDistribution(\n                self.axes,\n                dist_theme=\"ellipse\",\n                color=GREEN,\n                mean=np.array([0.4, -0.35]),\n                cov=np.array([[1, 0], [0, 1]]),\n            ).scale(0.3),\n        ]\n        # Some assumptions\n        assert len(self.query_locations) == len(self.query_image_paths)\n        assert len(self.query_locations) == len(self.posterior_distributions)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        \"\"\"Steps through each localization time instance\"\"\"\n        if self.index < len(self.query_image_paths):\n            self.index += 1\n        else:\n            raise StopIteration\n\n        # Return query_paths, query_locations, posterior\n        out_tuple = (\n            self.query_image_paths[self.index],\n            self.query_locations[self.index],\n            self.posterior_distributions[self.index],\n            self.query_covariances[self.index],\n        )\n\n        return out_tuple\n\n\nclass OracleGuidanceVisualization(Scene):\n    def __init__(self):\n        super().__init__()\n        self.neural_network, self.embedding_layer = self.make_vae()\n        self.localizer = iter(Localizer(self.embedding_layer.axes))\n        self.subtitle = None\n        self.title = None\n        # Set image paths\n        # VAE embedding animation image paths\n        self.assets_path = ROOT_DIR / \"assets/oracle_guidance\"\n        self.input_embed_image_path = os.path.join(self.assets_path, \"input_image.jpg\")\n        self.output_embed_image_path = os.path.join(\n            self.assets_path, \"output_image.jpg\"\n        )\n\n    def make_vae(self):\n        \"\"\"Makes a simple VAE architecture\"\"\"\n        embedding_layer = EmbeddingLayer(dist_theme=\"ellipse\")\n        self.encoder = NeuralNetwork(\n            [\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                embedding_layer,\n            ]\n        )\n\n        self.decoder = NeuralNetwork(\n            [\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n            ]\n        )\n\n        neural_network = NeuralNetwork([self.encoder, self.decoder])\n\n        neural_network.shift(DOWN * 0.4)\n        return neural_network, embedding_layer\n\n    @override_animation(Create)\n    def _create_animation(self):\n        animation_group = AnimationGroup(Create(self.neural_network))\n\n        return animation_group\n\n    def insert_at_start(self, layer, create=True):\n        \"\"\"Inserts a layer at the beggining of the network\"\"\"\n        # Note: Does not move the rest of the network\n        current_first_layer = self.encoder.all_layers[0]\n        # Get connective layer\n        connective_layer = get_connective_layer(layer, current_first_layer)\n        # Insert both layers\n        self.encoder.all_layers.insert(0, layer)\n        self.encoder.all_layers.insert(1, connective_layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        layer.shift(DOWN * 0.4)\n        layer.shift(LEFT * 2.35)\n        # Make insert animation\n        if not create:\n            animation_group = AnimationGroup(Create(connective_layer))\n        else:\n            animation_group = AnimationGroup(Create(layer), Create(connective_layer))\n        self.play(animation_group)\n\n    def remove_start_layer(self):\n        \"\"\"Removes the first layer of the network\"\"\"\n        first_layer = self.encoder.all_layers.remove_at_index(0)\n        first_connective = self.encoder.all_layers.remove_at_index(0)\n        # Make remove animations\n        animation_group = AnimationGroup(\n            FadeOut(first_layer), FadeOut(first_connective)\n        )\n\n        self.play(animation_group)\n\n    def insert_at_end(self, layer):\n        \"\"\"Inserts the given layer at the end of the network\"\"\"\n        current_last_layer = self.decoder.all_layers[-1]\n        # Get connective layer\n        connective_layer = get_connective_layer(current_last_layer, layer)\n        # Insert both layers\n        self.decoder.all_layers.add(connective_layer)\n        self.decoder.all_layers.add(layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        layer.shift(DOWN * 0.4)\n        layer.shift(RIGHT * 2.35)\n        # Make insert animation\n        animation_group = AnimationGroup(Create(layer), Create(connective_layer))\n        self.play(animation_group)\n\n    def remove_end_layer(self):\n        \"\"\"Removes the lsat layer of the network\"\"\"\n        first_layer = self.decoder.all_layers.remove_at_index(-1)\n        first_connective = self.decoder.all_layers.remove_at_index(-1)\n        # Make remove animations\n        animation_group = AnimationGroup(\n            FadeOut(first_layer), FadeOut(first_connective)\n        )\n\n        self.play(animation_group)\n\n    def change_title(self, text, title_location=np.array([0, 1.25, 0]), font_size=24):\n        \"\"\"Changes title to the given text\"\"\"\n        if self.title is None:\n            self.title = Text(text, font_size=font_size)\n            self.title.move_to(title_location)\n            self.add(self.title)\n            self.play(Write(self.title), run_time=1)\n            self.wait(1)\n            return\n\n        self.play(Unwrite(self.title))\n        new_title = Text(text, font_size=font_size)\n        new_title.move_to(self.title)\n        self.title = new_title\n        self.wait(0.1)\n        self.play(Write(self.title))\n\n    def change_subtitle(self, text, title_location=np.array([0, 0.9, 0]), font_size=20):\n        \"\"\"Changes subtitle to the next algorithm step\"\"\"\n        if self.subtitle is None:\n            self.subtitle = Text(text, font_size=font_size)\n            self.subtitle.move_to(title_location)\n            self.play(Write(self.subtitle))\n            return\n\n        self.play(Unwrite(self.subtitle))\n        new_title = Text(text, font_size=font_size)\n        new_title.move_to(title_location)\n        self.subtitle = new_title\n        self.wait(0.1)\n        self.play(Write(self.subtitle))\n\n    def make_embed_input_image_animation(self, input_image_path, output_image_path):\n        \"\"\"Makes embed input image animation\"\"\"\n        # insert the input image at the begginging\n        input_image_layer = ImageLayer.from_path(input_image_path)\n        input_image_layer.scale(0.6)\n        current_first_layer = self.encoder.all_layers[0]\n        # Get connective layer\n        connective_layer = get_connective_layer(input_image_layer, current_first_layer)\n        # Insert both layers\n        self.encoder.all_layers.insert(0, input_image_layer)\n        self.encoder.all_layers.insert(1, connective_layer)\n        # Move layers to the correct location\n        # TODO: Fix this cause its hacky\n        input_image_layer.shift(DOWN * 0.4)\n        input_image_layer.shift(LEFT * 2.35)\n        # Play full forward pass\n        forward_pass = self.neural_network.make_forward_pass_animation(\n            layer_args={\n                self.encoder: {\n                    self.embedding_layer: {\n                        \"dist_args\": {\n                            \"cov\": np.array([[1.5, 0], [0, 1.5]]),\n                            \"mean\": np.array([0.5, 0.5]),\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": ORANGE,\n                        }\n                    }\n                }\n            }\n        )\n        self.play(forward_pass)\n        # insert the output image at the end\n        output_image_layer = ImageLayer.from_path(output_image_path)\n        output_image_layer.scale(0.6)\n        self.insert_at_end(output_image_layer)\n        # Remove the input and output layers\n        self.remove_start_layer()\n        self.remove_end_layer()\n        # Remove the latent distribution\n        self.play(FadeOut(self.embedding_layer.latent_distribution))\n\n    def make_localization_time_step(self, old_posterior):\n        \"\"\"\n        Performs one query update for the localization procedure\n\n        Procedure:\n        a. Embed query input images\n        b. Oracle is asked a query\n        c. Query is embedded\n        d. Show posterior update\n        e. Show current recomendation\n        \"\"\"\n        # Helper functions\n        def embed_query_to_latent_space(query_locations, query_covariance):\n            \"\"\"Makes animation for a paired query\"\"\"\n            # Assumes first layer of neural network is a PairedQueryLayer\n            # Make the embedding animation\n            # Wait\n            self.play(Wait(1))\n            # Embed the query to latent space\n            self.change_subtitle(\"3. Embed the Query in Latent Space\")\n            # Make forward pass animation\n            self.embedding_layer.paired_query_mode = True\n            # Make embedding embed query animation\n            embed_query_animation = self.encoder.make_forward_pass_animation(\n                run_time=5,\n                layer_args={\n                    self.embedding_layer: {\n                        \"positive_dist_args\": {\n                            \"cov\": query_covariance[0],\n                            \"mean\": query_locations[0],\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": BLUE,\n                        },\n                        \"negative_dist_args\": {\n                            \"cov\": query_covariance[1],\n                            \"mean\": query_locations[1],\n                            \"dist_theme\": \"ellipse\",\n                            \"color\": RED,\n                        },\n                    }\n                },\n            )\n            self.play(embed_query_animation)\n\n        # Access localizer information\n        query_paths, query_locations, posterior_distribution, query_covariances = next(\n            self.localizer\n        )\n        positive_path, negative_path = query_paths\n        # Make subtitle for present user with query\n        self.change_subtitle(\"2. Present User with Query\")\n        # Insert the layer into the encoder\n        query_layer = PairedQueryLayer.from_paths(\n            positive_path, negative_path, grayscale=False\n        )\n        query_layer.scale(0.5)\n        self.insert_at_start(query_layer)\n        # Embed query to latent space\n        query_to_latent_space_animation = embed_query_to_latent_space(\n            query_locations, query_covariances\n        )\n        # Wait\n        self.play(Wait(1))\n        # Update the posterior\n        self.change_subtitle(\"4. Update the Posterior\")\n        # Remove the old posterior\n        self.play(ReplacementTransform(old_posterior, posterior_distribution))\n        \"\"\"\n        self.play(\n            self.embedding_layer.remove_gaussian_distribution(self.localizer.posterior_distribution)\n        )\n        \"\"\"\n        # self.embedding_layer.add_gaussian_distribution(posterior_distribution)\n        # self.localizer.posterior_distribution = posterior_distribution\n        # Remove query layer\n        self.remove_start_layer()\n        # Remove query ellipses\n\n        fade_outs = []\n        for dist in self.embedding_layer.gaussian_distributions:\n            self.embedding_layer.gaussian_distributions.remove(dist)\n            fade_outs.append(FadeOut(dist))\n\n        if not len(fade_outs) == 0:\n            fade_outs = AnimationGroup(*fade_outs)\n            self.play(fade_outs)\n\n        return posterior_distribution\n\n    def make_generate_estimate_animation(self, estimate_image_path):\n        \"\"\"Makes the generate estimate animation\"\"\"\n        # Change embedding layer mode\n        self.embedding_layer.paired_query_mode = False\n        # Sample from posterior distribution\n        # self.embedding_layer.latent_distribution = self.localizer.posterior_distribution\n        emb_to_ff_ind = self.neural_network.all_layers.index_of(self.encoder)\n        embedding_to_ff = self.neural_network.all_layers[emb_to_ff_ind + 1]\n        self.play(embedding_to_ff.make_forward_pass_animation())\n        # Pass through decoder\n        self.play(self.decoder.make_forward_pass_animation(), run_time=1)\n        # Create Image layer after the decoder\n        output_image_layer = ImageLayer.from_path(estimate_image_path)\n        output_image_layer.scale(0.5)\n        self.insert_at_end(output_image_layer)\n        # Wait\n        self.wait(1)\n        # Remove the image at the end\n        print(self.neural_network)\n        self.remove_end_layer()\n\n    def make_triplet_forward_animation(self):\n        \"\"\"Make triplet forward animation\"\"\"\n        # Make triplet layer\n        anchor_path = os.path.join(self.assets_path, \"anchor.jpg\")\n        positive_path = os.path.join(self.assets_path, \"positive.jpg\")\n        negative_path = os.path.join(self.assets_path, \"negative.jpg\")\n        triplet_layer = TripletLayer.from_paths(\n            anchor_path,\n            positive_path,\n            negative_path,\n            grayscale=False,\n            font_size=100,\n            buff=1.05,\n        )\n        triplet_layer.scale(0.10)\n        self.insert_at_start(triplet_layer)\n        # Make latent triplet animation\n        self.play(\n            self.encoder.make_forward_pass_animation(\n                layer_args={\n                    self.embedding_layer: {\n                        \"triplet_args\": {\n                            \"anchor_dist\": {\n                                \"cov\": np.array([[0.3, 0], [0, 0.3]]),\n                                \"mean\": np.array([0.7, 1.4]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": BLUE,\n                            },\n                            \"positive_dist\": {\n                                \"cov\": np.array([[0.2, 0], [0, 0.2]]),\n                                \"mean\": np.array([0.8, -0.4]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": GREEN,\n                            },\n                            \"negative_dist\": {\n                                \"cov\": np.array([[0.4, 0], [0, 0.25]]),\n                                \"mean\": np.array([-1, -1.2]),\n                                \"dist_theme\": \"ellipse\",\n                                \"color\": RED,\n                            },\n                        }\n                    }\n                },\n                run_time=3,\n            )\n        )\n\n    def construct(self):\n        \"\"\"\n        Makes the whole visualization.\n\n        1. Create the Architecture\n            a. Create the traditional VAE architecture with images\n        2. The Localization Procedure\n        3. The Training Procedure\n        \"\"\"\n        # 1. Create the Architecture\n        self.neural_network.scale(1.2)\n        create_vae = Create(self.neural_network)\n        self.play(create_vae, run_time=3)\n        # Make changing title\n        self.change_title(\"Oracle Guided Image Synthesis\\n      with Relative Queries\")\n        # 2. The Localization Procedure\n        self.change_title(\"The Localization Procedure\")\n        # Make algorithm subtitle\n        self.change_subtitle(\"Algorithm Steps\")\n        # Wait\n        self.play(Wait(1))\n        # Make prior distribution subtitle\n        self.change_subtitle(\"1. Calculate Prior Distribution\")\n        # Draw the prior distribution\n        self.play(Create(self.localizer.prior_distribution))\n        old_posterior = self.localizer.prior_distribution\n        # For N queries update the posterior\n        for query_index in range(self.localizer.num_queries):\n            # Make localization iteration\n            old_posterior = self.make_localization_time_step(old_posterior)\n            self.play(Wait(1))\n            if not query_index == self.localizer.num_queries - 1:\n                # Repeat\n                self.change_subtitle(\"5. Repeat\")\n                # Wait a second\n                self.play(Wait(1))\n        # Generate final estimate\n        self.change_subtitle(\"5. Generate Estimate Image\")\n        # Generate an estimate image\n        estimate_image_path = os.path.join(self.assets_path, \"estimate_image.jpg\")\n        self.make_generate_estimate_animation(estimate_image_path)\n        self.wait(1)\n        # Remove old posterior\n        self.play(FadeOut(old_posterior))\n        # 3. The Training Procedure\n        self.change_title(\"The Training Procedure\")\n        # Make training animation\n        # Do an Image forward pass\n        self.change_subtitle(\"1. Unsupervised Image Reconstruction\")\n        self.make_embed_input_image_animation(\n            self.input_embed_image_path, self.output_embed_image_path\n        )\n        self.wait(1)\n        # Do triplet forward pass\n        self.change_subtitle(\"2. Triplet Loss in Latent Space\")\n        self.make_triplet_forward_animation()\n        self.wait(1)\n", "original_prompt": "Paper Visualizations Oracle Guidance Oracle Guidance"}
{"prompt": "\"Show a 3D neural network with three feedforward layers in different sizes, centered at the origin.\"\n\nNote: This prompt is concise and descriptive, focusing on the key visual element (neural network) without going into technical details.", "response": "from manim import *\n\nfrom manim_ml.neural_network import FeedForwardLayer, NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1200\nconfig.frame_height = 4.0\nconfig.frame_width = 4.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                FeedForwardLayer(num_nodes=3),\n                FeedForwardLayer(num_nodes=5),\n                FeedForwardLayer(num_nodes=3),\n            ]\n        )\n        self.add(nn)\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n", "original_prompt": "Readme Example A Simple Feed Forward Network"}
{"prompt": "\"Animate a 3D neural network with convolutional and feedforward layers unfolding like origami.\"", "response": "from manim import *\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, filter_spacing=0.32),\n                Convolutional2DLayer(\n                    3, 5, 3, filter_spacing=0.32, activation_function=\"ReLU\"\n                ),\n                FeedForwardLayer(3, activation_function=\"Sigmoid\"),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "original_prompt": "Readme Example Activation Functions"}
{"prompt": "Animate a neural network diagram that unfolds and processes input data with color filters.", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "original_prompt": "Readme Example Animating The Forward Pass"}
{"prompt": "\"Animate a neural network with layered images and filters transforming through multiple convolutional layers\"", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n    ImageLayer,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        image = Image.open(\"../../assets/mnist/digit.jpeg\")\n        numpy_image = np.asarray(image)\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "original_prompt": "Readme Example Convolutional Neural Network With Images"}
{"prompt": "\"Animate a 3D neural network diagram with layers transforming into simplified blocks during a fast-forward simulation.\"", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n", "original_prompt": "Readme Example Convolutional Neural Networks"}
{"prompt": "\"Animate a neural network diagram, with layers unfolding and processing data in a feedforward motion.\"", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "original_prompt": "Readme Example Example"}
{"prompt": "\"Animate a 3D neural network diagram moving into frame while performing forward pass calculations.\"", "response": "from manim import *\n\nfrom manim_ml.neural_network import (\n    Convolutional2DLayer,\n    FeedForwardLayer,\n    NeuralNetwork,\n)\n\n# Make the specific scene\nconfig.pixel_height = 700\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(forward_pass)\n", "original_prompt": "Readme Example First Neural Network"}
{"prompt": "\"Create a 3D animated neural network that undergoes a series of transformations.\"", "response": "from manim import *\nfrom PIL import Image\nimport numpy as np\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.max_pooling_2d import MaxPooling2DLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass MaxPoolingScene(ThreeDScene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 8),\n                Convolutional2DLayer(3, 6, 3),\n                MaxPooling2DLayer(kernel_size=2),\n                Convolutional2DLayer(5, 2, 2),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation()\n        self.play(ChangeSpeed(forward_pass, speedinfo={}), run_time=10)\n        self.wait(1)\n", "original_prompt": "Readme Example Max Pooling"}
{"prompt": "\"Animate a neural network with multiple layers to fade into a drop-out effect.\"", "response": "from manim import *\nfrom manim_ml.neural_network.animations.dropout import (\n    make_neural_network_dropout_animation,\n)\nfrom manim_ml.neural_network import FeedForwardLayer, NeuralNetwork\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 5.0\nconfig.frame_width = 5.0\n\n\nclass DropoutNeuralNetworkScene(Scene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                FeedForwardLayer(3, rectangle_color=BLUE),\n                FeedForwardLayer(5, rectangle_color=BLUE),\n                FeedForwardLayer(3, rectangle_color=BLUE),\n                FeedForwardLayer(5, rectangle_color=BLUE),\n                FeedForwardLayer(4, rectangle_color=BLUE),\n            ],\n            layer_spacing=0.4,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        # Play animation\n        self.play(\n            make_neural_network_dropout_animation(\n                nn, dropout_rate=0.25, do_forward_pass=True\n            )\n        )\n        self.wait(1)\n", "original_prompt": "Readme Example Neural Network Dropout"}
{"prompt": "\"Draw a neural network diagram with layers of 2D convolutional filters, animated forward pass.\"", "response": "from manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n\nclass ConvolutionalNetworkScene(Scene):\n    def construct(self):\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                Convolutional2DLayer(1, 7, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 3, filter_spacing=0.32),\n                Convolutional2DLayer(5, 3, 3, filter_spacing=0.18),\n                FeedForwardLayer(3),\n                FeedForwardLayer(3),\n            ],\n            layer_spacing=0.25,\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        self.add(nn)\n        self.play(nn.make_forward_pass_animation())\n", "original_prompt": "Readme Example Old Example"}
{"prompt": "Animate a simple 3D text label in space.", "response": "from manim import *\n\n# Import modules here\n\n\nclass BasicScene(ThreeDScene):\n    def construct(self):\n        # Your code goes here\n        text = Text(\"Your first scene!\")\n        self.add(text)\n", "original_prompt": "Readme Example Setting Up A Scene"}
{"prompt": "\"Animate a 3D neural network diagram rotating slowly, with layers transforming and showing image input.\"", "response": "from manim import *\nfrom PIL import Image\n\nfrom manim_ml.neural_network.layers.convolutional_2d import Convolutional2DLayer\nfrom manim_ml.neural_network.layers.feed_forward import FeedForwardLayer\nfrom manim_ml.neural_network.layers.image import ImageLayer\nfrom manim_ml.neural_network.layers.parent_layers import ThreeDLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\n# Make the specific scene\nconfig.pixel_height = 1200\nconfig.pixel_width = 800\nconfig.frame_height = 6.0\nconfig.frame_width = 6.0\n\n\nclass CombinedScene(ThreeDScene):\n    def construct(self):\n        image = Image.open(\"../../assets/doggo.jpeg\")\n        numpy_image = np.asarray(image)\n        # Rotate the Three D layer position\n        ThreeDLayer.rotation_angle = 15 * DEGREES\n        ThreeDLayer.rotation_axis = [1, -1.0, 0]\n        # Make nn\n        nn = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.5),\n                Convolutional2DLayer(1, 7, 7, 3, 3, filter_spacing=0.32),\n                Convolutional2DLayer(3, 5, 5, 1, 1, filter_spacing=0.18),\n            ],\n            layer_spacing=0.25,\n            layout_direction=\"top_to_bottom\",\n        )\n        # Center the nn\n        nn.move_to(ORIGIN)\n        nn.scale(1.5)\n        self.add(nn)\n        # Play animation\n        forward_pass = nn.make_forward_pass_animation(\n            highlight_active_feature_map=True,\n        )\n        self.wait(1)\n        self.play(forward_pass)\n", "original_prompt": "Translation Equivariance Translation Equivariance"}
{"prompt": "\"Rotate a circle around the origin, then fade it into a square with linear motion.\"", "response": "", "original_prompt": "Variational Autoencoder Autoencoder Models   Init  "}
{"prompt": "Animate a 3D histogram that transforms into a 2D scatter plot of digit images, with fading colors over time.", "response": "import pickle\nimport sys\nimport os\n\nsys.path.append(os.environ[\"PROJECT_ROOT\"])\nfrom autoencoder_models.variational_autoencoder import (\n    VAE,\n    load_dataset,\n    load_vae_from_path,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport scipy\nimport scipy.stats\nimport cv2\n\n\ndef binned_images(model_path, num_x_bins=6, plot=False):\n    latent_dim = 2\n    model = load_vae_from_path(model_path, latent_dim)\n    image_dataset = load_dataset(digit=2)\n    # Compute embedding\n    num_images = 500\n    embedding = []\n    images = []\n    for i in range(num_images):\n        image, _ = image_dataset[i]\n        mean, _, recon, _ = model.forward(image)\n        mean = mean.detach().numpy()\n        recon = recon.detach().numpy()\n        recon = recon.reshape(32, 32)\n        images.append(recon.squeeze())\n        if latent_dim > 2:\n            mean = mean[:2]\n        embedding.append(mean)\n    images = np.stack(images)\n    tsne_points = np.array(embedding)\n    tsne_points = (tsne_points - tsne_points.mean(axis=0)) / (tsne_points.std(axis=0))\n    # make vis\n    num_points = np.shape(tsne_points)[0]\n    x_min = np.amin(tsne_points.T[0])\n    y_min = np.amin(tsne_points.T[1])\n    y_max = np.amax(tsne_points.T[1])\n    x_max = np.amax(tsne_points.T[0])\n    # make the bins from the ranges\n    # to keep it square the same width is used for x and y dim\n    x_bins, step = np.linspace(x_min, x_max, num_x_bins, retstep=True)\n    x_bins = x_bins.astype(float)\n    num_y_bins = np.absolute(np.ceil((y_max - y_min) / step)).astype(int)\n    y_bins = np.linspace(y_min, y_max, num_y_bins)\n    # sort the tsne_points into a 2d histogram\n    tsne_points = tsne_points.squeeze()\n    hist_obj = scipy.stats.binned_statistic_dd(\n        tsne_points,\n        np.arange(num_points),\n        statistic=\"count\",\n        bins=[x_bins, y_bins],\n        expand_binnumbers=True,\n    )\n    # sample one point from each bucket\n    binnumbers = hist_obj.binnumber\n    num_x_bins = np.amax(binnumbers[0]) + 1\n    num_y_bins = np.amax(binnumbers[1]) + 1\n    binnumbers = binnumbers.T\n    # some places have no value in a region\n    used_mask = np.zeros((num_y_bins, num_x_bins))\n    image_bins = np.zeros(\n        (num_y_bins, num_x_bins, 3, np.shape(images)[2], np.shape(images)[2])\n    )\n    for i, bin_num in enumerate(list(binnumbers)):\n        used_mask[bin_num[1], bin_num[0]] = 1\n        image_bins[bin_num[1], bin_num[0]] = images[i]\n    # plot a grid of the images\n    fig, axs = plt.subplots(\n        nrows=np.shape(y_bins)[0],\n        ncols=np.shape(x_bins)[0],\n        constrained_layout=False,\n        dpi=50,\n    )\n    images = []\n    bin_indices = []\n    for y in range(num_y_bins):\n        for x in range(num_x_bins):\n            if used_mask[y, x] > 0.0:\n                image = np.uint8(image_bins[y][x].squeeze() * 255)\n                image = np.rollaxis(image, 0, 3)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                axs[num_y_bins - 1 - y][x].imshow(image)\n                images.append(image)\n                bin_indices.append((y, x))\n            axs[y, x].axis(\"off\")\n    if plot:\n        plt.axis(\"off\")\n        plt.show()\n    else:\n        return images, bin_indices\n\n\ndef generate_disentanglement(model_path=\"saved_models/model_dim2.pth\"):\n    \"\"\"Generates disentanglement visualization and serializes it\"\"\"\n    # Disentanglement object\n    disentanglement_object = {}\n    # Make Disentanglement\n    images, bin_indices = binned_images(model_path)\n    disentanglement_object[\"images\"] = images\n    disentanglement_object[\"bin_indices\"] = bin_indices\n    # Serialize Images\n    with open(\"disentanglement.pkl\", \"wb\") as f:\n        pickle.dump(disentanglement_object, f)\n\n\nif __name__ == \"__main__\":\n    plot = False\n    if plot:\n        model_path = \"saved_models/model_dim2.pth\"\n        # uniform_image_sample(model_path)\n        binned_images(model_path)\n    else:\n        generate_disentanglement()\n", "original_prompt": "Variational Autoencoder Autoencoder Models Generate Disentanglement"}
{"prompt": "\"Display MNIST images with their VAE reconstructions side-by-side\"", "response": "import torch\nfrom variational_autoencoder import VAE\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport numpy as np\nimport pickle\n\n# Load model\nvae = VAE(latent_dim=16)\nvae.load_state_dict(torch.load(\"saved_models/model.pth\"))\n# Transforms images to a PyTorch Tensor\ntensor_transform = transforms.ToTensor()\n# Download the MNIST Dataset\ndataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=tensor_transform\n)\n# Generate reconstructions\nnum_recons = 10\nfig, axs = plt.subplots(num_recons, 2, figsize=(2, num_recons))\nimage_pairs = []\nfor i in range(num_recons):\n    base_image, _ = dataset[i]\n    base_image = base_image.reshape(-1, 28 * 28)\n    _, _, recon_image, _ = vae.forward(base_image)\n    base_image = base_image.detach().numpy()\n    base_image = np.reshape(base_image, (28, 28)) * 255\n    recon_image = recon_image.detach().numpy()\n    recon_image = np.reshape(recon_image, (28, 28)) * 255\n    # Add to plot\n    axs[i][0].imshow(base_image)\n    axs[i][1].imshow(recon_image)\n    # image pairs\n    image_pairs.append((base_image, recon_image))\n\nwith open(\"image_pairs.pkl\", \"wb\") as f:\n    pickle.dump(image_pairs, f)\n\nplt.show()\n", "original_prompt": "Variational Autoencoder Autoencoder Models Generate Images"}
{"prompt": "Animate a series of smoothly interpolated images between two faces on a grid.", "response": "import torch\nfrom variational_autoencoder import VAE, load_dataset\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom tqdm import tqdm\nimport numpy as np\nimport pickle\n\n# Load model\nvae = VAE(latent_dim=16)\nvae.load_state_dict(torch.load(\"saved_models/model.pth\"))\ndataset = load_dataset()\n# Generate reconstructions\nnum_images = 50\nimage_pairs = []\nsave_object = {\"interpolation_path\": [], \"interpolation_images\": []}\n\n# Make interpolation path\nimage_a, image_b = dataset[0][0], dataset[1][0]\nimage_a = image_a.view(32 * 32)\nimage_b = image_b.view(32 * 32)\nz_a, _, _, _ = vae.forward(image_a)\nz_a = z_a.detach().cpu().numpy()\nz_b, _, _, _ = vae.forward(image_b)\nz_b = z_b.detach().cpu().numpy()\ninterpolation_path = np.linspace(z_a, z_b, num=num_images)\n# interpolation_path[:, 4] = np.linspace(-3, 3, num=num_images)\nsave_object[\"interpolation_path\"] = interpolation_path\n\nfor i in range(num_images):\n    # Generate\n    z = torch.Tensor(interpolation_path[i]).unsqueeze(0)\n    gen_image = vae.decode(z).detach().numpy()\n    gen_image = np.reshape(gen_image, (32, 32)) * 255\n    save_object[\"interpolation_images\"].append(gen_image)\n\nfig, axs = plt.subplots(num_images, 1, figsize=(1, num_images))\nimage_pairs = []\nfor i in range(num_images):\n    recon_image = save_object[\"interpolation_images\"][i]\n    # Add to plot\n    axs[i].imshow(recon_image)\n\n# Perform intrpolations\nwith open(\"interpolations.pkl\", \"wb\") as f:\n    pickle.dump(save_object, f)\n\nplt.show()\n", "original_prompt": "Variational Autoencoder Autoencoder Models Generate Interpolation"}
{"prompt": "Here's a breakdown of what this code does and some suggestions for improvement:\n\n**Overview**\n\nThis code defines a Variational Autoencoder (VAE) using PyTorch. The VAE is trained to reconstruct images from their latent space. The model consists of an encoder that maps the input image to a mean and log variance of a Gaussian distribution in the latent space, and a decoder that samples from this distribution to generate reconstructed images.\n\n**Code Structure**\n\nThe code is organized into several sections:\n\n1. **VAE Model Definition**: This section defines the VAE model using PyTorch.\n2. **Training Loop**: This section implements the training loop for the VAE model.\n3. **Loss Function**: This section defines the loss function used to train the VAE model.\n\n**Code Quality**\n\nThe code is well-organized and follows good practices. However, there are a few areas that could be improved:\n\n* **Variable Naming**: Some variable names could be more descriptive. For example, `x` could be renamed to `input_image`.\n* **Comments**: There are no comments in the code to explain what each section does or how it works.\n* **Error Handling**: The code catches `NaN` values that occur during training but does not provide any further error messages.\n\n**Suggestions for Improvement**\n\n1. **Add Comments**: Add comments to explain what each section of the code does and how it works.\n2. **Improve Variable Names**: Rename variables to be more descriptive, such as `input_image` instead of `x`.\n3. **Error Handling**: Consider adding more error handling to handle cases where training fails or other exceptions occur.\n4. **Plotting Losses**: Instead of only plotting the last 100 values, consider plotting all losses over time to get a better sense of how the model is performing.\n\nHere's an updated version of the code with some of these suggestions:\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\n# Define the VAE model\nclass VAE(nn.Module):\n    def __init__(self, latent_dim):\n        super(VAE, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(128, 128, kernel_size=3),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=3),\n            nn.ReLU()\n        )\n        self.mean = nn.Linear(128*8*8, latent_dim)\n        self.log_var = nn.Linear(128*8*8, latent_dim)\n\n    def encode(self, x):\n        x = self.encoder(x)\n        x = x.view(-1, 128*8*8)\n        mean = self.mean(x)\n        log_var = self.log_var(x)\n        return mean, log_var\n\n    def decode(self, z):\n        z = z.view(-1, 128, 8, 8)\n        x_recon = torch.sigmoid(self.decoder(z))\n        return x_recon\n\n    def forward(self, x):\n        mean, log_var = self.encode(x)\n        eps = torch.randn_like(mean)\n        z = mean + torch.exp(log_var / 2) * eps\n        x_recon = self.decode(z)\n        return mean, log_var, x_recon\n\n# Define the loss function\ndef loss_function(mean, log_var, reconstructed, original):\n    kl = -0.5 * torch.sum(1 + log_var - mean**2 - log_var.exp(), dim=1).sum()\n    recon = torch.nn.functional.mse_loss(reconstructed, original)\n    return 0.01 * kl + recon\n\n# Train the VAE model\ndef train_model(latent_dim, plot=False):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    vae = VAE(latent_dim).to(device)\n\n    # Initialize the optimizer and scheduler\n    optimizer = optim.Adam(vae.parameters(), lr=1e-4, weight_decay=0e-8)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n\n    # Define the data loader\n    train_loader = torch.utils.data.DataLoader(\n        torchvision.datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True),\n        batch_size=32,\n        shuffle=True,\n        num_workers=2\n    )\n\n    # Train the model\n    for epoch in range(200):\n        for (image, _) in train_loader:\n            image = image.to(device)\n            mean, log_var, reconstructed = vae(image)\n            loss = 0.01 * torch.sum(log_var) + torch.nn.functional.mse_loss(reconstructed, image)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        scheduler.step()\n\n        if plot:\n            plt.plot(loss.item())\n            plt.show()\n\n# Run the training loop\ntrain_model(2, plot=True)\n```\n\nNote that this is just one possible way to improve the code, and there may be other approaches depending on your specific needs.", "response": "import torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport math\n\n\"\"\"\n    These are utility functions that help to calculate the input and output\n    sizes of convolutional neural networks\n\"\"\"\n\n\ndef num2tuple(num):\n    return num if isinstance(num, tuple) else (num, num)\n\n\ndef conv2d_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    h_w, kernel_size, stride, pad, dilation = (\n        num2tuple(h_w),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(pad),\n        num2tuple(dilation),\n    )\n    pad = num2tuple(pad[0]), num2tuple(pad[1])\n\n    h = math.floor(\n        (h_w[0] + sum(pad[0]) - dilation[0] * (kernel_size[0] - 1) - 1) / stride[0] + 1\n    )\n    w = math.floor(\n        (h_w[1] + sum(pad[1]) - dilation[1] * (kernel_size[1] - 1) - 1) / stride[1] + 1\n    )\n\n    return h, w\n\n\ndef convtransp2d_output_shape(\n    h_w, kernel_size=1, stride=1, pad=0, dilation=1, out_pad=0\n):\n    h_w, kernel_size, stride, pad, dilation, out_pad = (\n        num2tuple(h_w),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(pad),\n        num2tuple(dilation),\n        num2tuple(out_pad),\n    )\n    pad = num2tuple(pad[0]), num2tuple(pad[1])\n\n    h = (\n        (h_w[0] - 1) * stride[0]\n        - sum(pad[0])\n        + dialation[0] * (kernel_size[0] - 1)\n        + out_pad[0]\n        + 1\n    )\n    w = (\n        (h_w[1] - 1) * stride[1]\n        - sum(pad[1])\n        + dialation[1] * (kernel_size[1] - 1)\n        + out_pad[1]\n        + 1\n    )\n\n    return h, w\n\n\ndef conv2d_get_padding(h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1):\n    h_w_in, h_w_out, kernel_size, stride, dilation = (\n        num2tuple(h_w_in),\n        num2tuple(h_w_out),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(dilation),\n    )\n\n    p_h = (\n        (h_w_out[0] - 1) * stride[0]\n        - h_w_in[0]\n        + dilation[0] * (kernel_size[0] - 1)\n        + 1\n    )\n    p_w = (\n        (h_w_out[1] - 1) * stride[1]\n        - h_w_in[1]\n        + dilation[1] * (kernel_size[1] - 1)\n        + 1\n    )\n\n    return (math.floor(p_h / 2), math.ceil(p_h / 2)), (\n        math.floor(p_w / 2),\n        math.ceil(p_w / 2),\n    )\n\n\ndef convtransp2d_get_padding(\n    h_w_in, h_w_out, kernel_size=1, stride=1, dilation=1, out_pad=0\n):\n    h_w_in, h_w_out, kernel_size, stride, dilation, out_pad = (\n        num2tuple(h_w_in),\n        num2tuple(h_w_out),\n        num2tuple(kernel_size),\n        num2tuple(stride),\n        num2tuple(dilation),\n        num2tuple(out_pad),\n    )\n\n    p_h = (\n        -(\n            h_w_out[0]\n            - 1\n            - out_pad[0]\n            - dilation[0] * (kernel_size[0] - 1)\n            - (h_w_in[0] - 1) * stride[0]\n        )\n        / 2\n    )\n    p_w = (\n        -(\n            h_w_out[1]\n            - 1\n            - out_pad[1]\n            - dilation[1] * (kernel_size[1] - 1)\n            - (h_w_in[1] - 1) * stride[1]\n        )\n        / 2\n    )\n\n    return (math.floor(p_h / 2), math.ceil(p_h / 2)), (\n        math.floor(p_w / 2),\n        math.ceil(p_w / 2),\n    )\n\n\ndef load_dataset(train=True, digit=None):\n    # Transforms images to a PyTorch Tensor\n    tensor_transform = transforms.Compose([transforms.Pad(2), transforms.ToTensor()])\n\n    # Download the MNIST Dataset\n    dataset = datasets.MNIST(\n        root=\"./data\", train=train, download=True, transform=tensor_transform\n    )\n    # Load specific image\n    if not digit is None:\n        idx = dataset.train_labels == digit\n        dataset.targets = dataset.targets[idx]\n        dataset.data = dataset.data[idx]\n\n    return dataset\n\n\ndef load_vae_from_path(path, latent_dim):\n    model = VAE(latent_dim)\n    model.load_state_dict(torch.load(path))\n\n    return model\n\n\n# Creating a PyTorch class\n# 28*28 ==> 9 ==> 28*28\nclass VAE(torch.nn.Module):\n    def __init__(self, latent_dim=5, layer_count=4, channels=1):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.in_shape = 32\n        self.layer_count = layer_count\n        self.channels = channels\n        self.d = 128\n        mul = 1\n        inputs = self.channels\n        out_sizes = [(self.in_shape, self.in_shape)]\n        for i in range(self.layer_count):\n            setattr(self, \"conv%d\" % (i + 1), nn.Conv2d(inputs, self.d * mul, 4, 2, 1))\n            setattr(self, \"conv%d_bn\" % (i + 1), nn.BatchNorm2d(self.d * mul))\n            h_w = (out_sizes[-1][-1], out_sizes[-1][-1])\n            out_sizes.append(\n                conv2d_output_shape(h_w, kernel_size=4, stride=2, pad=1, dilation=1)\n            )\n            inputs = self.d * mul\n            mul *= 2\n\n        self.d_max = inputs\n        self.last_size = out_sizes[-1][-1]\n        self.num_linear = self.last_size**2 * self.d_max\n        # Encoder linear layers\n        self.encoder_mean_linear = nn.Linear(self.num_linear, self.latent_dim)\n        self.encoder_logvar_linear = nn.Linear(self.num_linear, self.latent_dim)\n        # Decoder linear layer\n        self.decoder_linear = nn.Linear(self.latent_dim, self.num_linear)\n\n        mul = inputs // self.d // 2\n\n        for i in range(1, self.layer_count):\n            setattr(\n                self,\n                \"deconv%d\" % (i + 1),\n                nn.ConvTranspose2d(inputs, self.d * mul, 4, 2, 1),\n            )\n            setattr(self, \"deconv%d_bn\" % (i + 1), nn.BatchNorm2d(self.d * mul))\n            inputs = self.d * mul\n            mul //= 2\n\n        setattr(\n            self,\n            \"deconv%d\" % (self.layer_count + 1),\n            nn.ConvTranspose2d(inputs, self.channels, 4, 2, 1),\n        )\n\n    def encode(self, x):\n        if len(x.shape) < 3:\n            x = x.unsqueeze(0)\n        if len(x.shape) < 4:\n            x = x.unsqueeze(1)\n        batch_size = x.shape[0]\n\n        for i in range(self.layer_count):\n            x = F.relu(\n                getattr(self, \"conv%d_bn\" % (i + 1))(\n                    getattr(self, \"conv%d\" % (i + 1))(x)\n                )\n            )\n\n        x = x.view(batch_size, -1)\n\n        mean = self.encoder_mean_linear(x)\n        logvar = self.encoder_logvar_linear(x)\n\n        return mean, logvar\n\n    def decode(self, x):\n        x = x.view(x.shape[0], self.latent_dim)\n        x = self.decoder_linear(x)\n        x = x.view(x.shape[0], self.d_max, self.last_size, self.last_size)\n        # x = self.deconv1_bn(x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(1, self.layer_count):\n            x = F.leaky_relu(\n                getattr(self, \"deconv%d_bn\" % (i + 1))(\n                    getattr(self, \"deconv%d\" % (i + 1))(x)\n                ),\n                0.2,\n            )\n        x = getattr(self, \"deconv%d\" % (self.layer_count + 1))(x)\n        x = torch.sigmoid(x)\n        return x\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        mean, logvar = self.encode(x)\n        eps = torch.randn(batch_size, self.latent_dim)\n        z = mean + torch.exp(logvar / 2) * eps\n        reconstructed = self.decode(z)\n        return mean, logvar, reconstructed, x\n\n\ndef train_model(latent_dim=16, plot=True, digit=1, epochs=200):\n    dataset = load_dataset(train=True, digit=digit)\n    # DataLoader is used to load the dataset\n    # for training\n    loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)\n    # Model Initialization\n    model = VAE(latent_dim=latent_dim)\n    # Validation using MSE Loss function\n    def loss_function(mean, log_var, reconstructed, original, kl_beta=0.0001):\n        kl = torch.mean(\n            -0.5 * torch.sum(1 + log_var - mean**2 - log_var.exp(), dim=1), dim=0\n        )\n        recon = torch.nn.functional.mse_loss(reconstructed, original)\n        # print(f\"KL Error {kl}, Recon Error {recon}\")\n        return kl_beta * kl + recon\n\n    # Using an Adam Optimizer with lr = 0.1\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0e-8)\n\n    outputs = []\n    losses = []\n    for epoch in tqdm(range(epochs)):\n        for (image, _) in loader:\n            # Output of Autoencoder\n            mean, log_var, reconstructed, image = model(image)\n            # Calculating the loss function\n            loss = loss_function(mean, log_var, reconstructed, image)\n            # The gradients are set to zero,\n            # the the gradient is computed and stored.\n            # .step() performs parameter update\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            # Storing the losses in a list for plotting\n            if torch.isnan(loss):\n                raise Exception()\n            losses.append(loss.detach().cpu())\n            outputs.append((epochs, image, reconstructed))\n\n    torch.save(\n        model.state_dict(),\n        os.path.join(\n            os.environ[\"PROJECT_ROOT\"],\n            f\"examples/variational_autoencoder/autoencoder_model/saved_models/model_dim{latent_dim}.pth\",\n        ),\n    )\n\n    if plot:\n        # Defining the Plot Style\n        plt.style.use(\"fivethirtyeight\")\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        # Plotting the last 100 values\n        plt.plot(losses)\n        plt.show()\n\n\nif __name__ == \"__main__\":\n    train_model(latent_dim=2, digit=2, epochs=40)\n", "original_prompt": "Variational Autoencoder Autoencoder Models Variational Autoencoder"}
{"prompt": "\"Animate a neural network with multiple layers and inputs, generating an output image.\"", "response": "\"\"\"Autoencoder Manim Visualizations\n\nIn this module I define Manim visualizations for Variational Autoencoders\nand Traditional Autoencoders.\n\n\"\"\"\nfrom pathlib import Path\n\nfrom manim import *\nimport numpy as np\nfrom PIL import Image\nfrom manim_ml.neural_network.layers import EmbeddingLayer\nfrom manim_ml.neural_network.layers import FeedForwardLayer\nfrom manim_ml.neural_network.layers import ImageLayer\nfrom manim_ml.neural_network.neural_network import NeuralNetwork\n\nROOT_DIR = Path(__file__).parents[2]\n\nconfig.pixel_height = 1200\nconfig.pixel_width = 1900\nconfig.frame_height = 7.0\nconfig.frame_width = 7.0\n\n\nclass VAEScene(Scene):\n    \"\"\"Scene object for a Variational Autoencoder and Autoencoder\"\"\"\n\n    def construct(self):\n        numpy_image = np.asarray(Image.open(ROOT_DIR / \"assets/mnist/digit.jpeg\"))\n        vae = NeuralNetwork(\n            [\n                ImageLayer(numpy_image, height=1.4),\n                FeedForwardLayer(5),\n                FeedForwardLayer(3),\n                EmbeddingLayer(dist_theme=\"ellipse\"),\n                FeedForwardLayer(3),\n                FeedForwardLayer(5),\n                ImageLayer(numpy_image, height=1.4),\n            ]\n        )\n\n        self.play(Create(vae))\n        self.play(vae.make_forward_pass_animation(run_time=15))\n", "original_prompt": "Variational Autoencoder Variational Autoencoder"}
